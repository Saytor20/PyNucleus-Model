{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Model - Developer Notebook Clean** 🔧\n",
        "\n",
        "## **Advanced Development Interface** \n",
        "\n",
        "This notebook provides advanced developer tools for the PyNucleus system with deep diagnostics, performance analysis, and system monitoring capabilities.\n",
        "\n",
        "### **🔧 Developer Features:**\n",
        "- **📊 System Diagnostics**: Comprehensive health checks and performance monitoring\n",
        "- **🧪 Advanced Testing**: Detailed evaluation with per-question analysis\n",
        "- **⚙️ Configuration Management**: Dynamic settings modification and validation\n",
        "- **📈 Performance Profiling**: Embedding quality analysis and retrieval optimization\n",
        "- **🔍 Debug Tools**: Error analysis, logging, and troubleshooting utilities\n",
        "- **🚀 Production Tools**: Monitoring, alerting, and deployment validation\n",
        "\n",
        "### **✨ Advanced Capabilities:**\n",
        "- ✅ **Real-time Monitoring**: Live system health and performance metrics\n",
        "- ✅ **Custom Evaluations**: Build and run custom test suites\n",
        "- ✅ **Model Comparison**: Test different models and configurations\n",
        "- ✅ **Retrieval Analysis**: Deep dive into document retrieval quality\n",
        "- ✅ **Error Tracking**: Comprehensive error logging and analysis\n",
        "- ✅ **Performance Optimization**: Automated tuning and recommendations\n",
        "\n",
        "### **📋 Developer Workflow:**\n",
        "1. **🔍 System Analysis** (Cell 1): Deep system diagnostics and health checks\n",
        "2. **⚙️ Configuration Management** (Cell 2): Dynamic settings and environment setup\n",
        "3. **🧪 Advanced Testing** (Cell 3): Comprehensive evaluation and validation\n",
        "4. **📊 Performance Profiling** (Cell 4): Detailed performance analysis\n",
        "5. **🔧 Debug & Troubleshooting** (Cell 5): Error analysis and system debugging\n",
        "6. **🚀 Production Monitoring** (Cell 6): Real-time monitoring and alerting\n",
        "\n",
        "**⚡ Professional development tools for production-grade AI systems!**\n",
        "\n",
        "---\n",
        "**📖 For basic usage**: See `Capstone_Project_Clean.ipynb` for simplified interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Advanced System Diagnostics & Health Checks\n",
        "# =====================================================\n",
        "# This cell provides comprehensive system analysis for developers\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "print(\"🔍 PyNucleus Advanced System Diagnostics\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📅 Diagnostic session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "class SystemDiagnostics:\n",
        "    \"\"\"Advanced system diagnostics and monitoring\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.diagnostics = {}\n",
        "        \n",
        "    def check_system_resources(self) -> Dict[str, Any]:\n",
        "        \"\"\"Check system resource usage\"\"\"\n",
        "        print(\"\\n🖥️ System Resource Analysis:\")\n",
        "        \n",
        "        # CPU usage\n",
        "        cpu_percent = psutil.cpu_percent(interval=1)\n",
        "        cpu_count = psutil.cpu_count()\n",
        "        \n",
        "        # Memory usage\n",
        "        memory = psutil.virtual_memory()\n",
        "        memory_gb = memory.total / (1024**3)\n",
        "        memory_used_percent = memory.percent\n",
        "        \n",
        "        # Disk usage\n",
        "        disk = psutil.disk_usage('/')\n",
        "        disk_gb = disk.total / (1024**3)\n",
        "        disk_used_percent = (disk.used / disk.total) * 100\n",
        "        \n",
        "        resources = {\n",
        "            'cpu_percent': cpu_percent,\n",
        "            'cpu_count': cpu_count,\n",
        "            'memory_total_gb': memory_gb,\n",
        "            'memory_used_percent': memory_used_percent,\n",
        "            'disk_total_gb': disk_gb,\n",
        "            'disk_used_percent': disk_used_percent\n",
        "        }\n",
        "        \n",
        "        print(f\"   • CPU Usage: {cpu_percent:.1f}% ({cpu_count} cores)\")\n",
        "        print(f\"   • Memory: {memory_used_percent:.1f}% used of {memory_gb:.1f} GB\")\n",
        "        print(f\"   • Disk: {disk_used_percent:.1f}% used of {disk_gb:.1f} GB\")\n",
        "        \n",
        "        return resources\n",
        "    \n",
        "    def check_python_environment(self) -> Dict[str, Any]:\n",
        "        \"\"\"Check Python environment and dependencies\"\"\"\n",
        "        print(\"\\n🐍 Python Environment Analysis:\")\n",
        "        \n",
        "        # Python version\n",
        "        python_version = sys.version\n",
        "        python_executable = sys.executable\n",
        "        \n",
        "        # Check key dependencies\n",
        "        dependencies = {}\n",
        "        required_packages = [\n",
        "            'torch', 'transformers', 'sentence_transformers', \n",
        "            'chromadb', 'pydantic', 'loguru', 'tiktoken'\n",
        "        ]\n",
        "        \n",
        "        for package in required_packages:\n",
        "            try:\n",
        "                module = __import__(package)\n",
        "                version = getattr(module, '__version__', 'unknown')\n",
        "                dependencies[package] = {'installed': True, 'version': version}\n",
        "                print(f\"   ✅ {package}: {version}\")\n",
        "            except ImportError:\n",
        "                dependencies[package] = {'installed': False, 'version': None}\n",
        "                print(f\"   ❌ {package}: Not installed\")\n",
        "        \n",
        "        env_info = {\n",
        "            'python_version': python_version.split()[0],\n",
        "            'python_executable': python_executable,\n",
        "            'dependencies': dependencies\n",
        "        }\n",
        "        \n",
        "        return env_info\n",
        "    \n",
        "    def check_pynucleus_components(self) -> Dict[str, Any]:\n",
        "        \"\"\"Check PyNucleus specific components\"\"\"\n",
        "        print(\"\\n🧪 PyNucleus Component Analysis:\")\n",
        "        \n",
        "        components = {}\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.settings import settings\n",
        "            components['settings'] = {'loaded': True, 'config': dict(settings)}\n",
        "            print(\"   ✅ Settings: Loaded successfully\")\n",
        "            \n",
        "            # Check configuration values\n",
        "            critical_settings = ['CHROMA_PATH', 'MODEL_ID', 'EMB_MODEL']\n",
        "            for setting in critical_settings:\n",
        "                value = getattr(settings, setting, None)\n",
        "                print(f\"      • {setting}: {value}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            components['settings'] = {'loaded': False, 'error': str(e)}\n",
        "            print(f\"   ❌ Settings: Error - {e}\")\n",
        "        \n",
        "        # Check other components\n",
        "        component_modules = {\n",
        "            'logger': 'pynucleus.utils.logger',\n",
        "            'rag_engine': 'pynucleus.rag.engine',\n",
        "            'rag_collector': 'pynucleus.rag.collector',\n",
        "            'qwen_loader': 'pynucleus.llm.qwen_loader',\n",
        "            'golden_eval': 'pynucleus.eval.golden_eval'\n",
        "        }\n",
        "        \n",
        "        for name, module_path in component_modules.items():\n",
        "            try:\n",
        "                module = __import__(module_path, fromlist=[''])\n",
        "                components[name] = {'loaded': True, 'module_path': module_path}\n",
        "                print(f\"   ✅ {name}: Available\")\n",
        "            except Exception as e:\n",
        "                components[name] = {'loaded': False, 'error': str(e)}\n",
        "                print(f\"   ❌ {name}: Error - {e}\")\n",
        "        \n",
        "        return components\n",
        "    \n",
        "    def check_data_structure(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze data directory structure and contents\"\"\"\n",
        "        print(\"\\n📁 Data Structure Deep Analysis:\")\n",
        "        \n",
        "        data_analysis = {}\n",
        "        \n",
        "        # Define expected directory structure\n",
        "        data_structure = {\n",
        "            'data/01_raw/source_documents': {'type': 'dir', 'description': 'Source documents'},\n",
        "            'data/01_raw/web_sources': {'type': 'dir', 'description': 'Web content'},\n",
        "            'data/03_intermediate/converted_chunked_data': {'type': 'dir', 'description': 'Processed chunks'},\n",
        "            'data/03_intermediate/vector_db': {'type': 'dir', 'description': 'ChromaDB storage'},\n",
        "            'data/04_models/chunk_reports': {'type': 'dir', 'description': 'Chunking analysis'},\n",
        "            'data/04_models/recall_evaluation': {'type': 'dir', 'description': 'Recall metrics'},\n",
        "            'data/validation/golden_dataset.csv': {'type': 'file', 'description': 'Golden dataset'},\n",
        "            'data/validation/results': {'type': 'dir', 'description': 'Validation results'}\n",
        "        }\n",
        "        \n",
        "        for path, info in data_structure.items():\n",
        "            path_obj = Path(path)\n",
        "            analysis = {\n",
        "                'exists': path_obj.exists(),\n",
        "                'type': info['type'],\n",
        "                'description': info['description']\n",
        "            }\n",
        "            \n",
        "            if path_obj.exists():\n",
        "                if path_obj.is_dir():\n",
        "                    files = list(path_obj.iterdir())\n",
        "                    analysis['file_count'] = len([f for f in files if f.is_file()])\n",
        "                    analysis['dir_count'] = len([d for d in files if d.is_dir()])\n",
        "                    analysis['total_size_mb'] = sum(\n",
        "                        f.stat().st_size for f in path_obj.rglob('*') if f.is_file()\n",
        "                    ) / (1024 * 1024)\n",
        "                    \n",
        "                    status = f\"✅ {analysis['file_count']} files, {analysis['dir_count']} dirs, {analysis['total_size_mb']:.1f} MB\"\n",
        "                else:\n",
        "                    analysis['size_mb'] = path_obj.stat().st_size / (1024 * 1024)\n",
        "                    status = f\"✅ File exists, {analysis['size_mb']:.1f} MB\"\n",
        "            else:\n",
        "                status = \"❌ Missing\"\n",
        "            \n",
        "            print(f\"   {status} - {path}\")\n",
        "            data_analysis[path] = analysis\n",
        "        \n",
        "        return data_analysis\n",
        "    \n",
        "    def run_comprehensive_diagnostic(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run all diagnostic checks\"\"\"\n",
        "        print(\"🚀 Running Comprehensive System Diagnostic...\")\n",
        "        \n",
        "        try:\n",
        "            self.diagnostics['resources'] = self.check_system_resources()\n",
        "            self.diagnostics['python_env'] = self.check_python_environment()\n",
        "            self.diagnostics['pynucleus'] = self.check_pynucleus_components()\n",
        "            self.diagnostics['data_structure'] = self.check_data_structure()\n",
        "            \n",
        "            # Performance summary\n",
        "            end_time = time.time()\n",
        "            duration = end_time - self.start_time\n",
        "            \n",
        "            print(f\"\\n📊 Diagnostic Summary:\")\n",
        "            print(f\"   • Total diagnostic time: {duration:.2f} seconds\")\n",
        "            print(f\"   • System status: {'✅ HEALTHY' if self._is_system_healthy() else '⚠️ ISSUES DETECTED'}\")\n",
        "            \n",
        "            self.diagnostics['summary'] = {\n",
        "                'duration_seconds': duration,\n",
        "                'healthy': self._is_system_healthy(),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            return self.diagnostics\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Diagnostic error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def _is_system_healthy(self) -> bool:\n",
        "        \"\"\"Determine if system is healthy based on checks\"\"\"\n",
        "        try:\n",
        "            # Check if core components loaded\n",
        "            pynucleus_healthy = self.diagnostics.get('pynucleus', {}).get('settings', {}).get('loaded', False)\n",
        "            \n",
        "            # Check if essential directories exist\n",
        "            data_healthy = any(\n",
        "                self.diagnostics.get('data_structure', {}).get(path, {}).get('exists', False)\n",
        "                for path in ['data/01_raw/source_documents', 'data/03_intermediate/vector_db']\n",
        "            )\n",
        "            \n",
        "            # Check resource usage isn't critical\n",
        "            resources = self.diagnostics.get('resources', {})\n",
        "            resource_healthy = (\n",
        "                resources.get('memory_used_percent', 0) < 90 and\n",
        "                resources.get('disk_used_percent', 0) < 95\n",
        "            )\n",
        "            \n",
        "            return pynucleus_healthy and resource_healthy\n",
        "            \n",
        "        except:\n",
        "            return False\n",
        "\n",
        "# Run comprehensive diagnostics\n",
        "diagnostics = SystemDiagnostics()\n",
        "diagnostic_results = diagnostics.run_comprehensive_diagnostic()\n",
        "\n",
        "print(f\"\\n🎯 Next Steps:\")\n",
        "print(\"   • 📊 Run Cell 2 for configuration management\")\n",
        "print(\"   • 🧪 Run Cell 3 for advanced testing\")\n",
        "print(\"   • 🔧 Check logs for detailed error information\")\n",
        "\n",
        "# Store results for other cells\n",
        "globals()['diagnostic_results'] = diagnostic_results\n",
        "globals()['system_diagnostics'] = diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Advanced Configuration Management & Environment Setup\n",
        "# ==============================================================\n",
        "# This cell provides dynamic configuration management for developers\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from typing import Dict, Any, Optional\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"⚙️ Advanced Configuration Management\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class ConfigurationManager:\n",
        "    \"\"\"Advanced configuration management and validation\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.config_history = []\n",
        "        self.backup_configs = {}\n",
        "        \n",
        "    def analyze_current_settings(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze current PyNucleus settings\"\"\"\n",
        "        print(\"\\n📋 Current Configuration Analysis:\")\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.settings import settings\n",
        "            \n",
        "            config_analysis = {}\n",
        "            \n",
        "            # Core settings\n",
        "            core_settings = {\n",
        "                'CHROMA_PATH': settings.CHROMA_PATH,\n",
        "                'MODEL_ID': settings.MODEL_ID,\n",
        "                'EMB_MODEL': settings.EMB_MODEL,\n",
        "                'MAX_TOKENS': settings.MAX_TOKENS,\n",
        "                'RETRIEVE_TOP_K': settings.RETRIEVE_TOP_K,\n",
        "                'USE_CUDA': settings.USE_CUDA,\n",
        "                'LOG_LEVEL': settings.LOG_LEVEL\n",
        "            }\n",
        "            \n",
        "            for key, value in core_settings.items():\n",
        "                print(f\"   • {key}: {value} ({type(value).__name__})\")\n",
        "                config_analysis[key] = {'value': value, 'type': type(value).__name__}\n",
        "            \n",
        "            # Validate paths\n",
        "            path_validation = self._validate_paths(settings)\n",
        "            config_analysis['path_validation'] = path_validation\n",
        "            \n",
        "            return config_analysis\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error analyzing settings: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def _validate_paths(self, settings) -> Dict[str, bool]:\n",
        "        \"\"\"Validate file and directory paths in settings\"\"\"\n",
        "        print(\"\\n🔍 Path Validation:\")\n",
        "        \n",
        "        paths_to_check = {\n",
        "            'CHROMA_PATH': settings.CHROMA_PATH,\n",
        "        }\n",
        "        \n",
        "        validation_results = {}\n",
        "        \n",
        "        for name, path in paths_to_check.items():\n",
        "            path_obj = Path(path)\n",
        "            exists = path_obj.exists()\n",
        "            validation_results[name] = exists\n",
        "            status = \"✅\" if exists else \"❌\"\n",
        "            print(f\"   {status} {name}: {path}\")\n",
        "        \n",
        "        return validation_results\n",
        "    \n",
        "    def create_environment_configs(self) -> Dict[str, str]:\n",
        "        \"\"\"Create environment-specific configuration files\"\"\"\n",
        "        print(\"\\n🏗️ Creating Environment Configurations:\")\n",
        "        \n",
        "        configs = {}\n",
        "        \n",
        "        # Development configuration\n",
        "        dev_config = {\n",
        "            'MODEL_ID': 'Qwen/Qwen1.5-0.5B-Chat',\n",
        "            'EMB_MODEL': 'all-MiniLM-L6-v2',\n",
        "            'MAX_TOKENS': 256,\n",
        "            'RETRIEVE_TOP_K': 4,\n",
        "            'USE_CUDA': False,\n",
        "            'LOG_LEVEL': 'DEBUG',\n",
        "            'CHROMA_PATH': 'data/03_intermediate/vector_db_dev'\n",
        "        }\n",
        "        \n",
        "        # Production configuration\n",
        "        prod_config = {\n",
        "            'MODEL_ID': 'Qwen/Qwen1.5-0.5B-Chat',\n",
        "            'EMB_MODEL': 'all-mpnet-base-v2',\n",
        "            'MAX_TOKENS': 512,\n",
        "            'RETRIEVE_TOP_K': 6,\n",
        "            'USE_CUDA': True,\n",
        "            'LOG_LEVEL': 'INFO',\n",
        "            'CHROMA_PATH': 'data/03_intermediate/vector_db_prod'\n",
        "        }\n",
        "        \n",
        "        # Testing configuration\n",
        "        test_config = {\n",
        "            'MODEL_ID': 'Qwen/Qwen1.5-0.5B-Chat',\n",
        "            'EMB_MODEL': 'all-MiniLM-L6-v2',\n",
        "            'MAX_TOKENS': 128,\n",
        "            'RETRIEVE_TOP_K': 2,\n",
        "            'USE_CUDA': False,\n",
        "            'LOG_LEVEL': 'WARNING',\n",
        "            'CHROMA_PATH': 'data/03_intermediate/vector_db_test'\n",
        "        }\n",
        "        \n",
        "        config_files = {\n",
        "            'development': dev_config,\n",
        "            'production': prod_config,\n",
        "            'testing': test_config\n",
        "        }\n",
        "        \n",
        "        # Save configuration files\n",
        "        configs_dir = Path('configs')\n",
        "        configs_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        for env_name, config in config_files.items():\n",
        "            config_file = configs_dir / f'{env_name}_config.json'\n",
        "            \n",
        "            with open(config_file, 'w') as f:\n",
        "                json.dump(config, f, indent=2)\n",
        "            \n",
        "            configs[env_name] = str(config_file)\n",
        "            print(f\"   ✅ Created {env_name} config: {config_file}\")\n",
        "        \n",
        "        return configs\n",
        "    \n",
        "    def validate_model_availability(self) -> Dict[str, bool]:\n",
        "        \"\"\"Check availability of different models\"\"\"\n",
        "        print(\"\\n🤖 Model Availability Check:\")\n",
        "        \n",
        "        models_to_check = [\n",
        "            'Qwen/Qwen1.5-0.5B-Chat',\n",
        "            'Qwen/Qwen1.5-1.8B-Chat',\n",
        "            'microsoft/DialoGPT-medium'\n",
        "        ]\n",
        "        \n",
        "        model_status = {}\n",
        "        \n",
        "        for model_id in models_to_check:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "                model_status[model_id] = True\n",
        "                print(f\"   ✅ {model_id}: Available\")\n",
        "            except Exception as e:\n",
        "                model_status[model_id] = False\n",
        "                print(f\"   ❌ {model_id}: Error - {str(e)[:50]}...\")\n",
        "        \n",
        "        return model_status\n",
        "    \n",
        "    def check_embedding_models(self) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Check embedding model performance and availability\"\"\"\n",
        "        print(\"\\n🧮 Embedding Model Analysis:\")\n",
        "        \n",
        "        embedding_models = [\n",
        "            'all-MiniLM-L6-v2',\n",
        "            'all-mpnet-base-v2',\n",
        "            'multi-qa-MiniLM-L6-cos-v1'\n",
        "        ]\n",
        "        \n",
        "        model_analysis = {}\n",
        "        \n",
        "        for model_name in embedding_models:\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                \n",
        "                start_time = time.time()\n",
        "                model = SentenceTransformer(model_name)\n",
        "                \n",
        "                # Test embedding\n",
        "                test_text = \"Chemical engineering process optimization\"\n",
        "                embedding = model.encode([test_text])\n",
        "                \n",
        "                load_time = time.time() - start_time\n",
        "                \n",
        "                analysis = {\n",
        "                    'available': True,\n",
        "                    'load_time_seconds': load_time,\n",
        "                    'embedding_dimension': embedding.shape[1],\n",
        "                    'model_size_mb': sum(\n",
        "                        p.numel() * p.element_size() \n",
        "                        for p in model.parameters()\n",
        "                    ) / (1024 * 1024)\n",
        "                }\n",
        "                \n",
        "                model_analysis[model_name] = analysis\n",
        "                print(f\"   ✅ {model_name}:\")\n",
        "                print(f\"      • Load time: {load_time:.2f}s\")\n",
        "                print(f\"      • Dimensions: {analysis['embedding_dimension']}\")\n",
        "                print(f\"      • Size: {analysis['model_size_mb']:.1f} MB\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                model_analysis[model_name] = {\n",
        "                    'available': False,\n",
        "                    'error': str(e)\n",
        "                }\n",
        "                print(f\"   ❌ {model_name}: Error - {str(e)[:50]}...\")\n",
        "        \n",
        "        return model_analysis\n",
        "    \n",
        "    def export_diagnostic_report(self, diagnostic_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Export comprehensive diagnostic report\"\"\"\n",
        "        print(\"\\n📄 Exporting Diagnostic Report:\")\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'system_info': diagnostic_data,\n",
        "            'configuration': self.analyze_current_settings(),\n",
        "            'model_availability': self.validate_model_availability(),\n",
        "            'embedding_analysis': self.check_embedding_models()\n",
        "        }\n",
        "        \n",
        "        report_file = f\"diagnostic_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        \n",
        "        with open(report_file, 'w') as f:\n",
        "            json.dump(report, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"   ✅ Report exported: {report_file}\")\n",
        "        print(f\"   📊 Report size: {Path(report_file).stat().st_size / 1024:.1f} KB\")\n",
        "        \n",
        "        return report_file\n",
        "\n",
        "# Run configuration management\n",
        "if 'diagnostic_results' in globals():\n",
        "    config_manager = ConfigurationManager()\n",
        "    \n",
        "    # Analyze current configuration\n",
        "    current_config = config_manager.analyze_current_settings()\n",
        "    \n",
        "    # Create environment configs\n",
        "    env_configs = config_manager.create_environment_configs()\n",
        "    \n",
        "    # Check model availability\n",
        "    model_status = config_manager.validate_model_availability()\n",
        "    \n",
        "    # Analyze embedding models\n",
        "    embedding_analysis = config_manager.check_embedding_models()\n",
        "    \n",
        "    # Export comprehensive report\n",
        "    if diagnostic_results:\n",
        "        report_file = config_manager.export_diagnostic_report(diagnostic_results)\n",
        "    \n",
        "    print(f\"\\n🎯 Configuration Management Complete!\")\n",
        "    print(f\"   • Environment configs created: {len(env_configs)}\")\n",
        "    print(f\"   • Models checked: {len(model_status)}\")\n",
        "    print(f\"   • Embedding models analyzed: {len(embedding_analysis)}\")\n",
        "    \n",
        "    # Store for next cells\n",
        "    globals()['config_manager'] = config_manager\n",
        "    globals()['current_config'] = current_config\n",
        "    globals()['model_status'] = model_status\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Please run Cell 1 (System Diagnostics) first to get diagnostic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Advanced Testing & Evaluation Suite\n",
        "# =============================================\n",
        "# This cell provides comprehensive testing and evaluation capabilities\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"🧪 Advanced Testing & Evaluation Suite\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class AdvancedEvaluator:\n",
        "    \"\"\"Comprehensive evaluation and testing framework\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.test_results = {}\n",
        "        self.evaluation_history = []\n",
        "        \n",
        "    def load_golden_dataset(self) -> pd.DataFrame:\n",
        "        \"\"\"Load and analyze golden dataset\"\"\"\n",
        "        print(\"\\n📄 Golden Dataset Analysis:\")\n",
        "        \n",
        "        try:\n",
        "            dataset_path = Path(\"data/validation/golden_dataset.csv\")\n",
        "            if not dataset_path.exists():\n",
        "                print(f\"   ❌ Golden dataset not found at {dataset_path}\")\n",
        "                return pd.DataFrame()\n",
        "            \n",
        "            df = pd.read_csv(dataset_path)\n",
        "            \n",
        "            print(f\"   ✅ Dataset loaded: {len(df)} questions\")\n",
        "            print(f\"   📊 Domains: {list(df['domain'].unique())}\")\n",
        "            print(f\"   📊 Difficulties: {list(df['difficulty'].unique())}\")\n",
        "            \n",
        "            # Analyze dataset distribution\n",
        "            domain_counts = df['domain'].value_counts()\n",
        "            difficulty_counts = df['difficulty'].value_counts()\n",
        "            \n",
        "            print(f\"\\n   📈 Domain Distribution:\")\n",
        "            for domain, count in domain_counts.items():\n",
        "                print(f\"      • {domain}: {count} questions ({count/len(df)*100:.1f}%)\")\n",
        "            \n",
        "            print(f\"\\n   📈 Difficulty Distribution:\")\n",
        "            for difficulty, count in difficulty_counts.items():\n",
        "                print(f\"      • {difficulty}: {count} questions ({count/len(df)*100:.1f}%)\")\n",
        "            \n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error loading dataset: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "    def run_detailed_evaluation(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Run detailed evaluation with per-question analysis\"\"\"\n",
        "        print(\"\\n🔍 Running Detailed Evaluation:\")\n",
        "        \n",
        "        if df.empty:\n",
        "            print(\"   ⚠️ No dataset available for evaluation\")\n",
        "            return {}\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.rag.engine import ask\n",
        "            from pynucleus.eval.golden_eval import evaluate_answer\n",
        "            \n",
        "            results = []\n",
        "            \n",
        "            print(f\"   ⏳ Evaluating {len(df)} questions...\")\n",
        "            \n",
        "            for idx, row in df.iterrows():\n",
        "                question = row['question']\n",
        "                expected_keywords = row['expected_answer'].split(',') if pd.notna(row['expected_answer']) else []\n",
        "                domain = row['domain']\n",
        "                difficulty = row['difficulty']\n",
        "                \n",
        "                try:\n",
        "                    # Get answer from system\n",
        "                    start_time = time.time()\n",
        "                    result = ask(question)\n",
        "                    response_time = time.time() - start_time\n",
        "                    \n",
        "                    answer = result.get('answer', '')\n",
        "                    sources = result.get('sources', [])\n",
        "                    \n",
        "                    # Evaluate answer quality\n",
        "                    keyword_matches = sum(1 for keyword in expected_keywords \n",
        "                                        if keyword.lower().strip() in answer.lower())\n",
        "                    keyword_score = keyword_matches / len(expected_keywords) if expected_keywords else 0\n",
        "                    \n",
        "                    # Additional metrics\n",
        "                    answer_length = len(answer)\n",
        "                    source_count = len(sources)\n",
        "                    has_sources = source_count > 0\n",
        "                    \n",
        "                    question_result = {\n",
        "                        'question_id': idx,\n",
        "                        'question': question,\n",
        "                        'domain': domain,\n",
        "                        'difficulty': difficulty,\n",
        "                        'answer': answer,\n",
        "                        'answer_length': answer_length,\n",
        "                        'keyword_score': keyword_score,\n",
        "                        'keyword_matches': keyword_matches,\n",
        "                        'total_keywords': len(expected_keywords),\n",
        "                        'source_count': source_count,\n",
        "                        'has_sources': has_sources,\n",
        "                        'response_time': response_time,\n",
        "                        'success': keyword_score >= 0.6\n",
        "                    }\n",
        "                    \n",
        "                    results.append(question_result)\n",
        "                    \n",
        "                    # Progress indicator\n",
        "                    if (idx + 1) % 5 == 0:\n",
        "                        print(f\"      • Completed {idx + 1}/{len(df)} questions...\")\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(f\"      ❌ Error with question {idx}: {e}\")\n",
        "                    results.append({\n",
        "                        'question_id': idx,\n",
        "                        'question': question,\n",
        "                        'domain': domain,\n",
        "                        'difficulty': difficulty,\n",
        "                        'error': str(e),\n",
        "                        'success': False\n",
        "                    })\n",
        "            \n",
        "            # Analyze results\n",
        "            results_df = pd.DataFrame(results)\n",
        "            analysis = self._analyze_evaluation_results(results_df)\n",
        "            \n",
        "            return {\n",
        "                'results': results_df,\n",
        "                'analysis': analysis,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Evaluation error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def _analyze_evaluation_results(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze evaluation results in detail\"\"\"\n",
        "        print(\"\\n📊 Evaluation Results Analysis:\")\n",
        "        \n",
        "        if results_df.empty:\n",
        "            return {}\n",
        "        \n",
        "        # Overall metrics\n",
        "        total_questions = len(results_df)\n",
        "        successful_questions = results_df['success'].sum() if 'success' in results_df.columns else 0\n",
        "        overall_accuracy = successful_questions / total_questions if total_questions > 0 else 0\n",
        "        \n",
        "        print(f\"   📈 Overall Performance:\")\n",
        "        print(f\"      • Accuracy: {overall_accuracy:.1%} ({successful_questions}/{total_questions})\")\n",
        "        \n",
        "        # Performance by domain\n",
        "        if 'domain' in results_df.columns:\n",
        "            domain_performance = results_df.groupby('domain')['success'].agg(['count', 'sum', 'mean']).round(3)\n",
        "            print(f\"\\n   📈 Performance by Domain:\")\n",
        "            for domain, stats in domain_performance.iterrows():\n",
        "                accuracy = stats['mean']\n",
        "                print(f\"      • {domain}: {accuracy:.1%} ({stats['sum']:.0f}/{stats['count']:.0f})\")\n",
        "        \n",
        "        # Performance by difficulty\n",
        "        if 'difficulty' in results_df.columns:\n",
        "            difficulty_performance = results_df.groupby('difficulty')['success'].agg(['count', 'sum', 'mean']).round(3)\n",
        "            print(f\"\\n   📈 Performance by Difficulty:\")\n",
        "            for difficulty, stats in difficulty_performance.iterrows():\n",
        "                accuracy = stats['mean']\n",
        "                print(f\"      • {difficulty}: {accuracy:.1%} ({stats['sum']:.0f}/{stats['count']:.0f})\")\n",
        "        \n",
        "        # Response time analysis\n",
        "        if 'response_time' in results_df.columns:\n",
        "            avg_response_time = results_df['response_time'].mean()\n",
        "            min_response_time = results_df['response_time'].min()\n",
        "            max_response_time = results_df['response_time'].max()\n",
        "            \n",
        "            print(f\"\\n   ⏱️ Response Time Analysis:\")\n",
        "            print(f\"      • Average: {avg_response_time:.2f}s\")\n",
        "            print(f\"      • Min: {min_response_time:.2f}s\")\n",
        "            print(f\"      • Max: {max_response_time:.2f}s\")\n",
        "        \n",
        "        # Quality metrics\n",
        "        if 'keyword_score' in results_df.columns:\n",
        "            avg_keyword_score = results_df['keyword_score'].mean()\n",
        "            print(f\"\\n   📝 Answer Quality:\")\n",
        "            print(f\"      • Average keyword score: {avg_keyword_score:.1%}\")\n",
        "        \n",
        "        analysis = {\n",
        "            'overall_accuracy': overall_accuracy,\n",
        "            'total_questions': total_questions,\n",
        "            'successful_questions': successful_questions,\n",
        "            'domain_performance': domain_performance.to_dict() if 'domain' in results_df.columns else {},\n",
        "            'difficulty_performance': difficulty_performance.to_dict() if 'difficulty' in results_df.columns else {},\n",
        "            'avg_response_time': avg_response_time if 'response_time' in results_df.columns else None,\n",
        "            'avg_keyword_score': avg_keyword_score if 'keyword_score' in results_df.columns else None\n",
        "        }\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def run_retrieval_quality_test(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test retrieval quality and relevance\"\"\"\n",
        "        print(\"\\n🔍 Retrieval Quality Assessment:\")\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            \n",
        "            test_queries = [\n",
        "                \"modular chemical plants\",\n",
        "                \"distillation process\",\n",
        "                \"reactor design\",\n",
        "                \"heat transfer\",\n",
        "                \"process safety\",\n",
        "                \"chemical engineering optimization\"\n",
        "            ]\n",
        "            \n",
        "            retrieval_results = []\n",
        "            \n",
        "            for query in test_queries:\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    docs = retrieve(query, top_k=5)\n",
        "                    retrieval_time = time.time() - start_time\n",
        "                    \n",
        "                    if docs:\n",
        "                        avg_doc_length = np.mean([len(doc) for doc in docs])\n",
        "                        doc_count = len(docs)\n",
        "                        \n",
        "                        # Simple relevance check (keyword overlap)\n",
        "                        query_words = set(query.lower().split())\n",
        "                        relevance_scores = []\n",
        "                        \n",
        "                        for doc in docs:\n",
        "                            doc_words = set(doc.lower().split())\n",
        "                            overlap = len(query_words.intersection(doc_words))\n",
        "                            relevance = overlap / len(query_words) if query_words else 0\n",
        "                            relevance_scores.append(relevance)\n",
        "                        \n",
        "                        avg_relevance = np.mean(relevance_scores)\n",
        "                        \n",
        "                        retrieval_results.append({\n",
        "                            'query': query,\n",
        "                            'doc_count': doc_count,\n",
        "                            'avg_doc_length': avg_doc_length,\n",
        "                            'avg_relevance': avg_relevance,\n",
        "                            'retrieval_time': retrieval_time,\n",
        "                            'success': True\n",
        "                        })\n",
        "                        \n",
        "                        print(f\"   ✅ '{query}': {doc_count} docs, {avg_relevance:.2f} relevance\")\n",
        "                    else:\n",
        "                        retrieval_results.append({\n",
        "                            'query': query,\n",
        "                            'success': False,\n",
        "                            'error': 'No documents retrieved'\n",
        "                        })\n",
        "                        print(f\"   ❌ '{query}': No documents retrieved\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    retrieval_results.append({\n",
        "                        'query': query,\n",
        "                        'success': False,\n",
        "                        'error': str(e)\n",
        "                    })\n",
        "                    print(f\"   ❌ '{query}': Error - {e}\")\n",
        "            \n",
        "            return {\n",
        "                'retrieval_results': retrieval_results,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Retrieval test error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def save_evaluation_results(self, results: Dict[str, Any], filename: str = None) -> str:\n",
        "        \"\"\"Save evaluation results to file\"\"\"\n",
        "        if not filename:\n",
        "            filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        \n",
        "        filepath = Path(\"data/validation/results\") / filename\n",
        "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Convert DataFrames to dict for JSON serialization\n",
        "        serializable_results = results.copy()\n",
        "        if 'results' in serializable_results and hasattr(serializable_results['results'], 'to_dict'):\n",
        "            serializable_results['results'] = serializable_results['results'].to_dict('records')\n",
        "        \n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"\\n💾 Results saved: {filepath}\")\n",
        "        return str(filepath)\n",
        "\n",
        "# Run advanced evaluation\n",
        "if 'system_diagnostics' in globals():\n",
        "    evaluator = AdvancedEvaluator()\n",
        "    \n",
        "    # Load golden dataset\n",
        "    golden_df = evaluator.load_golden_dataset()\n",
        "    \n",
        "    if not golden_df.empty:\n",
        "        # Run detailed evaluation\n",
        "        print(\"\\n🚀 Starting comprehensive evaluation...\")\n",
        "        evaluation_results = evaluator.run_detailed_evaluation(golden_df)\n",
        "        \n",
        "        # Run retrieval quality test\n",
        "        retrieval_results = evaluator.run_retrieval_quality_test()\n",
        "        \n",
        "        # Save results\n",
        "        if evaluation_results:\n",
        "            results_file = evaluator.save_evaluation_results({\n",
        "                'evaluation': evaluation_results,\n",
        "                'retrieval_quality': retrieval_results\n",
        "            })\n",
        "        \n",
        "        print(f\"\\n✅ Advanced evaluation complete!\")\n",
        "        \n",
        "        # Store for next cells\n",
        "        globals()['evaluator'] = evaluator\n",
        "        globals()['evaluation_results'] = evaluation_results\n",
        "        globals()['retrieval_results'] = retrieval_results\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Please run Cell 1 (System Diagnostics) first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Performance Profiling & Optimization\n",
        "# ==============================================\n",
        "# This cell provides detailed performance analysis and optimization recommendations\n",
        "\n",
        "import time\n",
        "import gc\n",
        "import memory_profiler\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"📊 Performance Profiling & Optimization\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class PerformanceProfiler:\n",
        "    \"\"\"Advanced performance profiling and optimization\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.profile_results = {}\n",
        "        \n",
        "    def profile_model_loading(self) -> Dict[str, Any]:\n",
        "        \"\"\"Profile model loading performance\"\"\"\n",
        "        print(\"\\n🤖 Model Loading Performance:\")\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.llm.qwen_loader import QwenLoader\n",
        "            \n",
        "            # Profile Qwen model loading\n",
        "            start_time = time.time()\n",
        "            start_memory = memory_profiler.memory_usage()[0]\n",
        "            \n",
        "            loader = QwenLoader()\n",
        "            model, tokenizer = loader.load_model()\n",
        "            \n",
        "            end_time = time.time()\n",
        "            end_memory = memory_profiler.memory_usage()[0]\n",
        "            \n",
        "            loading_time = end_time - start_time\n",
        "            memory_usage = end_memory - start_memory\n",
        "            \n",
        "            print(f\"   • Model loading time: {loading_time:.2f} seconds\")\n",
        "            print(f\"   • Memory usage: {memory_usage:.1f} MB\")\n",
        "            \n",
        "            # Test inference speed\n",
        "            test_prompt = \"What is chemical engineering?\"\n",
        "            \n",
        "            start_time = time.time()\n",
        "            response = loader.generate(test_prompt, max_tokens=50)\n",
        "            inference_time = time.time() - start_time\n",
        "            \n",
        "            tokens_generated = len(response.split())\n",
        "            tokens_per_second = tokens_generated / inference_time if inference_time > 0 else 0\n",
        "            \n",
        "            print(f\"   • First inference time: {inference_time:.2f} seconds\")\n",
        "            print(f\"   • Tokens per second: {tokens_per_second:.1f}\")\n",
        "            \n",
        "            # Test subsequent inference (warm)\n",
        "            start_time = time.time()\n",
        "            response2 = loader.generate(\"Another test prompt\", max_tokens=50)\n",
        "            warm_inference_time = time.time() - start_time\n",
        "            \n",
        "            print(f\"   • Warm inference time: {warm_inference_time:.2f} seconds\")\n",
        "            print(f\"   • Speedup: {inference_time / warm_inference_time:.1f}x\")\n",
        "            \n",
        "            return {\n",
        "                'loading_time': loading_time,\n",
        "                'memory_usage_mb': memory_usage,\n",
        "                'cold_inference_time': inference_time,\n",
        "                'warm_inference_time': warm_inference_time,\n",
        "                'tokens_per_second': tokens_per_second\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Model profiling error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def profile_embedding_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Profile embedding model performance\"\"\"\n",
        "        print(\"\\n🧮 Embedding Performance Analysis:\")\n",
        "        \n",
        "        try:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            \n",
        "            embedding_models = ['all-MiniLM-L6-v2', 'all-mpnet-base-v2']\n",
        "            test_texts = [\n",
        "                \"Chemical engineering process optimization\",\n",
        "                \"Distillation column design and operation\", \n",
        "                \"Reactor kinetics and mass transfer\",\n",
        "                \"Heat exchanger efficiency analysis\",\n",
        "                \"Process safety and risk assessment\"\n",
        "            ]\n",
        "            \n",
        "            model_performance = {}\n",
        "            \n",
        "            for model_name in embedding_models:\n",
        "                try:\n",
        "                    # Load model\n",
        "                    start_time = time.time()\n",
        "                    model = SentenceTransformer(model_name)\n",
        "                    load_time = time.time() - start_time\n",
        "                    \n",
        "                    # Profile embedding generation\n",
        "                    start_time = time.time()\n",
        "                    embeddings = model.encode(test_texts)\n",
        "                    encoding_time = time.time() - start_time\n",
        "                    \n",
        "                    # Calculate metrics\n",
        "                    texts_per_second = len(test_texts) / encoding_time\n",
        "                    avg_text_length = sum(len(text) for text in test_texts) / len(test_texts)\n",
        "                    \n",
        "                    model_performance[model_name] = {\n",
        "                        'load_time': load_time,\n",
        "                        'encoding_time': encoding_time,\n",
        "                        'texts_per_second': texts_per_second,\n",
        "                        'embedding_dimension': embeddings.shape[1],\n",
        "                        'avg_text_length': avg_text_length\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"   ✅ {model_name}:\")\n",
        "                    print(f\"      • Load time: {load_time:.2f}s\")\n",
        "                    print(f\"      • Encoding time: {encoding_time:.3f}s\") \n",
        "                    print(f\"      • Texts/second: {texts_per_second:.1f}\")\n",
        "                    print(f\"      • Dimensions: {embeddings.shape[1]}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ {model_name}: Error - {e}\")\n",
        "                    model_performance[model_name] = {'error': str(e)}\n",
        "            \n",
        "            return model_performance\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Embedding profiling error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def profile_retrieval_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Profile document retrieval performance\"\"\"\n",
        "        print(\"\\n🔍 Retrieval Performance Analysis:\")\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            \n",
        "            test_queries = [\n",
        "                \"modular chemical plants\",\n",
        "                \"distillation efficiency\", \n",
        "                \"reactor design optimization\",\n",
        "                \"heat transfer mechanisms\",\n",
        "                \"process safety protocols\"\n",
        "            ]\n",
        "            \n",
        "            retrieval_metrics = []\n",
        "            \n",
        "            for query in test_queries:\n",
        "                try:\n",
        "                    # Test different top_k values\n",
        "                    for top_k in [1, 3, 5, 10]:\n",
        "                        start_time = time.time()\n",
        "                        docs = retrieve(query, top_k=top_k)\n",
        "                        retrieval_time = time.time() - start_time\n",
        "                        \n",
        "                        if docs:\n",
        "                            avg_doc_length = sum(len(doc) for doc in docs) / len(docs)\n",
        "                            total_length = sum(len(doc) for doc in docs)\n",
        "                        else:\n",
        "                            avg_doc_length = 0\n",
        "                            total_length = 0\n",
        "                        \n",
        "                        retrieval_metrics.append({\n",
        "                            'query': query,\n",
        "                            'top_k': top_k,\n",
        "                            'retrieval_time': retrieval_time,\n",
        "                            'docs_returned': len(docs) if docs else 0,\n",
        "                            'avg_doc_length': avg_doc_length,\n",
        "                            'total_length': total_length\n",
        "                        })\n",
        "                        \n",
        "                        print(f\"   📝 '{query}' (k={top_k}): {retrieval_time:.3f}s, {len(docs) if docs else 0} docs\")\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Query '{query}': Error - {e}\")\n",
        "            \n",
        "            # Analyze performance patterns\n",
        "            if retrieval_metrics:\n",
        "                avg_time_by_k = {}\n",
        "                for metric in retrieval_metrics:\n",
        "                    k = metric['top_k']\n",
        "                    if k not in avg_time_by_k:\n",
        "                        avg_time_by_k[k] = []\n",
        "                    avg_time_by_k[k].append(metric['retrieval_time'])\n",
        "                \n",
        "                print(f\"\\n   📊 Average retrieval time by top_k:\")\n",
        "                for k, times in avg_time_by_k.items():\n",
        "                    avg_time = sum(times) / len(times)\n",
        "                    print(f\"      • k={k}: {avg_time:.3f}s\")\n",
        "            \n",
        "            return {\n",
        "                'retrieval_metrics': retrieval_metrics,\n",
        "                'avg_time_by_k': {k: sum(times)/len(times) for k, times in avg_time_by_k.items()}\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Retrieval profiling error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def profile_end_to_end_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Profile complete question-answering pipeline\"\"\"\n",
        "        print(\"\\n🚀 End-to-End Pipeline Performance:\")\n",
        "        \n",
        "        try:\n",
        "            from pynucleus.rag.engine import ask\n",
        "            \n",
        "            test_questions = [\n",
        "                \"What are the advantages of modular chemical plants?\",\n",
        "                \"How does distillation work in chemical processes?\",\n",
        "                \"What factors affect reactor efficiency?\"\n",
        "            ]\n",
        "            \n",
        "            pipeline_metrics = []\n",
        "            \n",
        "            for question in test_questions:\n",
        "                try:\n",
        "                    # Profile complete pipeline\n",
        "                    start_time = time.time()\n",
        "                    result = ask(question)\n",
        "                    total_time = time.time() - start_time\n",
        "                    \n",
        "                    answer = result.get('answer', '')\n",
        "                    sources = result.get('sources', [])\n",
        "                    \n",
        "                    metrics = {\n",
        "                        'question': question,\n",
        "                        'total_time': total_time,\n",
        "                        'answer_length': len(answer),\n",
        "                        'source_count': len(sources),\n",
        "                        'words_per_second': len(answer.split()) / total_time if total_time > 0 else 0\n",
        "                    }\n",
        "                    \n",
        "                    pipeline_metrics.append(metrics)\n",
        "                    \n",
        "                    print(f\"   ✅ Question: {question[:50]}...\")\n",
        "                    print(f\"      • Total time: {total_time:.2f}s\")\n",
        "                    print(f\"      • Answer length: {len(answer)} chars\")\n",
        "                    print(f\"      • Sources used: {len(sources)}\")\n",
        "                    print(f\"      • Words/second: {metrics['words_per_second']:.1f}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"   ❌ Question error: {e}\")\n",
        "                    pipeline_metrics.append({\n",
        "                        'question': question,\n",
        "                        'error': str(e)\n",
        "                    })\n",
        "            \n",
        "            # Calculate summary statistics\n",
        "            if pipeline_metrics:\n",
        "                valid_metrics = [m for m in pipeline_metrics if 'total_time' in m]\n",
        "                if valid_metrics:\n",
        "                    avg_time = sum(m['total_time'] for m in valid_metrics) / len(valid_metrics)\n",
        "                    avg_answer_length = sum(m['answer_length'] for m in valid_metrics) / len(valid_metrics)\n",
        "                    \n",
        "                    print(f\"\\n   📊 Pipeline Summary:\")\n",
        "                    print(f\"      • Average response time: {avg_time:.2f}s\")\n",
        "                    print(f\"      • Average answer length: {avg_answer_length:.0f} chars\")\n",
        "            \n",
        "            return {\n",
        "                'pipeline_metrics': pipeline_metrics,\n",
        "                'summary': {\n",
        "                    'avg_response_time': avg_time if 'avg_time' in locals() else None,\n",
        "                    'avg_answer_length': avg_answer_length if 'avg_answer_length' in locals() else None\n",
        "                }\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ End-to-end profiling error: {e}\")\n",
        "            return {'error': str(e)}\n",
        "    \n",
        "    def generate_optimization_recommendations(self, profile_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Generate performance optimization recommendations\"\"\"\n",
        "        print(\"\\n💡 Performance Optimization Recommendations:\")\n",
        "        \n",
        "        recommendations = []\n",
        "        \n",
        "        # Model loading optimizations\n",
        "        if 'model_loading' in profile_data:\n",
        "            loading_data = profile_data['model_loading']\n",
        "            if 'loading_time' in loading_data and loading_data['loading_time'] > 10:\n",
        "                recommendations.append(\"🤖 Consider model caching or keeping model loaded in memory\")\n",
        "            \n",
        "            if 'memory_usage_mb' in loading_data and loading_data['memory_usage_mb'] > 2000:\n",
        "                recommendations.append(\"💾 Consider using a smaller model or quantization for memory efficiency\")\n",
        "        \n",
        "        # Embedding optimizations\n",
        "        if 'embedding_performance' in profile_data:\n",
        "            for model, perf in profile_data['embedding_performance'].items():\n",
        "                if 'texts_per_second' in perf and perf['texts_per_second'] < 5:\n",
        "                    recommendations.append(f\"🧮 Consider switching from {model} to a faster embedding model\")\n",
        "        \n",
        "        # Retrieval optimizations\n",
        "        if 'retrieval_performance' in profile_data:\n",
        "            avg_times = profile_data['retrieval_performance'].get('avg_time_by_k', {})\n",
        "            if 5 in avg_times and 10 in avg_times:\n",
        "                if avg_times[10] > avg_times[5] * 1.5:\n",
        "                    recommendations.append(\"🔍 Consider using lower top_k values for better performance\")\n",
        "        \n",
        "        # General recommendations\n",
        "        recommendations.extend([\n",
        "            \"⚡ Use GPU acceleration if available (set USE_CUDA=True)\",\n",
        "            \"🗄️ Ensure ChromaDB is using persistent storage for better caching\",\n",
        "            \"📊 Monitor memory usage and implement cleanup for long-running sessions\",\n",
        "            \"🔧 Consider batch processing for multiple queries\",\n",
        "            \"📈 Profile with production-sized document collections\"\n",
        "        ])\n",
        "        \n",
        "        for i, rec in enumerate(recommendations, 1):\n",
        "            print(f\"   {i}. {rec}\")\n",
        "        \n",
        "        return recommendations\n",
        "\n",
        "# Run performance profiling\n",
        "if 'config_manager' in globals():\n",
        "    profiler = PerformanceProfiler()\n",
        "    \n",
        "    print(\"🚀 Starting comprehensive performance profiling...\")\n",
        "    \n",
        "    # Profile different components\n",
        "    model_perf = profiler.profile_model_loading()\n",
        "    embedding_perf = profiler.profile_embedding_performance()\n",
        "    retrieval_perf = profiler.profile_retrieval_performance()\n",
        "    pipeline_perf = profiler.profile_end_to_end_performance()\n",
        "    \n",
        "    # Collect all performance data\n",
        "    performance_data = {\n",
        "        'model_loading': model_perf,\n",
        "        'embedding_performance': embedding_perf,\n",
        "        'retrieval_performance': retrieval_perf,\n",
        "        'pipeline_performance': pipeline_perf,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Generate recommendations\n",
        "    recommendations = profiler.generate_optimization_recommendations(performance_data)\n",
        "    \n",
        "    print(f\"\\n✅ Performance profiling complete!\")\n",
        "    \n",
        "    # Store for next cells\n",
        "    globals()['profiler'] = profiler\n",
        "    globals()['performance_data'] = performance_data\n",
        "    globals()['optimization_recommendations'] = recommendations\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Please run previous cells first to initialize the system.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Debug Tools & Troubleshooting\n",
        "# ======================================\n",
        "# This cell provides advanced debugging and troubleshooting capabilities\n",
        "\n",
        "import traceback\n",
        "import logging\n",
        "import sys\n",
        "from typing import Dict, List, Any\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"🔧 Debug Tools & Troubleshooting\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class DebugToolkit:\n",
        "    \"\"\"Advanced debugging and troubleshooting tools\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.debug_logs = []\n",
        "        self.error_history = []\n",
        "        \n",
        "    def setup_detailed_logging(self) -> None:\n",
        "        \"\"\"Setup detailed logging for debugging\"\"\"\n",
        "        print(\"\\n📝 Setting up detailed logging:\")\n",
        "        \n",
        "        # Create logs directory\n",
        "        logs_dir = Path(\"logs\")\n",
        "        logs_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Configure detailed logging\n",
        "        log_file = logs_dir / f\"debug_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "        \n",
        "        logging.basicConfig(\n",
        "            level=logging.DEBUG,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler(sys.stdout)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        print(f\"   ✅ Debug logging enabled: {log_file}\")\n",
        "        \n",
        "        # Test logging\n",
        "        logger = logging.getLogger(\"PyNucleus.Debug\")\n",
        "        logger.debug(\"Debug logging system initialized\")\n",
        "        logger.info(\"Ready for detailed debugging\")\n",
        "    \n",
        "    def diagnose_import_issues(self) -> Dict[str, Any]:\n",
        "        \"\"\"Diagnose common import and module issues\"\"\"\n",
        "        print(\"\\n🐍 Diagnosing Import Issues:\")\n",
        "        \n",
        "        import_diagnosis = {\n",
        "            'python_path': sys.path.copy(),\n",
        "            'current_directory': str(Path.cwd()),\n",
        "            'module_issues': {}\n",
        "        }\n",
        "        \n",
        "        # Test critical imports\n",
        "        critical_modules = [\n",
        "            'pynucleus.settings',\n",
        "            'pynucleus.rag.engine', \n",
        "            'pynucleus.rag.collector',\n",
        "            'pynucleus.llm.qwen_loader',\n",
        "            'pynucleus.eval.golden_eval'\n",
        "        ]\n",
        "        \n",
        "        for module_name in critical_modules:\n",
        "            try:\n",
        "                module = __import__(module_name, fromlist=[''])\n",
        "                import_diagnosis['module_issues'][module_name] = {\n",
        "                    'status': 'success',\n",
        "                    'path': getattr(module, '__file__', 'unknown')\n",
        "                }\n",
        "                print(f\"   ✅ {module_name}: OK\")\n",
        "            except ImportError as e:\n",
        "                import_diagnosis['module_issues'][module_name] = {\n",
        "                    'status': 'import_error',\n",
        "                    'error': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                }\n",
        "                print(f\"   ❌ {module_name}: ImportError - {e}\")\n",
        "            except Exception as e:\n",
        "                import_diagnosis['module_issues'][module_name] = {\n",
        "                    'status': 'other_error',\n",
        "                    'error': str(e),\n",
        "                    'traceback': traceback.format_exc()\n",
        "                }\n",
        "                print(f\"   ⚠️ {module_name}: Error - {e}\")\n",
        "        \n",
        "        # Check if src is in path\n",
        "        src_path = Path.cwd() / \"src\"\n",
        "        if str(src_path) not in sys.path:\n",
        "            print(f\"   ⚠️ Warning: {src_path} not in Python path\")\n",
        "            import_diagnosis['warnings'] = [f\"src directory not in Python path: {src_path}\"]\n",
        "        \n",
        "        return import_diagnosis\n",
        "    \n",
        "    def test_core_functionality(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test core PyNucleus functionality with detailed error reporting\"\"\"\n",
        "        print(\"\\n🧪 Testing Core Functionality:\")\n",
        "        \n",
        "        test_results = {}\n",
        "        \n",
        "        # Test 1: Settings loading\n",
        "        try:\n",
        "            from pynucleus.settings import settings\n",
        "            test_results['settings'] = {\n",
        "                'status': 'success',\n",
        "                'chroma_path': settings.CHROMA_PATH,\n",
        "                'model_id': settings.MODEL_ID\n",
        "            }\n",
        "            print(\"   ✅ Settings: Loaded successfully\")\n",
        "        except Exception as e:\n",
        "            test_results['settings'] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            }\n",
        "            print(f\"   ❌ Settings: Error - {e}\")\n",
        "        \n",
        "        # Test 2: Vector database connection\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            test_docs = retrieve(\"test query\", top_k=1)\n",
        "            test_results['vector_db'] = {\n",
        "                'status': 'success',\n",
        "                'docs_returned': len(test_docs) if test_docs else 0\n",
        "            }\n",
        "            print(f\"   ✅ Vector DB: Connected, {len(test_docs) if test_docs else 0} docs found\")\n",
        "        except Exception as e:\n",
        "            test_results['vector_db'] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            }\n",
        "            print(f\"   ❌ Vector DB: Error - {e}\")\n",
        "        \n",
        "        # Test 3: Model loading\n",
        "        try:\n",
        "            from pynucleus.llm.qwen_loader import generate\n",
        "            test_response = generate(\"test\", max_tokens=5)\n",
        "            test_results['model'] = {\n",
        "                'status': 'success',\n",
        "                'response_length': len(test_response) if test_response else 0\n",
        "            }\n",
        "            print(\"   ✅ Model: Loaded and responding\")\n",
        "        except Exception as e:\n",
        "            test_results['model'] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            }\n",
        "            print(f\"   ❌ Model: Error - {e}\")\n",
        "        \n",
        "        # Test 4: End-to-end pipeline\n",
        "        try:\n",
        "            from pynucleus.rag.engine import ask\n",
        "            result = ask(\"What is chemical engineering?\")\n",
        "            test_results['pipeline'] = {\n",
        "                'status': 'success',\n",
        "                'answer_length': len(result.get('answer', '')) if result else 0,\n",
        "                'sources_count': len(result.get('sources', [])) if result else 0\n",
        "            }\n",
        "            print(\"   ✅ Pipeline: End-to-end test successful\")\n",
        "        except Exception as e:\n",
        "            test_results['pipeline'] = {\n",
        "                'status': 'error', \n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            }\n",
        "            print(f\"   ❌ Pipeline: Error - {e}\")\n",
        "        \n",
        "        return test_results\n",
        "    \n",
        "    def analyze_error_patterns(self, test_results: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Analyze error patterns and provide specific troubleshooting steps\"\"\"\n",
        "        print(\"\\n🔍 Error Pattern Analysis:\")\n",
        "        \n",
        "        troubleshooting_steps = []\n",
        "        \n",
        "        # Check for common error patterns\n",
        "        for component, result in test_results.items():\n",
        "            if result.get('status') == 'error':\n",
        "                error_msg = result.get('error', '').lower()\n",
        "                \n",
        "                # Pattern 1: Import errors\n",
        "                if 'no module named' in error_msg:\n",
        "                    troubleshooting_steps.append(f\"📦 {component}: Run 'pip install -e .' to install PyNucleus package\")\n",
        "                \n",
        "                # Pattern 2: Model not found\n",
        "                elif 'model' in error_msg and ('not found' in error_msg or 'does not exist' in error_msg):\n",
        "                    troubleshooting_steps.append(f\"🤖 {component}: Check internet connection and HuggingFace access\")\n",
        "                \n",
        "                # Pattern 3: CUDA/device errors\n",
        "                elif 'cuda' in error_msg or 'device' in error_msg:\n",
        "                    troubleshooting_steps.append(f\"🎮 {component}: Try setting USE_CUDA=False in settings\")\n",
        "                \n",
        "                # Pattern 4: ChromaDB issues\n",
        "                elif 'chroma' in error_msg or 'vector' in error_msg:\n",
        "                    troubleshooting_steps.append(f\"🗄️ {component}: Check ChromaDB installation and permissions\")\n",
        "                \n",
        "                # Pattern 5: Memory issues\n",
        "                elif 'memory' in error_msg or 'out of memory' in error_msg:\n",
        "                    troubleshooting_steps.append(f\"💾 {component}: Reduce batch size or use smaller model\")\n",
        "                \n",
        "                # Pattern 6: Permission errors\n",
        "                elif 'permission' in error_msg or 'access denied' in error_msg:\n",
        "                    troubleshooting_steps.append(f\"🔐 {component}: Check file/directory permissions\")\n",
        "                \n",
        "                # Generic troubleshooting\n",
        "                else:\n",
        "                    troubleshooting_steps.append(f\"⚠️ {component}: Check logs for detailed error information\")\n",
        "        \n",
        "        # General troubleshooting steps\n",
        "        if troubleshooting_steps:\n",
        "            troubleshooting_steps.extend([\n",
        "                \"🔄 Try restarting the Jupyter kernel\",\n",
        "                \"📋 Verify all requirements are installed: pip install -r requirements.txt\",\n",
        "                \"🏗️ Check if you're in the correct directory (PyNucleus-Model)\",\n",
        "                \"🌐 Ensure internet connection for model downloads\"\n",
        "            ])\n",
        "        \n",
        "        if troubleshooting_steps:\n",
        "            print(\"   📝 Recommended troubleshooting steps:\")\n",
        "            for i, step in enumerate(troubleshooting_steps, 1):\n",
        "                print(f\"      {i}. {step}\")\n",
        "        else:\n",
        "            print(\"   ✅ No critical errors detected\")\n",
        "        \n",
        "        return troubleshooting_steps\n",
        "    \n",
        "    def generate_debug_report(self, all_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate comprehensive debug report\"\"\"\n",
        "        print(\"\\n📄 Generating Debug Report:\")\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'system_info': {\n",
        "                'python_version': sys.version,\n",
        "                'platform': sys.platform,\n",
        "                'working_directory': str(Path.cwd())\n",
        "            },\n",
        "            'diagnostic_results': all_data.get('diagnostics', {}),\n",
        "            'import_diagnosis': all_data.get('import_diagnosis', {}),\n",
        "            'functionality_tests': all_data.get('functionality_tests', {}),\n",
        "            'performance_data': all_data.get('performance_data', {}),\n",
        "            'troubleshooting_steps': all_data.get('troubleshooting_steps', [])\n",
        "        }\n",
        "        \n",
        "        # Save debug report\n",
        "        debug_file = f\"debug_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        \n",
        "        with open(debug_file, 'w') as f:\n",
        "            json.dump(report, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"   ✅ Debug report saved: {debug_file}\")\n",
        "        print(f\"   📊 Report size: {Path(debug_file).stat().st_size / 1024:.1f} KB\")\n",
        "        \n",
        "        return debug_file\n",
        "\n",
        "# Run debugging tools\n",
        "if 'performance_data' in globals():\n",
        "    debugger = DebugToolkit()\n",
        "    \n",
        "    # Setup detailed logging\n",
        "    debugger.setup_detailed_logging()\n",
        "    \n",
        "    # Diagnose import issues\n",
        "    import_diagnosis = debugger.diagnose_import_issues()\n",
        "    \n",
        "    # Test core functionality\n",
        "    functionality_tests = debugger.test_core_functionality()\n",
        "    \n",
        "    # Analyze error patterns\n",
        "    troubleshooting_steps = debugger.analyze_error_patterns(functionality_tests)\n",
        "    \n",
        "    # Generate comprehensive debug report\n",
        "    debug_data = {\n",
        "        'diagnostics': diagnostic_results if 'diagnostic_results' in globals() else {},\n",
        "        'import_diagnosis': import_diagnosis,\n",
        "        'functionality_tests': functionality_tests,\n",
        "        'performance_data': performance_data,\n",
        "        'troubleshooting_steps': troubleshooting_steps\n",
        "    }\n",
        "    \n",
        "    debug_report_file = debugger.generate_debug_report(debug_data)\n",
        "    \n",
        "    print(f\"\\n✅ Debug analysis complete!\")\n",
        "    print(f\"   📊 Tests run: {len(functionality_tests)}\")\n",
        "    print(f\"   ⚠️ Issues found: {sum(1 for r in functionality_tests.values() if r.get('status') == 'error')}\")\n",
        "    print(f\"   💡 Troubleshooting steps: {len(troubleshooting_steps)}\")\n",
        "    \n",
        "    # Store for next cells\n",
        "    globals()['debugger'] = debugger\n",
        "    globals()['debug_data'] = debug_data\n",
        "    globals()['functionality_tests'] = functionality_tests\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Please run previous cells first to gather diagnostic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Production Monitoring & Alerting\n",
        "# =========================================\n",
        "# This cell provides production-grade monitoring and alerting capabilities\n",
        "\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"🚀 Production Monitoring & Alerting\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class ProductionMonitor:\n",
        "    \"\"\"Production-grade monitoring and alerting system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics_queue = queue.Queue()\n",
        "        self.alerts = []\n",
        "        self.monitoring_active = False\n",
        "        self.monitor_thread = None\n",
        "        \n",
        "    def start_real_time_monitoring(self, interval: int = 30) -> None:\n",
        "        \"\"\"Start real-time system monitoring\"\"\"\n",
        "        print(f\"\\n📊 Starting real-time monitoring (interval: {interval}s):\")\n",
        "        \n",
        "        if self.monitoring_active:\n",
        "            print(\"   ⚠️ Monitoring already active\")\n",
        "            return\n",
        "        \n",
        "        self.monitoring_active = True\n",
        "        self.monitor_thread = threading.Thread(\n",
        "            target=self._monitor_loop,\n",
        "            args=(interval,),\n",
        "            daemon=True\n",
        "        )\n",
        "        self.monitor_thread.start()\n",
        "        \n",
        "        print(\"   ✅ Real-time monitoring started\")\n",
        "        print(\"   📈 Monitoring system resources, response times, and error rates\")\n",
        "    \n",
        "    def _monitor_loop(self, interval: int) -> None:\n",
        "        \"\"\"Main monitoring loop\"\"\"\n",
        "        while self.monitoring_active:\n",
        "            try:\n",
        "                # Collect metrics\n",
        "                metrics = self._collect_metrics()\n",
        "                self.metrics_queue.put(metrics)\n",
        "                \n",
        "                # Check for alerts\n",
        "                self._check_alerts(metrics)\n",
        "                \n",
        "                time.sleep(interval)\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.alerts.append({\n",
        "                    'timestamp': datetime.now().isoformat(),\n",
        "                    'type': 'monitoring_error',\n",
        "                    'message': f\"Monitoring error: {e}\",\n",
        "                    'severity': 'high'\n",
        "                })\n",
        "    \n",
        "    def _collect_metrics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Collect current system metrics\"\"\"\n",
        "        import psutil\n",
        "        \n",
        "        # System metrics\n",
        "        cpu_percent = psutil.cpu_percent()\n",
        "        memory = psutil.virtual_memory()\n",
        "        disk = psutil.disk_usage('/')\n",
        "        \n",
        "        # Test response time\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            start_time = time.time()\n",
        "            test_docs = retrieve(\"test\", top_k=1)\n",
        "            response_time = time.time() - start_time\n",
        "            retrieval_success = True\n",
        "        except Exception:\n",
        "            response_time = None\n",
        "            retrieval_success = False\n",
        "        \n",
        "        return {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'cpu_percent': cpu_percent,\n",
        "            'memory_percent': memory.percent,\n",
        "            'disk_percent': (disk.used / disk.total) * 100,\n",
        "            'response_time': response_time,\n",
        "            'retrieval_success': retrieval_success\n",
        "        }\n",
        "    \n",
        "    def _check_alerts(self, metrics: Dict[str, Any]) -> None:\n",
        "        \"\"\"Check metrics against alert thresholds\"\"\"\n",
        "        alerts = []\n",
        "        \n",
        "        # CPU alert\n",
        "        if metrics['cpu_percent'] > 80:\n",
        "            alerts.append({\n",
        "                'type': 'high_cpu',\n",
        "                'message': f\"High CPU usage: {metrics['cpu_percent']:.1f}%\",\n",
        "                'severity': 'medium' if metrics['cpu_percent'] < 90 else 'high'\n",
        "            })\n",
        "        \n",
        "        # Memory alert\n",
        "        if metrics['memory_percent'] > 85:\n",
        "            alerts.append({\n",
        "                'type': 'high_memory',\n",
        "                'message': f\"High memory usage: {metrics['memory_percent']:.1f}%\",\n",
        "                'severity': 'medium' if metrics['memory_percent'] < 95 else 'high'\n",
        "            })\n",
        "        \n",
        "        # Disk alert\n",
        "        if metrics['disk_percent'] > 90:\n",
        "            alerts.append({\n",
        "                'type': 'high_disk',\n",
        "                'message': f\"High disk usage: {metrics['disk_percent']:.1f}%\",\n",
        "                'severity': 'high'\n",
        "            })\n",
        "        \n",
        "        # Response time alert\n",
        "        if metrics['response_time'] and metrics['response_time'] > 5.0:\n",
        "            alerts.append({\n",
        "                'type': 'slow_response',\n",
        "                'message': f\"Slow response time: {metrics['response_time']:.2f}s\",\n",
        "                'severity': 'medium'\n",
        "            })\n",
        "        \n",
        "        # Service availability alert\n",
        "        if not metrics['retrieval_success']:\n",
        "            alerts.append({\n",
        "                'type': 'service_down',\n",
        "                'message': \"Retrieval service not responding\",\n",
        "                'severity': 'high'\n",
        "            })\n",
        "        \n",
        "        # Add alerts with timestamp\n",
        "        for alert in alerts:\n",
        "            alert['timestamp'] = metrics['timestamp']\n",
        "            self.alerts.append(alert)\n",
        "    \n",
        "    def get_monitoring_dashboard(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate monitoring dashboard data\"\"\"\n",
        "        print(\"\\n📊 Real-time Monitoring Dashboard:\")\n",
        "        \n",
        "        # Get recent metrics\n",
        "        recent_metrics = []\n",
        "        temp_queue = queue.Queue()\n",
        "        \n",
        "        while not self.metrics_queue.empty():\n",
        "            metric = self.metrics_queue.get()\n",
        "            recent_metrics.append(metric)\n",
        "            temp_queue.put(metric)\n",
        "        \n",
        "        # Put metrics back\n",
        "        while not temp_queue.empty():\n",
        "            self.metrics_queue.put(temp_queue.get())\n",
        "        \n",
        "        if recent_metrics:\n",
        "            latest = recent_metrics[-1]\n",
        "            \n",
        "            print(f\"   🖥️ System Status (Last Update: {latest['timestamp']}):\")\n",
        "            print(f\"      • CPU Usage: {latest['cpu_percent']:.1f}%\")\n",
        "            print(f\"      • Memory Usage: {latest['memory_percent']:.1f}%\")\n",
        "            print(f\"      • Disk Usage: {latest['disk_percent']:.1f}%\")\n",
        "            \n",
        "            if latest['response_time']:\n",
        "                print(f\"      • Response Time: {latest['response_time']:.3f}s\")\n",
        "            \n",
        "            status = \"🟢 HEALTHY\" if latest['retrieval_success'] else \"🔴 ISSUES\"\n",
        "            print(f\"      • Service Status: {status}\")\n",
        "            \n",
        "            # Recent alerts\n",
        "            recent_alerts = [a for a in self.alerts if a['timestamp'] > (datetime.now() - timedelta(minutes=30)).isoformat()]\n",
        "            \n",
        "            print(f\"\\n   🚨 Recent Alerts ({len(recent_alerts)} in last 30min):\")\n",
        "            if recent_alerts:\n",
        "                for alert in recent_alerts[-5:]:  # Show last 5 alerts\n",
        "                    severity_icon = {\"low\": \"🟡\", \"medium\": \"🟠\", \"high\": \"🔴\"}.get(alert['severity'], \"⚪\")\n",
        "                    print(f\"      {severity_icon} {alert['message']} ({alert['timestamp'][:19]})\")\n",
        "            else:\n",
        "                print(\"      ✅ No recent alerts\")\n",
        "        \n",
        "        else:\n",
        "            print(\"   ⚠️ No monitoring data available\")\n",
        "        \n",
        "        return {\n",
        "            'metrics': recent_metrics,\n",
        "            'alerts': self.alerts,\n",
        "            'monitoring_active': self.monitoring_active\n",
        "        }\n",
        "    \n",
        "    def run_health_check(self) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive health check for production readiness\"\"\"\n",
        "        print(\"\\n🏥 Production Health Check:\")\n",
        "        \n",
        "        health_status = {\n",
        "            'overall_health': 'unknown',\n",
        "            'checks': {},\n",
        "            'recommendations': []\n",
        "        }\n",
        "        \n",
        "        # Check 1: Core services\n",
        "        try:\n",
        "            from pynucleus.rag.engine import ask\n",
        "            result = ask(\"test question\")\n",
        "            service_healthy = bool(result and result.get('answer'))\n",
        "            health_status['checks']['core_service'] = {\n",
        "                'status': 'pass' if service_healthy else 'fail',\n",
        "                'message': 'Core Q&A service responding' if service_healthy else 'Core service not responding'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            health_status['checks']['core_service'] = {\n",
        "                'status': 'fail',\n",
        "                'message': f'Core service error: {e}'\n",
        "            }\n",
        "        \n",
        "        # Check 2: Vector database\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            docs = retrieve(\"test\", top_k=1)\n",
        "            db_healthy = docs is not None\n",
        "            health_status['checks']['vector_database'] = {\n",
        "                'status': 'pass' if db_healthy else 'fail',\n",
        "                'message': f'Vector DB responding ({len(docs) if docs else 0} docs)' if db_healthy else 'Vector DB not responding'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            health_status['checks']['vector_database'] = {\n",
        "                'status': 'fail',\n",
        "                'message': f'Vector DB error: {e}'\n",
        "            }\n",
        "        \n",
        "        # Check 3: Model availability\n",
        "        try:\n",
        "            from pynucleus.llm.qwen_loader import generate\n",
        "            response = generate(\"test\", max_tokens=5)\n",
        "            model_healthy = bool(response)\n",
        "            health_status['checks']['ai_model'] = {\n",
        "                'status': 'pass' if model_healthy else 'fail',\n",
        "                'message': 'AI model responding' if model_healthy else 'AI model not responding'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            health_status['checks']['ai_model'] = {\n",
        "                'status': 'fail',\n",
        "                'message': f'AI model error: {e}'\n",
        "            }\n",
        "        \n",
        "        # Check 4: Resource availability\n",
        "        import psutil\n",
        "        memory = psutil.virtual_memory()\n",
        "        disk = psutil.disk_usage('/')\n",
        "        \n",
        "        resource_issues = []\n",
        "        if memory.percent > 90:\n",
        "            resource_issues.append(f\"High memory usage: {memory.percent:.1f}%\")\n",
        "        if (disk.used / disk.total) * 100 > 95:\n",
        "            resource_issues.append(f\"High disk usage: {(disk.used / disk.total) * 100:.1f}%\")\n",
        "        \n",
        "        health_status['checks']['resources'] = {\n",
        "            'status': 'pass' if not resource_issues else 'warn',\n",
        "            'message': 'Resources available' if not resource_issues else '; '.join(resource_issues)\n",
        "        }\n",
        "        \n",
        "        # Check 5: Data integrity\n",
        "        golden_dataset_path = Path(\"data/validation/golden_dataset.csv\")\n",
        "        vector_db_path = Path(\"data/03_intermediate/vector_db\")\n",
        "        \n",
        "        data_issues = []\n",
        "        if not golden_dataset_path.exists():\n",
        "            data_issues.append(\"Golden dataset missing\")\n",
        "        if not vector_db_path.exists():\n",
        "            data_issues.append(\"Vector database missing\")\n",
        "        \n",
        "        health_status['checks']['data_integrity'] = {\n",
        "            'status': 'pass' if not data_issues else 'fail',\n",
        "            'message': 'Data files present' if not data_issues else '; '.join(data_issues)\n",
        "        }\n",
        "        \n",
        "        # Calculate overall health\n",
        "        statuses = [check['status'] for check in health_status['checks'].values()]\n",
        "        if all(s == 'pass' for s in statuses):\n",
        "            health_status['overall_health'] = 'healthy'\n",
        "        elif any(s == 'fail' for s in statuses):\n",
        "            health_status['overall_health'] = 'unhealthy'\n",
        "        else:\n",
        "            health_status['overall_health'] = 'degraded'\n",
        "        \n",
        "        # Generate recommendations\n",
        "        if health_status['overall_health'] != 'healthy':\n",
        "            failed_checks = [name for name, check in health_status['checks'].items() if check['status'] == 'fail']\n",
        "            health_status['recommendations'] = [\n",
        "                f\"Address failed checks: {', '.join(failed_checks)}\",\n",
        "                \"Check system logs for detailed error information\",\n",
        "                \"Verify all dependencies are properly installed\",\n",
        "                \"Ensure sufficient system resources are available\"\n",
        "            ]\n",
        "        \n",
        "        # Display results\n",
        "        status_icon = {\n",
        "            'healthy': '🟢',\n",
        "            'degraded': '🟡', \n",
        "            'unhealthy': '🔴'\n",
        "        }.get(health_status['overall_health'], '⚪')\n",
        "        \n",
        "        print(f\"   {status_icon} Overall Health: {health_status['overall_health'].upper()}\")\n",
        "        \n",
        "        for check_name, check_result in health_status['checks'].items():\n",
        "            status_icon = {'pass': '✅', 'warn': '⚠️', 'fail': '❌'}.get(check_result['status'], '⚪')\n",
        "            print(f\"   {status_icon} {check_name.replace('_', ' ').title()}: {check_result['message']}\")\n",
        "        \n",
        "        return health_status\n",
        "    \n",
        "    def stop_monitoring(self) -> None:\n",
        "        \"\"\"Stop real-time monitoring\"\"\"\n",
        "        if self.monitoring_active:\n",
        "            self.monitoring_active = False\n",
        "            print(\"\\n🛑 Stopping real-time monitoring...\")\n",
        "            print(\"   ✅ Monitoring stopped\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ Monitoring not active\")\n",
        "\n",
        "# Run production monitoring\n",
        "if 'debug_data' in globals():\n",
        "    monitor = ProductionMonitor()\n",
        "    \n",
        "    # Run health check\n",
        "    health_status = monitor.run_health_check()\n",
        "    \n",
        "    # Start monitoring (optional - will run in background)\n",
        "    monitor.start_real_time_monitoring(interval=30)\n",
        "    \n",
        "    # Give monitoring a moment to collect initial data\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Show dashboard\n",
        "    dashboard_data = monitor.get_monitoring_dashboard()\n",
        "    \n",
        "    print(f\"\\n🎯 Production Monitoring Summary:\")\n",
        "    print(f\"   • Overall Health: {health_status['overall_health']}\")\n",
        "    print(f\"   • Checks Passed: {sum(1 for c in health_status['checks'].values() if c['status'] == 'pass')}/{len(health_status['checks'])}\")\n",
        "    print(f\"   • Real-time Monitoring: {'🟢 Active' if monitor.monitoring_active else '🔴 Inactive'}\")\n",
        "    \n",
        "    print(f\"\\n💡 Next Steps:\")\n",
        "    print(\"   • Monitor the dashboard for real-time metrics\")\n",
        "    print(\"   • Set up automated alerting for production deployment\")\n",
        "    print(\"   • Review logs regularly for performance optimization\")\n",
        "    print(\"   • Scale resources based on monitoring data\")\n",
        "    \n",
        "    # Store for session\n",
        "    globals()['monitor'] = monitor\n",
        "    globals()['health_status'] = health_status\n",
        "    globals()['dashboard_data'] = dashboard_data\n",
        "    \n",
        "    print(f\"\\n✅ Production monitoring system ready!\")\n",
        "    print(f\"🔧 Use 'monitor.stop_monitoring()' to stop background monitoring\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ Please run previous cells first to gather system data.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
