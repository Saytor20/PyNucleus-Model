2025-06-04 12:38:21 Log:
- Fixed a minor issue in the data processing pipeline. The script now correctly processes the data and generates the necessary outputs.
2025-06-04 12:50:22 changes made and pushed to origin main


 2025-06-05 12:50:49: Log Update
- Right now the data scrapping script scrapes wikipedia only. Future work can be paid services for Google API as we have tried to use python library but the error 429.2025-06-04 13:23:04 changes made and pushed to origin main
- Finished with the data chunking code and analysis and is ready for use 
- Next to work on the Vector DB code 
2025-06-04 13:24:12 changes made and pushed to origin main

2025-06-04 14:09:22 Log Update
- Implemented VectorDB setup and management system using FAISS
- Added comprehensive output logging with score explanations
- Script has the following features:
  * Embedding model setup using sentence-transformers
  * FAISS vector database creation and management
  * Semantic search functionality with similarity scoring
  * Detailed output logging to text files
  * Score explanations and quality metrics
- Tested with multiple queries related to modular design and chemical plants
- Output files are now saved in vectordb_outputs directory with timestamps
-Updated the README.md file 


 2025-06-06 15:58:48 changes made and pushed to origin main

- all codes have been transferred into their own .py file for formatting purposes. The main notebook contains only function calling now
- new file named performance_analyzer have been created to measure the performance of the model 

To Do: 
- Need to figure out why the function calling is not working in the main .ipynb for performance_analyzer but works in the performance_analyzer.py
- Need to work on the process simulation integration and python scriptting
- Need to work on API integration for cost of goods, news, and others 
- Need to create a script that would clean the data before ingesting into LLM or VectorDB (to veryify best approach)
- understand the structure and how to view the vectorDB data 

 2025-06-07 17:58:12 changes made and pushed to origin main
 1) need to connect docker to the system so that it can on anything 
 2) need to fix the issue of pull from github because it is not pulling the latest update on github 

  2025-06-08 01:20:12 
  1) updated the folders and the did full clean house of the files naming and folders 
  2) implemented docker settings for cross board implementation of the system 
  3) enhanced the RAG sysstem implementation so that it is more efficient 
  4) tested the pipeline and made sure everything is working well 
  5) createed a file name system_diagonstic.py that can tell what is being wrong and should be enhanced frequently 
  6) installed the DWSIM lib successfully
  
  To Do: 
  1) Confirm everything is good from the docker app 
  2) start testing the DWSIM simulation 
  

 2025-06-10 01:33:47 changes made and pushed to origin main

1) New updates have been made in terms of formatting and organization of the folders 
2) system check and output verifications have been completed with satisfactory output 
3) new config.py have been created to include all configuration requirements 
4) DWSIM is now functionating and all results are converted to .csv files 
5) the .ipynb file Capstone Project have been revised so that it covers DWSIM, data processing, chuncking and converting to VectorDB 
6) all folders have been updated so that it is organized as per the function 


To do: 
1) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows
2) update system_diagonstic.py to be aligned with the new changes 
3) Need to work on the DWSIM conversion to be understood by the RAG and to have input and output integration between the two 
4) The results and analysis should then be converted to a .txt file so that it is used as input to the LLM model to have a robust output 
5) Need to revise the DWSIM so that it can be easily adjusted later 

 2025-06-10 16:27:15 changes made and pushed to origin main
 Completed: 
1) Need to work on the DWSIM conversion to be understood by the RAG and to have input and output integration between the two 
2) The results and analysis should then be converted to a .txt file so that it is used as input to the LLM model to have a robust output 
3) Need to revise the DWSIM so that it can be easily adjusted later
4) the .ipynb have been revised to include both basic pipeline and advanced pipeline with integration ready context to be fed to the LLM 

To Do: 
1) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows
2) update system_diagonstic.py to be aligned with the new changes 
3) change the folders directory for different outputs like LLM and the results 
4) make sure that the LLM ready document has the feed conditions which can be fetched easily from .json value 
5) update the @project_structure.md to have everything updated 
6) run another docker build 
 2025-06-10 17:46:14 changes made and pushed to origin main

Completed: 
1) change the folders directory for different outputs like LLM and the results 
2) make sure that the LLM ready document has the feed conditions which can be fetched easily from .json value 
3) update the @project_structure.md to have everything updated 
4) run another docker build- it is needed to build anything. Note: the current builder covers only the DWSIM API calling 
5) ran everything and everything is working fine 
6) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows

To Do: 
1) Enhance the system based on the generated output from ChatGPT to enhance the overall structure of the system before proceeding with LLM integration
2) fix the DWSIM pipeline and remove mocked data from output if possible 
3) Start implementing the LLM in google colab... but write the full script here with or very small model before transferring to google colab 


 2025-06-10 21:28:05 changes made and pushed to origin main
Completed: 
1) src-layout migration (core_modules â†’ src/pynucleus)- completed
2) Data directory consolidation- Completed 
3) Pydantic settings + ConfigManager refactor- Completed 
4) Jinja2 templating for LLMOutputGenerator- Completed 
5) Centralized Logging Configuration - Completed 
6) enhanced the logging so that it logs when comprehensive_system_diagnostic is ran - Completed 
7) Prompt Template for LLM Integration - Completed 
8) Prompt: Token Counting Utility- Completed 
9) all system functions have been tested and working perfectly - Completed 

To Do: 

1) Minimal LLMRunner Class
2) Rendering & Querying Utility
3) CLI Command for Integrated Pipeline & LLM Query
4) Create API calling for live news, costs, and other things to be considered 
5) Run a docker builder once everything is completed 
6) update @README , @Pipeline Summary, and Structure to better enhance the documentations of the project. refer to project info to update those and remove those we dont need 

 2025-06-11 01:44:40 changes made and pushed to origin main

 2025-06-12 12:44:40 
 Completed: 
1) Minimal LLMRunner Class - Completed 
2) Rendering & Querying Utility - Completed 
3) CLI Command for Integrated Pipeline & LLM Query- Completed 
4) make sure that all the documents and DWSIM results are integrated in the pipeline- Completed 
5) remove all duplication files and clean house the files - Completed 
6) create a new script beside comprehensive_system_diagnostic that would run all .py and checks for errors. The system is now 100% after being 66% - Completed 
7) update @README , @Pipeline Summary, and Structure to better enhance the documentations of the project. refer to project info to update those and remove those we dont need-completed
8) create a script so that all .py and .ipynb files and RAG pipeline are checked for healthniess- completed  (System_validator)
9) create two separate notebooks one that is user friendly and the other is developer one - completed 


To Do: 
1) Create API calling for live news, costs, and other things to be considered 
2) Run a docker builder once everything is completed 
4) Make the DWSIM code functional 
5) prepare for migration to google colab 
6) evalute including an MCP container so that the agent can call on it if needed for further assistance 

 2025-06-13 18:04:09 changes made and pushed to origin main

Completed: 
1) Debugged @capstone project to fix major changes 
2) we need to enhance the OCR- Compeleted but need verification with samples 
3) fixed the validator and system_diagonstic so that it shows healthy running code 
4) improved the embedding mechanism 
5) implemented structured performance metrics @test_production_mont logging (latency, memory, CPU utilization) with real-time alerts for abnormal conditions to proactively address pipeline robustness issues.
6) implemented comprehensive validation methods ( ground-truth datasets with known answers) and provide user-friendly citation backtracking from generated responses.
7) added a new .md file for comprehensive system documentation

To Do: 
1) Create API calling for live news, costs, and other things to be considered 
2) Run a docker builder once everything is completed 
4) Make the DWSIM code functional 
5) prepare for migration to google colab 
6) evalute including an MCP container so that the agent can call on it if needed for further assistance 

 2025-06-13 21:38:04 changes made and pushed to origin main

2025-06-16 22:34:15 changes made and pushed to origin main

Completed:
1) - update chunking to include metdata to remove titles and pages - COMPLETED
   - Implemented efficient metadata stripping with pre-compiled regexes
   - Added Counter-based frequency analysis for repeated headers/footers  
   - Enhanced chunking pipeline with standardized JSON output format
   - Added comprehensive test suite with 9 passing tests
   - Created demo script showcasing metadata removal capabilities
   - Fixed import issues and ensured proper function integration

To Do: 
1) DWSIM was a failure... we need to try to mix it with azure 
2) image processing is not available right now but we can add another VLLM model that can process images at later stages 
3) add a new txt file that tracks future enhancements- Completed
4) explore API integration for live news, costs, and market data  
5) need to work on the FAISS as it is showing bad results and also the filing is very messed up a lot of cleaning to do like what is validation and output in the folder data - Completed
7) RAG explainability and factual accuracy - Completed
8) Enhanced Embeddings: Install transformers for Stella-en-1.5B-v5 (30-50% recall improvement) - Completed
9) LLM Integration: Replace template-based answers with actual language model
10) Advanced Validation: More sophisticated fact-checking algorithms -Completed 
11) implemented archiving for the FAISS reports - completed
12) fix the token counter - Completed
13) fix notebooks - completed 
14) review of all the codes with 100% system diognostic - completed
16) Consolidate Diagnostics & Loggings- completed
17) The Flask API - Completed
18) Static HTMX UI-  Completed but need to make server always available 
19) fix the questions and citation issues on System_validator
20) Helper Script + Manual Checklist - Completed
21) Remote Vector Store Stub & Feature Flags- Completed
22) run all system components and make sure the agents are working well... using testing scripts to confirm they are handling all types of possible error 

 2025-06-18 16:37:51 changes made and pushed to origin main

To Do: 
1) Deploy the model to google colab and test it... make sure the set up for github pipeline to pull is there to always test it 
2) Redo the whole pipeline related to the LLM... a prompt have already been developed just proceed with the implementaiton - Completed 
3) test the model so that it spits out answers... right now we are focused on the system working rather than enhancing the system - Completed 
4) complete clean the system and remove all templates, files, and folders that are not used - Completed 
5) fix the citation 
6) run system_validator script and start fixing all the problems in it 
7) update the API script to include costs
8) create a finanical advisor model that handles the money 
9) add DSPY like framework to the system to enhance the model
10) we need to fix the reliance on RAG... should it be like that? 
11) evaluate if we should start training the model or the current set up is fine and just needs more data
12) once number 10 is fixed then we can proceed with uploading more data and enhancing the system 
13) just updated @comprehensive_system_diagnostica and @system_validator... run them to make sure everything is running


 2025-06-19 17:24:17 changes made and pushed to origin main

 2025-06-19 17:26:51 changes made and pushed to origin main

 2025-06-19 17:54:04 changes made and pushed to origin main

 2025-06-19 17:56:20 changes made and pushed to origin main

 2025-06-19 17:57:21 changes made and pushed to origin main
