2025-06-04 12:38:21 Log:
- Fixed a minor issue in the data processing pipeline. The script now correctly processes the data and generates the necessary outputs.
2025-06-04 12:50:22 changes made and pushed to origin main


 2025-06-05 12:50:49: Log Update
- Right now the data scrapping script scrapes wikipedia only. Future work can be paid services for Google API as we have tried to use python library but the error 429.2025-06-04 13:23:04 changes made and pushed to origin main
- Finished with the data chunking code and analysis and is ready for use 
- Next to work on the Vector DB code 
2025-06-04 13:24:12 changes made and pushed to origin main

2025-06-04 14:09:22 Log Update
- Implemented VectorDB setup and management system using FAISS
- Added comprehensive output logging with score explanations
- Script has the following features:
  * Embedding model setup using sentence-transformers
  * FAISS vector database creation and management
  * Semantic search functionality with similarity scoring
  * Detailed output logging to text files
  * Score explanations and quality metrics
- Tested with multiple queries related to modular design and chemical plants
- Output files are now saved in vectordb_outputs directory with timestamps
-Updated the README.md file 


 2025-06-06 15:58:48 changes made and pushed to origin main

- all codes have been transferred into their own .py file for formatting purposes. The main notebook contains only function calling now
- new file named performance_analyzer have been created to measure the performance of the model 

To Do: 
- Need to figure out why the function calling is not working in the main .ipynb for performance_analyzer but works in the performance_analyzer.py
- Need to work on the process simulation integration and python scriptting
- Need to work on API integration for cost of goods, news, and others 
- Need to create a script that would clean the data before ingesting into LLM or VectorDB (to veryify best approach)
- understand the structure and how to view the vectorDB data 

 2025-06-07 17:58:12 changes made and pushed to origin main
 1) need to connect docker to the system so that it can on anything 
 2) need to fix the issue of pull from github because it is not pulling the latest update on github 

  2025-06-08 01:20:12 
  1) updated the folders and the did full clean house of the files naming and folders 
  2) implemented docker settings for cross board implementation of the system 
  3) enhanced the RAG sysstem implementation so that it is more efficient 
  4) tested the pipeline and made sure everything is working well 
  5) createed a file name system_diagonstic.py that can tell what is being wrong and should be enhanced frequently 
  6) installed the DWSIM lib successfully
  
  To Do: 
  1) Confirm everything is good from the docker app 
  2) start testing the DWSIM simulation 
  

 2025-06-10 01:33:47 changes made and pushed to origin main

1) New updates have been made in terms of formatting and organization of the folders 
2) system check and output verifications have been completed with satisfactory output 
3) new config.py have been created to include all configuration requirements 
4) DWSIM is now functionating and all results are converted to .csv files 
5) the .ipynb file Capstone Project have been revised so that it covers DWSIM, data processing, chuncking and converting to VectorDB 
6) all folders have been updated so that it is organized as per the function 


To do: 
1) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows
2) update system_diagonstic.py to be aligned with the new changes 
3) Need to work on the DWSIM conversion to be understood by the RAG and to have input and output integration between the two 
4) The results and analysis should then be converted to a .txt file so that it is used as input to the LLM model to have a robust output 
5) Need to revise the DWSIM so that it can be easily adjusted later 

 2025-06-10 16:27:15 changes made and pushed to origin main
 Completed: 
1) Need to work on the DWSIM conversion to be understood by the RAG and to have input and output integration between the two 
2) The results and analysis should then be converted to a .txt file so that it is used as input to the LLM model to have a robust output 
3) Need to revise the DWSIM so that it can be easily adjusted later
4) the .ipynb have been revised to include both basic pipeline and advanced pipeline with integration ready context to be fed to the LLM 

To Do: 
1) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows
2) update system_diagonstic.py to be aligned with the new changes 
3) change the folders directory for different outputs like LLM and the results 
4) make sure that the LLM ready document has the feed conditions which can be fetched easily from .json value 
5) update the @project_structure.md to have everything updated 
6) run another docker build 
 2025-06-10 17:46:14 changes made and pushed to origin main

Completed: 
1) change the folders directory for different outputs like LLM and the results 
2) make sure that the LLM ready document has the feed conditions which can be fetched easily from .json value 
3) update the @project_structure.md to have everything updated 
4) run another docker build- it is needed to build anything. Note: the current builder covers only the DWSIM API calling 
5) ran everything and everything is working fine 
6) update the code to include DWSIM script for window so that it switches seamsly between hosting API and using windows

To Do: 
1) Enhance the system based on the generated output from ChatGPT to enhance the overall structure of the system before proceeding with LLM integration
2) fix the DWSIM pipeline and remove mocked data from output if possible 
3) Start implementing the LLM in google colab... but write the full script here with or very small model before transferring to google colab 


 2025-06-10 21:28:05 changes made and pushed to origin main
Completed: 
1) src-layout migration (core_modules â†’ src/pynucleus)- completed
2) Data directory consolidation- Completed 
3) Pydantic settings + ConfigManager refactor- Completed 
4) Jinja2 templating for LLMOutputGenerator- Completed 
5) Centralized Logging Configuration - Completed 
6) enhanced the logging so that it logs when comprehensive_system_diagnostic is ran - Completed 
7) Prompt Template for LLM Integration - Completed 
8) Prompt: Token Counting Utility- Completed 
9) all system functions have been tested and working perfectly - Completed 

To Do: 

1) Minimal LLMRunner Class
2) Rendering & Querying Utility
3) CLI Command for Integrated Pipeline & LLM Query
4) Create API calling for live news, costs, and other things to be considered 
5) Run a docker builder once everything is completed 
6) update @README , @Pipeline Summary, and Structure to better enhance the documentations of the project. refer to project info to update those and remove those we dont need 

 2025-06-11 01:44:40 changes made and pushed to origin main

 2025-06-12 12:44:40 
 Completed: 
1) Minimal LLMRunner Class - Completed 
2) Rendering & Querying Utility - Completed 
3) CLI Command for Integrated Pipeline & LLM Query- Completed 
4) make sure that all the documents and DWSIM results are integrated in the pipeline- Completed 
5) remove all duplication files and clean house the files - Completed 
6) create a new script beside comprehensive_system_diagnostic that would run all .py and checks for errors. The system is now 100% after being 66% - Completed 
7) update @README , @Pipeline Summary, and Structure to better enhance the documentations of the project. refer to project info to update those and remove those we dont need-completed
8) create a script so that all .py and .ipynb files and RAG pipeline are checked for healthniess- completed  (System_validator)
9) create two separate notebooks one that is user friendly and the other is developer one - completed 


To Do: 
1) Create API calling for live news, costs, and other things to be considered 
2) Run a docker builder once everything is completed 
4) Make the DWSIM code functional 
5) prepare for migration to google colab 
6) evalute including an MCP container so that the agent can call on it if needed for further assistance 

 2025-06-13 18:04:09 changes made and pushed to origin main

Completed: 
1) Debugged @capstone project to fix major changes 
2) we need to enhance the OCR- Compeleted but need verification with samples 
3) fixed the validator and system_diagonstic so that it shows healthy running code 
4) improved the embedding mechanism 
5) implemented structured performance metrics @test_production_mont logging (latency, memory, CPU utilization) with real-time alerts for abnormal conditions to proactively address pipeline robustness issues.
6) implemented comprehensive validation methods ( ground-truth datasets with known answers) and provide user-friendly citation backtracking from generated responses.
7) added a new .md file for comprehensive system documentation

To Do: 
1) Create API calling for live news, costs, and other things to be considered 
2) Run a docker builder once everything is completed 
4) Make the DWSIM code functional 
5) prepare for migration to google colab 
6) evalute including an MCP container so that the agent can call on it if needed for further assistance 

 2025-06-13 21:38:04 changes made and pushed to origin main

2025-06-16 22:34:15 changes made and pushed to origin main

Completed:
1) - update chunking to include metdata to remove titles and pages - COMPLETED
   - Implemented efficient metadata stripping with pre-compiled regexes
   - Added Counter-based frequency analysis for repeated headers/footers  
   - Enhanced chunking pipeline with standardized JSON output format
   - Added comprehensive test suite with 9 passing tests
   - Created demo script showcasing metadata removal capabilities
   - Fixed import issues and ensured proper function integration

To Do: 
1) DWSIM was a failure... we need to try to mix it with azure 
2) image processing is not available right now but we can add another VLLM model that can process images at later stages 
3) add a new txt file that tracks future enhancements- Completed
4) explore API integration for live news, costs, and market data  
5) need to work on the FAISS as it is showing bad results and also the filing is very messed up a lot of cleaning to do like what is validation and output in the folder data - Completed
7) RAG explainability and factual accuracy - Completed
8) Enhanced Embeddings: Install transformers for Stella-en-1.5B-v5 (30-50% recall improvement) - Completed
9) LLM Integration: Replace template-based answers with actual language model
10) Advanced Validation: More sophisticated fact-checking algorithms -Completed 
11) implemented archiving for the FAISS reports - completed
12) fix the token counter - Completed
13) fix notebooks - completed 
14) review of all the codes with 100% system diognostic - completed
16) Consolidate Diagnostics & Loggings- completed
17) The Flask API - Completed
18) Static HTMX UI-  Completed but need to make server always available 
19) fix the questions and citation issues on System_validator
20) Helper Script + Manual Checklist - Completed
21) Remote Vector Store Stub & Feature Flags- Completed
22) run all system components and make sure the agents are working well... using testing scripts to confirm they are handling all types of possible error 

 2025-06-18 16:37:51 changes made and pushed to origin main

Completed 
2) Redo the whole pipeline related to the LLM... a prompt have already been developed just proceed with the implementaiton - Completed 
3) test the model so that it spits out answers... right now we are focused on the system working rather than enhancing the system - Completed 
4) complete clean the system and remove all templates, files, and folders that are not used - Completed 
9) add DSPY like framework to the system to enhance the model - Completed "guidance" 
10) we need to fix the reliance on RAG... should it be like that? - completed 
12) just updated @comprehensive_system_diagnostica and @system_validator... run them to make sure everything is running - completed
13) Fix the github issue - completed... right now the model parameters is locally downloaded
14) we are using now QWEN 1.5B with no problems with it so far - completed 
15) updated vector store to be more robust - completed  
16) fix the "Failed to initialize ChromaDB:An instance of Chroma already exists" - completed 
17) we have added a source in the output which is soo cool- completed 
18) we need to now enhance the output and make it look pretty i am sure there is an existing library that can do that - completed
19) Stop the extra HF download - completed 
20) Minimal citation formatter - completed 
21) create a script to clean the tables in the during the pdf extraction using panda so that we can use these information part of @data - completed but need to be tested - completed
22) updates the golden and validation dataset - completed
23) updated the script to randomly select from both dataset insstead of using the whole dataset - completed
24)Citation Metadata Enhancement - completed 
25) Improve Recall & Prompt Quality- completed
26)Align Ingestion and Retrieval (Metadata & Golden Set Inclusion) - completed
27) Enrich Chunking and Document Re-indexing- completed
28) Hybrid Retrieval (Semantic + Keyword/BM25) - completed

 2025-06-20 10:25:50 changes made and pushed to origin main
To do: 
1) Deploy the model to google colab and test it... make sure the set up for github pipeline to pull is there to always test it 
2) fix the citation and ground truth
3) run system_validator script and start fixing all the problems in it 
4) update the API script to include costs
5) create a finanical advisor model that handles the money 
7) we need to understand what we have to do with the golden dataset and validation data are we trainning on them or what and what is the difference between the 2? 
8) we need to create a template so that it understand what it is doing instead of spitting out answers 
9) enhance the website to include system dignostic and make it into a developer like website where you can view stas and run system_diagonstic - completed
10) enable another function inside chat that asks either to chat or build if its a build then we build a template that fetches data from API sources uses finanical analyzer, and system designs to estimate cost using an existing template that we build ourselves using mock dwsim daa using 20 plants for now 
11) save the flask format as an image? evaluate anothe container with the current set up  .. add a clear button to the system dignostic also file upload is ugly... also the statisitcs are weird and its still spitting lies - completed 
12) switch to a better qwen model like qwen 3 0.6b 
12) ADD ROBUST ANSWER SYNTHESIS

# Update prompting.py build_prompt function:
"""
def build_prompt(context: str, question: str) -> str:
    return f\"\"\"You are an expert chemical-process engineer.
Using the provided context, think step-by-step to answer precisely.

Context:
{context}

Question:
{question}

Let's think step-by-step:
\"\"\"
"""

# Validation:
# Confirm improved reasoning clarity.

 2025-06-20 10:25:50 changes made and pushed to origin main

Compleeted: 
1) enhance the website to include system dignostic and make it into a developer like website where you can view stas and run system_diagonstic - completed
2) enhanced the statisitcs and now we can upload from the website and it will be processed right away 
3) system_diagonstic and system_validator are on the developer website 
4) redesigned the website to be like a terminal 



To do: 
1) Deploy the model to google colab and test it... make sure the set up for github pipeline to pull is there to always test it 
2) fix the citation and ground truth
3) run system_validator script and start fixing all the problems in it 
4) update the API script to include costs 
5) create a finanical advisor model that handles the money 
7) we need to understand what we have to do with the golden dataset and validation data are we trainning on them or what and what is the difference between the 2? 
8) we need to create a template so that it understand what it is doing instead of spitting out answers 
10) enable another function inside chat that asks either to chat or build if its a build then we build a template that fetches data from API sources uses finanical analyzer, and system designs to estimate cost using an existing template that we build ourselves using mock dwsim daa using 20 plants for now 
11) save the flask format as an image? evaluate anothe container with the current set up  .. add a clear button to the system dignostic also file upload is ugly... also the statisitcs are weird and its still spitting lies - completed 
12) switch to a better qwen model like qwen 3 0.6b 
12) ADD ROBUST ANSWER SYNTHESIS

# Update prompting.py build_prompt function:
"""
def build_prompt(context: str, question: str) -> str:
    return f\"\"\"You are an expert chemical-process engineer.
Using the provided context, think step-by-step to answer precisely.

Context:
{context}

Question:
{question}

Let's think step-by-step:
\"\"\"
"""

# Validation:
# Confirm improved reasoning clarity.
 2025-06-21 17:18:23 changes made and pushed to origin main
