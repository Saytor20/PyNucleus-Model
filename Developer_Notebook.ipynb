{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Developer Notebook** üõ†Ô∏è\n",
        "\n",
        "## **Advanced Development Environment**\n",
        "\n",
        "This notebook contains advanced features, diagnostics, and development tools for PyNucleus.\n",
        "\n",
        "### **üîß Developer Features:**\n",
        "- **System Diagnostics**: Comprehensive health checks and validation\n",
        "- **Enhanced Pipeline**: Advanced configuration and integration\n",
        "- **LLM Development**: Prompt engineering and model testing\n",
        "- **Performance Analysis**: Detailed metrics and benchmarking\n",
        "- **Configuration Management**: Custom templates and settings\n",
        "- **Debug Tools**: Logging, tracing, and error analysis\n",
        "\n",
        "### **üìã Notebook Sections:**\n",
        "1. **System Initialization & Diagnostics** (Cells 1-3)\n",
        "2. **Enhanced Pipeline Configuration** (Cells 4-6)\n",
        "3. **Advanced Analysis & Integration** (Cells 7-9)\n",
        "4. **LLM Development & Testing** (Cells 10-12)\n",
        "5. **Performance & Debugging** (Cells 13-15)\n",
        "6. **Version Control & Maintenance** (Cells 16-18)\n",
        "\n",
        "---\n",
        "**‚ö†Ô∏è Warning**: This notebook is intended for developers and advanced users. For basic usage, use `Capstone Project.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 1: System Initialization & Diagnostics\n",
        "# ===============================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üîß PyNucleus Developer Environment - Starting Initialization...\")\n",
        "print(f\"üìÖ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Import all PyNucleus components\n",
        "try:\n",
        "    from pynucleus.pipeline import RAGPipeline, DWSIMPipeline, ResultsExporter, PipelineUtils\n",
        "    from pynucleus.integration.config_manager import ConfigManager\n",
        "    from pynucleus.integration.dwsim_rag_integrator import DWSIMRAGIntegrator\n",
        "    from pynucleus.integration.llm_output_generator import LLMOutputGenerator\n",
        "    from pynucleus.llm import LLMRunner\n",
        "    from pynucleus.llm.query_llm import LLMQueryManager, quick_ask_llm\n",
        "    \n",
        "    print(\"‚úÖ All PyNucleus modules imported successfully\")\n",
        "    \n",
        "    # Initialize components\n",
        "    pipeline = PipelineUtils(results_dir=\"data/05_output/results\")\n",
        "    config_manager = ConfigManager(config_dir=\"configs\")\n",
        "    dwsim_rag_integrator = DWSIMRAGIntegrator(\n",
        "        rag_pipeline=None,  # Will be set after pipeline initialization\n",
        "        results_dir=\"data/05_output/results\"\n",
        "    )\n",
        "    llm_generator = LLMOutputGenerator(results_dir=\"data/05_output/llm_reports\")\n",
        "    \n",
        "    print(\"‚úÖ Core components initialized\")\n",
        "    print(\"üéØ Ready for development and testing!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Initialization error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 1.2: Comprehensive System Diagnostic\n",
        "# ============================================\n",
        "\n",
        "print(\"üîç Running Comprehensive System Diagnostic...\")\n",
        "\n",
        "try:\n",
        "    # Run system diagnostic\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"scripts/comprehensive_system_diagnostic.py\", \"--quiet\"\n",
        "    ], capture_output=True, text=True, cwd=\".\")\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ System diagnostic completed successfully\")\n",
        "        # Extract key metrics from output\n",
        "        lines = result.stdout.strip().split('\\n')\n",
        "        for line in lines[-10:]:  # Show last 10 lines for summary\n",
        "            if any(keyword in line for keyword in ['Health:', 'Status:', 'EXCELLENT', 'GOOD', 'passed']):\n",
        "                print(f\"   {line}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è System diagnostic issues detected:\")\n",
        "        print(result.stderr)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not run system diagnostic: {e}\")\n",
        "    print(\"üí° Continuing with manual checks...\")\n",
        "\n",
        "# Manual system checks\n",
        "print(\"\\nüîç Manual System Checks:\")\n",
        "\n",
        "# Check data directories\n",
        "data_dirs = ['data/01_raw', 'data/02_processed', 'data/03_intermediate', 'data/04_models', 'data/05_output']\n",
        "for dir_path in data_dirs:\n",
        "    exists = Path(dir_path).exists()\n",
        "    print(f\"   {'‚úÖ' if exists else '‚ùå'} {dir_path}\")\n",
        "\n",
        "# Check src structure\n",
        "src_dirs = ['src/pynucleus/pipeline', 'src/pynucleus/rag', 'src/pynucleus/integration', 'src/pynucleus/llm']\n",
        "for dir_path in src_dirs:\n",
        "    exists = Path(dir_path).exists()\n",
        "    print(f\"   {'‚úÖ' if exists else '‚ùå'} {dir_path}\")\n",
        "\n",
        "print(\"\\nüéØ System diagnostic complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 1.3: Pipeline Status and Health Check\n",
        "# =============================================\n",
        "\n",
        "print(\"üìä Detailed Pipeline Status Check...\")\n",
        "\n",
        "try:\n",
        "    # Pipeline component status\n",
        "    pipeline.print_pipeline_status()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Quick test with validation\n",
        "    test_results = pipeline.quick_test()\n",
        "    \n",
        "    # Validate test_results\n",
        "    if test_results is None:\n",
        "        print(\"‚ùå Quick test returned None - pipeline may not be properly initialized\")\n",
        "        test_results = {\n",
        "            'results_dir': 'data/05_output/results',\n",
        "            'csv_files_count': 0,\n",
        "            'csv_files': []\n",
        "        }\n",
        "    \n",
        "    # Safely access results with fallbacks\n",
        "    results_dir = test_results.get('results_dir', 'data/05_output/results')\n",
        "    csv_files_count = test_results.get('csv_files_count', 0)\n",
        "    csv_files = test_results.get('csv_files', [])\n",
        "    \n",
        "    print(f\"üìÅ Results Directory: {results_dir}\")\n",
        "    print(f\"üìÑ CSV Files: {csv_files_count}\")\n",
        "    \n",
        "    if csv_files_count > 0:\n",
        "        print(\"\\nüìã Existing Files:\")\n",
        "        for file_info in csv_files:\n",
        "            if isinstance(file_info, dict):\n",
        "                name = file_info.get('name', 'Unknown')\n",
        "                size = file_info.get('size', 0)\n",
        "                print(f\"   ‚Ä¢ {name} ({size} bytes)\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ {file_info}\")\n",
        "    \n",
        "    # Component health\n",
        "    print(f\"\\nüîß Component Health:\")\n",
        "    print(f\"   ‚Ä¢ Pipeline Utils: {'‚úÖ' if hasattr(pipeline, 'rag_pipeline') else '‚ö†Ô∏è'}\")\n",
        "    print(f\"   ‚Ä¢ Config Manager: {'‚úÖ' if config_manager else '‚ùå'}\")\n",
        "    print(f\"   ‚Ä¢ DWSIM-RAG Integrator: {'‚úÖ' if dwsim_rag_integrator else '‚ùå'}\")\n",
        "    print(f\"   ‚Ä¢ LLM Generator: {'‚úÖ' if llm_generator else '‚ùå'}\")\n",
        "    \n",
        "    # Additional diagnostics\n",
        "    component_status = test_results.get('component_status', {})\n",
        "    print(f\"\\nüîç Pipeline Component Status:\")\n",
        "    print(f\"   ‚Ä¢ RAG Pipeline: {'‚úÖ' if component_status.get('rag_pipeline', False) else '‚ùå'}\")\n",
        "    print(f\"   ‚Ä¢ DWSIM Pipeline: {'‚úÖ' if component_status.get('dwsim_pipeline', False) else '‚ùå'}\")\n",
        "    print(f\"   ‚Ä¢ Results Exporter: {'‚úÖ' if component_status.get('exporter', False) else '‚ùå'}\")\n",
        "    \n",
        "    # Integration status\n",
        "    integration_enabled = test_results.get('integration_enabled', False)\n",
        "    rag_chunks = test_results.get('rag_chunks', 0)\n",
        "    simulation_chunks = test_results.get('simulation_chunks', 0)\n",
        "    \n",
        "    print(f\"\\nüîó Integration Status:\")\n",
        "    print(f\"   ‚Ä¢ Total RAG Chunks: {rag_chunks:,}\")\n",
        "    print(f\"   ‚Ä¢ Simulation Chunks: {simulation_chunks:,}\")\n",
        "    print(f\"   ‚Ä¢ Integration Active: {'‚úÖ' if integration_enabled else '‚ö™'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Status check error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    # Provide troubleshooting tips\n",
        "    print(\"\\nüîß Troubleshooting Tips:\")\n",
        "    print(\"   1. Try restarting the notebook kernel\")\n",
        "    print(\"   2. Re-run Cell 1 to reinitialize components\")\n",
        "    print(\"   3. Check if all required directories exist\")\n",
        "    print(\"   4. Verify PyNucleus installation is complete\")\n",
        "\n",
        "print(\"\\n‚úÖ Status check complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 2: Enhanced Pipeline Configuration** üîß\n",
        "\n",
        "Advanced configuration management and template generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 2.1: Configuration Templates and Management\n",
        "# ===================================================\n",
        "\n",
        "print(\"üîß Advanced Configuration Management...\")\n",
        "\n",
        "# Create configuration templates\n",
        "try:\n",
        "    # Generate JSON and CSV templates\n",
        "    json_template = config_manager.create_template_json(\"dev_simulation_config.json\", verbose=True)\n",
        "    csv_template = config_manager.create_template_csv(\"dev_simulation_config.csv\", verbose=True)\n",
        "    \n",
        "    print(f\"‚úÖ Configuration templates created:\")\n",
        "    print(f\"   ‚Ä¢ JSON: {json_template}\")\n",
        "    print(f\"   ‚Ä¢ CSV: {csv_template}\")\n",
        "    \n",
        "    # Show template contents (first few lines)\n",
        "    if Path(json_template).exists():\n",
        "        with open(json_template, 'r') as f:\n",
        "            content = f.read()[:300]\n",
        "            print(f\"\\nüìã JSON Template Preview:\")\n",
        "            print(content + \"...\" if len(content) >= 300 else content)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Configuration error: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Configuration management ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 2.2: Run Enhanced Pipeline with Full Analysis\n",
        "# =====================================================\n",
        "\n",
        "print(\"üöÄ Running Enhanced Pipeline for Development Testing...\")\n",
        "\n",
        "# Run complete pipeline with detailed logging\n",
        "try:\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Execute pipeline\n",
        "    results = pipeline.run_complete_pipeline()\n",
        "    \n",
        "    if results:\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"\\nüéâ Pipeline completed in {duration:.1f} seconds!\")\n",
        "        \n",
        "        # Detailed results analysis\n",
        "        print(f\"\\nüìä Detailed Results:\")\n",
        "        print(f\"   ‚Ä¢ RAG Queries: {len(results.get('rag_data', []))}\")\n",
        "        print(f\"   ‚Ä¢ DWSIM Simulations: {len(results.get('dwsim_data', []))}\")\n",
        "        print(f\"   ‚Ä¢ Export Files: {len(results.get('exported_files', []))}\")\n",
        "        \n",
        "        # Set up integrator with pipeline data\n",
        "        if hasattr(pipeline, 'rag_pipeline'):\n",
        "            dwsim_rag_integrator.rag_pipeline = pipeline.rag_pipeline\n",
        "        \n",
        "        print(\"‚úÖ Pipeline data ready for enhanced analysis\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Pipeline execution failed\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Enhanced pipeline error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n‚úÖ Enhanced pipeline testing complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 3: Advanced Analysis & Integration** üî¨\n",
        "\n",
        "DWSIM-RAG integration, financial analysis, and enhanced reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 3.1: DWSIM-RAG Integration and Enhanced Analysis\n",
        "# ========================================================\n",
        "\n",
        "print(\"üî¨ Advanced DWSIM-RAG Integration Analysis...\")\n",
        "\n",
        "try:\n",
        "    # Get DWSIM results\n",
        "    dwsim_results = pipeline.dwsim_pipeline.get_results()\n",
        "    \n",
        "    if dwsim_results:\n",
        "        print(f\"üìä Processing {len(dwsim_results)} DWSIM simulations...\")\n",
        "        \n",
        "        # Perform enhanced integration\n",
        "        integrated_results = dwsim_rag_integrator.integrate_simulation_results(\n",
        "            dwsim_results, perform_rag_analysis=True\n",
        "        )\n",
        "        \n",
        "        # Export integrated results\n",
        "        integrated_export_file = dwsim_rag_integrator.export_integrated_results()\n",
        "        \n",
        "        print(f\"‚úÖ Enhanced integration complete:\")\n",
        "        print(f\"   ‚Ä¢ Integrated simulations: {len(integrated_results)}\")\n",
        "        print(f\"   ‚Ä¢ Export file: {integrated_export_file}\")\n",
        "        \n",
        "        # Show detailed analysis for first simulation\n",
        "        if integrated_results:\n",
        "            sample = integrated_results[0]\n",
        "            print(f\"\\\\nüìã Sample Analysis (First Simulation):\")\n",
        "            \n",
        "            # Safely access original simulation data\n",
        "            original_sim = sample.get('original_simulation', {})\n",
        "            print(f\"   ‚Ä¢ Case: {original_sim.get('case_name', 'Unknown')}\")\n",
        "            \n",
        "            # Dynamically display all available performance metrics\n",
        "            perf_metrics = sample.get('performance_metrics', {})\n",
        "            if perf_metrics:\n",
        "                print(f\"   üìä Performance Metrics:\")\n",
        "                for key, value in perf_metrics.items():\n",
        "                    # Format the key for display (convert snake_case to Title Case)\n",
        "                    display_key = key.replace('_', ' ').title()\n",
        "                    \n",
        "                    # Handle different value types appropriately\n",
        "                    if isinstance(value, (int, float)):\n",
        "                        if 'rate' in key.lower() or 'percentage' in key.lower():\n",
        "                            print(f\"      ‚Ä¢ {display_key}: {value:.1f}%\")\n",
        "                        elif isinstance(value, float):\n",
        "                            print(f\"      ‚Ä¢ {display_key}: {value:.3f}\")\n",
        "                        else:\n",
        "                            print(f\"      ‚Ä¢ {display_key}: {value}\")\n",
        "                    elif isinstance(value, dict):\n",
        "                        print(f\"      ‚Ä¢ {display_key}: {len(value)} items\")\n",
        "                        # Show nested dict items if not too many\n",
        "                        if len(value) <= 5:\n",
        "                            for sub_key, sub_value in value.items():\n",
        "                                sub_display_key = sub_key.replace('_', ' ').title()\n",
        "                                print(f\"        - {sub_display_key}: {sub_value}\")\n",
        "                    elif isinstance(value, list):\n",
        "                        print(f\"      ‚Ä¢ {display_key}: {len(value)} items\")\n",
        "                    else:\n",
        "                        print(f\"      ‚Ä¢ {display_key}: {value}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è No performance metrics available\")\n",
        "            \n",
        "            # Show other analysis results dynamically\n",
        "            analysis_sections = [\n",
        "                ('potential_issues', 'Potential Issues'),\n",
        "                ('recommendations', 'Recommendations'), \n",
        "                ('optimization_opportunities', 'Optimization Opportunities'),\n",
        "                ('rag_insights', 'RAG Insights')\n",
        "            ]\n",
        "            \n",
        "            for section_key, section_title in analysis_sections:\n",
        "                section_data = sample.get(section_key, [])\n",
        "                if section_data:\n",
        "                    print(f\"   üìã {section_title}: {len(section_data)} items\")\n",
        "                    # Show first few items as examples\n",
        "                    for i, item in enumerate(section_data[:3]):\n",
        "                        if isinstance(item, dict):\n",
        "                            # For RAG insights or complex items\n",
        "                            item_summary = item.get('content', item.get('query', str(item)))[:100]\n",
        "                            print(f\"      {i+1}. {item_summary}...\")\n",
        "                        else:\n",
        "                            # For simple string items\n",
        "                            print(f\"      {i+1}. {item}\")\n",
        "                    if len(section_data) > 3:\n",
        "                        print(f\"      ... and {len(section_data) - 3} more\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No DWSIM results available. Run Section 2.2 first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Integration error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\n‚úÖ Advanced integration analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 3.2: LLM Report Generation and Financial Analysis\n",
        "# =========================================================\n",
        "\n",
        "print(\"üí∞ Advanced Financial Analysis and LLM Report Generation...\")\n",
        "\n",
        "try:\n",
        "    if 'integrated_results' in locals() and integrated_results:\n",
        "        \n",
        "        # Generate LLM reports for all simulations\n",
        "        print(f\"üìÑ Generating LLM reports for {len(integrated_results)} simulations...\")\n",
        "        \n",
        "        llm_report_files = []\n",
        "        for i, result in enumerate(integrated_results):\n",
        "            try:\n",
        "                report_file = llm_generator.export_llm_ready_text(result)\n",
        "                llm_report_files.append(report_file)\n",
        "                print(f\"   ‚úÖ Report {i+1}: {Path(report_file).name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Report {i+1} failed: {e}\")\n",
        "        \n",
        "        # Generate comprehensive financial analysis\n",
        "        financial_file = llm_generator.export_financial_analysis(integrated_results)\n",
        "        metrics = llm_generator._calculate_key_metrics(integrated_results)\n",
        "        \n",
        "        print(f\"\\\\nüí∞ Comprehensive Financial Metrics:\")\n",
        "        print(f\"   ‚Ä¢ Average Recovery Rate: {metrics['avg_recovery']:.1f}%\")\n",
        "        print(f\"   ‚Ä¢ Estimated Daily Revenue: ${metrics['estimated_revenue']:,.2f}\")\n",
        "        print(f\"   ‚Ä¢ Estimated Daily Profit: ${metrics['net_profit']:,.2f}\")\n",
        "        print(f\"   ‚Ä¢ Return on Investment: {metrics['roi']:.1f}%\")\n",
        "        print(f\"   ‚Ä¢ Financial Analysis File: {financial_file}\")\n",
        "        \n",
        "        # Performance summary\n",
        "        print(f\"\\\\nüìä Performance Summary:\")\n",
        "        good_performance = sum(1 for r in integrated_results \n",
        "                             if r['performance_metrics']['overall_performance'] == 'Good')\n",
        "        print(f\"   ‚Ä¢ High Performance Simulations: {good_performance}/{len(integrated_results)}\")\n",
        "        \n",
        "        avg_efficiency = sum(1 for r in integrated_results \n",
        "                           if r['performance_metrics']['efficiency_rating'] == 'High') / len(integrated_results)\n",
        "        print(f\"   ‚Ä¢ High Efficiency Rate: {avg_efficiency:.1%}\")\n",
        "        \n",
        "        print(f\"\\\\nüìÑ Generated Files:\")\n",
        "        print(f\"   ‚Ä¢ LLM Reports: {len(llm_report_files)} files\")\n",
        "        print(f\"   ‚Ä¢ Financial Analysis: 1 file\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No integrated results available. Run Section 3.1 first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå LLM/Financial analysis error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\n‚úÖ Advanced analysis and reporting complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 4: LLM Development & Testing** ü§ñ\n",
        "\n",
        "LLM model testing, prompt engineering, and query development.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 4.1: LLM Model Testing and Initialization\n",
        "# =================================================\n",
        "\n",
        "print(\"ü§ñ LLM Development Environment Initialization...\")\n",
        "\n",
        "try:\n",
        "    # Initialize LLM components\n",
        "    llm_runner = LLMRunner()\n",
        "    llm_query_manager = LLMQueryManager(max_tokens=2048)\n",
        "    \n",
        "    # Test LLM functionality\n",
        "    print(f\"‚úÖ LLM Runner initialized\")\n",
        "    print(f\"‚úÖ LLM Query Manager initialized\")\n",
        "    print(f\"   ‚Ä¢ Template directory: {llm_query_manager.template_dir}\")\n",
        "    print(f\"   ‚Ä¢ Template exists: {llm_query_manager.template_dir.exists()}\")\n",
        "    \n",
        "    # Get model information\n",
        "    model_info = llm_runner.get_model_info()\n",
        "    print(f\"\\\\nüîß Model Information:\")\n",
        "    print(f\"   ‚Ä¢ Model ID: {model_info['model_id']}\")\n",
        "    print(f\"   ‚Ä¢ Vocabulary Size: {model_info['vocab_size']:,}\")\n",
        "    print(f\"   ‚Ä¢ Device: {model_info['device']}\")\n",
        "    \n",
        "    # Test basic prompt rendering\n",
        "    test_prompt = llm_query_manager.render_prompt(\n",
        "        user_query=\"Test chemical process optimization query\",\n",
        "        system_message=\"You are a chemical engineering expert.\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nüìã Prompt System Test:\")\n",
        "    print(f\"   ‚Ä¢ Template rendering: ‚úÖ Success\")\n",
        "    print(f\"   ‚Ä¢ Prompt length: {len(test_prompt)} characters\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå LLM initialization error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\n‚úÖ LLM development environment ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 4.2: Advanced Prompt Engineering and Testing\n",
        "# ====================================================\n",
        "\n",
        "print(\"üéØ Advanced Prompt Engineering and Testing...\")\n",
        "\n",
        "try:\n",
        "    # Test different prompt scenarios\n",
        "    test_scenarios = [\n",
        "        {\n",
        "            \"name\": \"Process Optimization\",\n",
        "            \"query\": \"How can we optimize the distillation column efficiency?\",\n",
        "            \"system\": \"You are a process optimization expert specializing in distillation systems.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Safety Analysis\", \n",
        "            \"query\": \"What safety considerations are important for modular chemical plants?\",\n",
        "            \"system\": \"You are a chemical safety engineer with expertise in process hazard analysis.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Economic Assessment\",\n",
        "            \"query\": \"Analyze the economic benefits of modular plant design.\",\n",
        "            \"system\": \"You are a chemical engineering economist specializing in plant design economics.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(f\"üß™ Testing {len(test_scenarios)} prompt scenarios...\")\n",
        "    \n",
        "    for i, scenario in enumerate(test_scenarios):\n",
        "        print(f\"\\\\nüìã Scenario {i+1}: {scenario['name']}\")\n",
        "        \n",
        "        try:\n",
        "            # Render prompt\n",
        "            prompt = llm_query_manager.render_prompt(\n",
        "                user_query=scenario['query'],\n",
        "                system_message=scenario['system']\n",
        "            )\n",
        "            \n",
        "            print(f\"   ‚úÖ Prompt rendered successfully ({len(prompt)} chars)\")\n",
        "            \n",
        "            # Quick test with LLM (generate a short response)\n",
        "            response = llm_runner.ask(\n",
        "                scenario['query'],\n",
        "                max_length=50,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            \n",
        "            print(f\"   ‚úÖ LLM response generated ({len(response)} chars)\")\n",
        "            print(f\"   üìù Preview: {response[:100]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Scenario {i+1} failed: {e}\")\n",
        "    \n",
        "    # Test prompt system validation\n",
        "    print(f\"\\\\nüîç Running Prompt System Validation...\")\n",
        "    try:\n",
        "        # Import prompt system if available\n",
        "        from src.pynucleus.llm.prompt_system import PyNucleusPromptSystem\n",
        "        \n",
        "        prompt_system = PyNucleusPromptSystem(template_dir=\"prompts\")\n",
        "        validation_result = prompt_system.validate_prompts()\n",
        "        \n",
        "        print(\"‚úÖ Prompt system validation completed\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Prompt system module not available for validation\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Prompt validation error: {e}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Prompt engineering error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\n‚úÖ Prompt engineering and testing complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 5: Performance & Debugging** üîç\n",
        "\n",
        "Performance analysis, debugging tools, and system optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 5.1: Performance Analysis and Benchmarking\n",
        "# ==================================================\n",
        "\n",
        "print(\"üìà Performance Analysis and System Benchmarking...\")\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "def measure_performance(func, name, *args, **kwargs):\n",
        "    \"\"\"Measure function performance\"\"\"\n",
        "    gc.collect()  # Clean memory before measurement\n",
        "    \n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    try:\n",
        "        result = func(*args, **kwargs)\n",
        "        success = True\n",
        "        error = None\n",
        "    except Exception as e:\n",
        "        result = None\n",
        "        success = False\n",
        "        error = str(e)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'duration': end_time - start_time,\n",
        "        'memory_used': end_memory - start_memory,\n",
        "        'success': success,\n",
        "        'error': error,\n",
        "        'result': result\n",
        "    }\n",
        "\n",
        "# Performance benchmarks\n",
        "benchmarks = []\n",
        "\n",
        "print(\"üß™ Running Performance Benchmarks...\")\n",
        "\n",
        "# Benchmark 1: Pipeline initialization\n",
        "bench1 = measure_performance(\n",
        "    lambda: PipelineUtils(results_dir=\"data/05_output/results\"),\n",
        "    \"Pipeline Initialization\"\n",
        ")\n",
        "benchmarks.append(bench1)\n",
        "\n",
        "# Benchmark 2: Configuration template creation\n",
        "bench2 = measure_performance(\n",
        "    lambda: config_manager.create_template_json(\"perf_test.json\"),\n",
        "    \"Configuration Template Creation\"\n",
        ")\n",
        "benchmarks.append(bench2)\n",
        "\n",
        "# Benchmark 3: Quick status check\n",
        "bench3 = measure_performance(\n",
        "    pipeline.quick_test,\n",
        "    \"Quick Status Check\"\n",
        ")\n",
        "benchmarks.append(bench3)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nüìä Performance Benchmark Results:\")\n",
        "print(\"-\" * 60)\n",
        "for bench in benchmarks:\n",
        "    status = \"‚úÖ\" if bench['success'] else \"‚ùå\"\n",
        "    print(f\"{status} {bench['name']:<30} {bench['duration']:>8.3f}s {bench['memory_used']:>8.1f}MB\")\n",
        "    if not bench['success']:\n",
        "        print(f\"   Error: {bench['error']}\")\n",
        "\n",
        "# System resource usage\n",
        "print(f\"\\nüíª Current System Resources:\")\n",
        "print(f\"   ‚Ä¢ CPU Usage: {psutil.cpu_percent():.1f}%\")\n",
        "print(f\"   ‚Ä¢ Memory Usage: {psutil.virtual_memory().percent:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Available Memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Performance analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 5.2: Debug Tools and System Cleanup\n",
        "# ============================================\n",
        "\n",
        "print(\"üîß Debug Tools and System Maintenance...\")\n",
        "\n",
        "# System cleanup functions\n",
        "def cleanup_temp_files():\n",
        "    \"\"\"Remove temporary files\"\"\"\n",
        "    temp_patterns = [\"perf_test.json\", \"dev_simulation_config.*\"]\n",
        "    cleaned = 0\n",
        "    \n",
        "    for pattern in temp_patterns:\n",
        "        if \"*\" in pattern:\n",
        "            import glob\n",
        "            files = glob.glob(pattern)\n",
        "            for file in files:\n",
        "                try:\n",
        "                    Path(file).unlink()\n",
        "                    cleaned += 1\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            try:\n",
        "                if Path(pattern).exists():\n",
        "                    Path(pattern).unlink()\n",
        "                    cleaned += 1\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def check_log_files():\n",
        "    \"\"\"Check system log files\"\"\"\n",
        "    log_dirs = [\"logs\", \"data/05_output/logs\"]\n",
        "    log_files = []\n",
        "    \n",
        "    for log_dir in log_dirs:\n",
        "        if Path(log_dir).exists():\n",
        "            for log_file in Path(log_dir).glob(\"*.log\"):\n",
        "                size = log_file.stat().st_size\n",
        "                log_files.append({\n",
        "                    'file': str(log_file),\n",
        "                    'size': size,\n",
        "                    'age': time.time() - log_file.stat().st_mtime\n",
        "                })\n",
        "    \n",
        "    return log_files\n",
        "\n",
        "# Run debug tools\n",
        "print(\"üóëÔ∏è Running System Cleanup...\")\n",
        "cleaned_files = cleanup_temp_files()\n",
        "print(f\"   ‚Ä¢ Cleaned {cleaned_files} temporary files\")\n",
        "\n",
        "print(\"\\nüìã Checking Log Files...\")\n",
        "log_files = check_log_files()\n",
        "if log_files:\n",
        "    print(f\"   ‚Ä¢ Found {len(log_files)} log files:\")\n",
        "    for log in log_files[-5:]:  # Show last 5\n",
        "        age_hours = log['age'] / 3600\n",
        "        print(f\"     - {Path(log['file']).name} ({log['size']} bytes, {age_hours:.1f}h old)\")\n",
        "else:\n",
        "    print(\"   ‚Ä¢ No log files found\")\n",
        "\n",
        "# Memory cleanup\n",
        "print(\"\\nüíæ Memory Cleanup...\")\n",
        "gc.collect()\n",
        "print(\"   ‚Ä¢ Garbage collection completed\")\n",
        "\n",
        "# Final status\n",
        "print(f\"\\nüìä Development Session Summary:\")\n",
        "print(f\"   ‚Ä¢ Session Duration: {(datetime.now() - start_time).total_seconds():.1f} seconds\")\n",
        "print(f\"   ‚Ä¢ Benchmarks Run: {len(benchmarks)}\")\n",
        "print(f\"   ‚Ä¢ Components Tested: {'‚úÖ' if all(b['success'] for b in benchmarks) else '‚ö†Ô∏è'}\")\n",
        "\n",
        "print(\"\\n‚úÖ Debug tools and cleanup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 6: Version Control & Maintenance** üìö\n",
        "\n",
        "Optional version control, documentation updates, and system maintenance tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 6.1: Version Control and Documentation\n",
        "# ===============================================\n",
        "# Uncomment and run these cells for version control operations\n",
        "\n",
        "# from datetime import datetime\n",
        "# import subprocess\n",
        "\n",
        "# def update_github():\n",
        "#     \"\"\"Update GitHub repository with changes\"\"\"\n",
        "#     print(\"üì¶ Starting GitHub update...\")\n",
        "#     \n",
        "#     try:\n",
        "#         # Add all changes\n",
        "#         subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
        "#         print(\"   ‚úÖ Files added to staging\")\n",
        "#         \n",
        "#         # Commit with timestamp\n",
        "#         commit_msg = f\"Developer update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "#         subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], check=True)\n",
        "#         print(\"   ‚úÖ Changes committed\")\n",
        "#         \n",
        "#         # Push to origin\n",
        "#         subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "#         print(\"   ‚úÖ Changes pushed to GitHub\")\n",
        "#         \n",
        "#         return True\n",
        "#         \n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         print(f\"   ‚ùå Git operation failed: {e}\")\n",
        "#         return False\n",
        "\n",
        "# def log_development_session():\n",
        "#     \"\"\"Log this development session\"\"\"\n",
        "#     log_file = \"update_log.txt\"\n",
        "#     \n",
        "#     with open(log_file, \"a\") as f:\n",
        "#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "#         f.write(f\"\\n{timestamp}: Developer session - System testing and validation\\n\")\n",
        "#     \n",
        "#     print(f\"‚úÖ Development session logged to {log_file}\")\n",
        "\n",
        "# # Uncomment to run version control operations:\n",
        "# # log_development_session()\n",
        "# # update_github()\n",
        "\n",
        "print(\"üîß Version control tools ready (uncomment to use)\")\n",
        "print(\"üí° Manual operations available:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
