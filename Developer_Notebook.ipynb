{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Developer Notebook** ğŸ› ï¸\n",
        "\n",
        "## **Advanced Development Environment**\n",
        "\n",
        "This notebook contains advanced features, diagnostics, and development tools for PyNucleus.\n",
        "\n",
        "### **ğŸ”§ Developer Features:**\n",
        "- **System Diagnostics**: Comprehensive health checks and validation\n",
        "- **Enhanced Pipeline**: Advanced configuration and integration\n",
        "- **LLM Development**: Prompt engineering and model testing\n",
        "- **Performance Analysis**: Detailed metrics and benchmarking\n",
        "- **Configuration Management**: Custom templates and settings\n",
        "- **Debug Tools**: Logging, tracing, and error analysis\n",
        "\n",
        "### **ğŸ“‹ Notebook Sections:**\n",
        "1. **System Initialization & Diagnostics** (Cells 1-3)\n",
        "2. **Enhanced Pipeline Configuration** (Cells 4-6)\n",
        "3. **Advanced Analysis & Integration** (Cells 7-9)\n",
        "4. **LLM Development & Testing** (Cells 10-12)\n",
        "5. **Performance & Debugging** (Cells 13-15)\n",
        "6. **Version Control & Maintenance** (Cells 16-18)\n",
        "\n",
        "---\n",
        "**âš ï¸ Warning**: This notebook is intended for developers and advanced users. For basic usage, use `Capstone Project.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ PyNucleus Developer Environment - Starting Initialization...\n",
            "ğŸ“… Session started: 2025-06-18 00:06:42\n",
            "âœ… All PyNucleus modules imported successfully\n",
            "ğŸ”§ Setting up RAG imports...\n",
            "Warning: wikipedia package not available. Wikipedia scraping disabled.\n",
            "âœ… RAG imports ready!\n",
            "ğŸ“‚ Loaded 5 existing DWSIM results from disk\n",
            "ğŸ”§ Setting up DWSIM imports...\n",
            "âœ… DWSIM modules imported successfully\n",
            "ğŸ“ Results directory: data/05_output/results\n",
            "ğŸ”§ Pipeline Utils initialized with results dir: data/05_output/results\n",
            "ğŸ”— DWSIM-RAG integration enabled\n",
            "âœ… Core components initialized\n",
            "ğŸ¯ Ready for development and testing!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 1: System Initialization & Diagnostics\n",
        "# ===============================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ”§ PyNucleus Developer Environment - Starting Initialization...\")\n",
        "print(f\"ğŸ“… Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Import all PyNucleus components\n",
        "try:\n",
        "    from pynucleus.pipeline import RAGPipeline, DWSIMPipeline, ResultsExporter, PipelineUtils\n",
        "    from pynucleus.integration.config_manager import ConfigManager\n",
        "    from pynucleus.integration.dwsim_rag_integrator import DWSIMRAGIntegrator\n",
        "    from pynucleus.integration.llm_output_generator import LLMOutputGenerator\n",
        "    from pynucleus.llm import LLMRunner\n",
        "    from pynucleus.llm.query_llm import LLMQueryManager, quick_ask_llm\n",
        "    \n",
        "    print(\"âœ… All PyNucleus modules imported successfully\")\n",
        "    \n",
        "    # Initialize components\n",
        "    pipeline = PipelineUtils(results_dir=\"data/05_output/results\")\n",
        "    config_manager = ConfigManager(config_dir=\"configs\")\n",
        "    dwsim_rag_integrator = DWSIMRAGIntegrator(\n",
        "        rag_pipeline=None,  # Will be set after pipeline initialization\n",
        "        results_dir=\"data/05_output/results\"\n",
        "    )\n",
        "    llm_generator = LLMOutputGenerator(results_dir=\"data/05_output/llm_reports\")\n",
        "    \n",
        "    print(\"âœ… Core components initialized\")\n",
        "    print(\"ğŸ¯ Ready for development and testing!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Initialization error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Running Comprehensive System Diagnostic...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(76265) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… System diagnostic completed successfully\n",
            "   ğŸŸ¢ SYSTEM HEALTH: 100.0% - EXCELLENT\n",
            "\n",
            "ğŸ” Manual System Checks:\n",
            "   âœ… data/01_raw\n",
            "   âœ… data/02_processed\n",
            "   âœ… data/03_intermediate\n",
            "   âœ… data/04_models\n",
            "   âœ… data/05_output\n",
            "   âœ… src/pynucleus/pipeline\n",
            "   âœ… src/pynucleus/rag\n",
            "   âœ… src/pynucleus/integration\n",
            "   âœ… src/pynucleus/llm\n",
            "\n",
            "ğŸ¯ System diagnostic complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 1.2: Comprehensive System Diagnostic\n",
        "# ============================================\n",
        "\n",
        "print(\"ğŸ” Running Comprehensive System Diagnostic...\")\n",
        "\n",
        "try:\n",
        "    # Run system diagnostic\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"scripts/comprehensive_system_diagnostic.py\", \"--quiet\"\n",
        "    ], capture_output=True, text=True, cwd=\".\")\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… System diagnostic completed successfully\")\n",
        "        # Extract key metrics from output\n",
        "        lines = result.stdout.strip().split('\\n')\n",
        "        for line in lines[-10:]:  # Show last 10 lines for summary\n",
        "            if any(keyword in line for keyword in ['Health:', 'Status:', 'EXCELLENT', 'GOOD', 'passed']):\n",
        "                print(f\"   {line}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ System diagnostic issues detected:\")\n",
        "        print(result.stderr)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Could not run system diagnostic: {e}\")\n",
        "    print(\"ğŸ’¡ Continuing with manual checks...\")\n",
        "\n",
        "# Manual system checks\n",
        "print(\"\\nğŸ” Manual System Checks:\")\n",
        "\n",
        "# Check data directories\n",
        "data_dirs = ['data/01_raw', 'data/02_processed', 'data/03_intermediate', 'data/04_models', 'data/05_output']\n",
        "for dir_path in data_dirs:\n",
        "    exists = Path(dir_path).exists()\n",
        "    print(f\"   {'âœ…' if exists else 'âŒ'} {dir_path}\")\n",
        "\n",
        "# Check src structure\n",
        "src_dirs = ['src/pynucleus/pipeline', 'src/pynucleus/rag', 'src/pynucleus/integration', 'src/pynucleus/llm']\n",
        "for dir_path in src_dirs:\n",
        "    exists = Path(dir_path).exists()\n",
        "    print(f\"   {'âœ…' if exists else 'âŒ'} {dir_path}\")\n",
        "\n",
        "print(\"\\nğŸ¯ System diagnostic complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Detailed Pipeline Status Check...\n",
            "\n",
            "ğŸ”§ PyNucleus Pipeline Status\n",
            "============================================================\n",
            "âš™ï¸ Configuration:\n",
            "   Results Directory: data/05_output/results\n",
            "   LLM Output Directory: data/05_output/llm_reports\n",
            "   DWSIM Integration: âœ… Enabled\n",
            "\n",
            "ğŸ“¦ Components:\n",
            "   RAG Pipeline: âœ… Ready\n",
            "   DWSIM Pipeline: âœ… Ready\n",
            "   Results Exporter: âœ… Ready\n",
            "\n",
            "ğŸ“Š RAG Pipeline Status:\n",
            "==================================================\n",
            "ğŸ“š Total Chunks: 7,141\n",
            "ğŸ“„ Document Sources: 19\n",
            "ğŸ”— Integration Status: Documents only\n",
            "ğŸ“ Average Chunk Size: 787.7 characters\n",
            "ğŸ” Vector Store: Not built\n",
            "==================================================\n",
            "\n",
            "ğŸ”— Integration Capabilities:\n",
            "   ğŸ“„ Document Processing: âœ… Available\n",
            "   ğŸ”¬ Simulation Integration: âœ… Available\n",
            "   ğŸ¤– LLM Querying: âœ… Enhanced (docs + sims)\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "ğŸ§ª Quick Pipeline Test\n",
            "------------------------------\n",
            "ğŸ“š RAG: 7141 chunks available\n",
            "ğŸ”— Integration: âšª Documents only\n",
            "ğŸ“Š DWSIM Statistics:\n",
            "   â€¢ Total Simulations: 5\n",
            "   â€¢ Success Rate: 100.0%\n",
            "   â€¢ Average Duration: 0.00s\n",
            "ğŸ”¬ DWSIM: 5 simulations\n",
            "ğŸ“ Output: 2 CSV files\n",
            "ğŸ“ Results Directory: data/05_output/results\n",
            "ğŸ“„ CSV Files: 2\n",
            "\n",
            "ğŸ“‹ Existing Files:\n",
            "   â€¢ dwsim_summary.csv (360 bytes)\n",
            "   â€¢ dwsim_simulation_results.csv (914 bytes)\n",
            "\n",
            "ğŸ”§ Component Health:\n",
            "   â€¢ Pipeline Utils: âœ…\n",
            "   â€¢ Config Manager: âœ…\n",
            "   â€¢ DWSIM-RAG Integrator: âœ…\n",
            "   â€¢ LLM Generator: âœ…\n",
            "\n",
            "ğŸ” Pipeline Component Status:\n",
            "   â€¢ RAG Pipeline: âœ…\n",
            "   â€¢ DWSIM Pipeline: âœ…\n",
            "   â€¢ Results Exporter: âœ…\n",
            "\n",
            "ğŸ”— Integration Status:\n",
            "   â€¢ Total RAG Chunks: 7,141\n",
            "   â€¢ Simulation Chunks: 0\n",
            "   â€¢ Integration Active: âšª\n",
            "\n",
            "âœ… Status check complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 1.3: Pipeline Status and Health Check\n",
        "# =============================================\n",
        "\n",
        "print(\"ğŸ“Š Detailed Pipeline Status Check...\")\n",
        "\n",
        "try:\n",
        "    # Pipeline component status\n",
        "    pipeline.print_pipeline_status()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Quick test with validation\n",
        "    test_results = pipeline.quick_test()\n",
        "    \n",
        "    # Validate test_results\n",
        "    if test_results is None:\n",
        "        print(\"âŒ Quick test returned None - pipeline may not be properly initialized\")\n",
        "        test_results = {\n",
        "            'results_dir': 'data/05_output/results',\n",
        "            'csv_files_count': 0,\n",
        "            'csv_files': []\n",
        "        }\n",
        "    \n",
        "    # Safely access results with fallbacks\n",
        "    results_dir = test_results.get('results_dir', 'data/05_output/results')\n",
        "    csv_files_count = test_results.get('csv_files_count', 0)\n",
        "    csv_files = test_results.get('csv_files', [])\n",
        "    \n",
        "    print(f\"ğŸ“ Results Directory: {results_dir}\")\n",
        "    print(f\"ğŸ“„ CSV Files: {csv_files_count}\")\n",
        "    \n",
        "    if csv_files_count > 0:\n",
        "        print(\"\\nğŸ“‹ Existing Files:\")\n",
        "        for file_info in csv_files:\n",
        "            if isinstance(file_info, dict):\n",
        "                name = file_info.get('name', 'Unknown')\n",
        "                size = file_info.get('size', 0)\n",
        "                print(f\"   â€¢ {name} ({size} bytes)\")\n",
        "            else:\n",
        "                print(f\"   â€¢ {file_info}\")\n",
        "    \n",
        "    # Component health\n",
        "    print(f\"\\nğŸ”§ Component Health:\")\n",
        "    print(f\"   â€¢ Pipeline Utils: {'âœ…' if hasattr(pipeline, 'rag_pipeline') else 'âš ï¸'}\")\n",
        "    print(f\"   â€¢ Config Manager: {'âœ…' if config_manager else 'âŒ'}\")\n",
        "    print(f\"   â€¢ DWSIM-RAG Integrator: {'âœ…' if dwsim_rag_integrator else 'âŒ'}\")\n",
        "    print(f\"   â€¢ LLM Generator: {'âœ…' if llm_generator else 'âŒ'}\")\n",
        "    \n",
        "    # Additional diagnostics\n",
        "    component_status = test_results.get('component_status', {})\n",
        "    print(f\"\\nğŸ” Pipeline Component Status:\")\n",
        "    print(f\"   â€¢ RAG Pipeline: {'âœ…' if component_status.get('rag_pipeline', False) else 'âŒ'}\")\n",
        "    print(f\"   â€¢ DWSIM Pipeline: {'âœ…' if component_status.get('dwsim_pipeline', False) else 'âŒ'}\")\n",
        "    print(f\"   â€¢ Results Exporter: {'âœ…' if component_status.get('exporter', False) else 'âŒ'}\")\n",
        "    \n",
        "    # Integration status\n",
        "    integration_enabled = test_results.get('integration_enabled', False)\n",
        "    rag_chunks = test_results.get('rag_chunks', 0)\n",
        "    simulation_chunks = test_results.get('simulation_chunks', 0)\n",
        "    \n",
        "    print(f\"\\nğŸ”— Integration Status:\")\n",
        "    print(f\"   â€¢ Total RAG Chunks: {rag_chunks:,}\")\n",
        "    print(f\"   â€¢ Simulation Chunks: {simulation_chunks:,}\")\n",
        "    print(f\"   â€¢ Integration Active: {'âœ…' if integration_enabled else 'âšª'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Status check error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    # Provide troubleshooting tips\n",
        "    print(\"\\nğŸ”§ Troubleshooting Tips:\")\n",
        "    print(\"   1. Try restarting the notebook kernel\")\n",
        "    print(\"   2. Re-run Cell 1 to reinitialize components\")\n",
        "    print(\"   3. Check if all required directories exist\")\n",
        "    print(\"   4. Verify PyNucleus installation is complete\")\n",
        "\n",
        "print(\"\\nâœ… Status check complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 2: Enhanced Pipeline Configuration** ğŸ”§\n",
        "\n",
        "Advanced configuration management and template generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Advanced Configuration Management...\n",
            "âœ… Pydantic template created: configs/dev_simulation_config.json\n",
            "âŒ Configuration error: dict contains fields not in fieldnames: 'catalyst_loading'\n",
            "\n",
            "âœ… Configuration management ready!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 2.1: Configuration Templates and Management\n",
        "# ===================================================\n",
        "\n",
        "print(\"ğŸ”§ Advanced Configuration Management...\")\n",
        "\n",
        "# Create configuration templates\n",
        "try:\n",
        "    # Generate JSON and CSV templates\n",
        "    json_template = config_manager.create_template_json(\"dev_simulation_config.json\", verbose=True)\n",
        "    csv_template = config_manager.create_template_csv(\"dev_simulation_config.csv\", verbose=True)\n",
        "    \n",
        "    print(f\"âœ… Configuration templates created:\")\n",
        "    print(f\"   â€¢ JSON: {json_template}\")\n",
        "    print(f\"   â€¢ CSV: {csv_template}\")\n",
        "    \n",
        "    # Show template contents (first few lines)\n",
        "    if Path(json_template).exists():\n",
        "        with open(json_template, 'r') as f:\n",
        "            content = f.read()[:300]\n",
        "            print(f\"\\nğŸ“‹ JSON Template Preview:\")\n",
        "            print(content + \"...\" if len(content) >= 300 else content)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Configuration error: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Configuration management ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Running Enhanced Pipeline for Development Testing...\n",
            "ğŸš€ Running complete PyNucleus pipeline...\n",
            "ğŸ—‘ï¸ RAG results cleared.\n",
            "ğŸ—‘ï¸ RAG results cleared.\n",
            "ğŸ—‘ï¸ DWSIM results cleared from memory and disk.\n",
            "ğŸ—‘ï¸ DWSIM results cleared.\n",
            "ğŸ”¬ Step 1: Running DWSIM simulations...\n",
            "ğŸ”¬ Starting DWSIM Simulations...\n",
            "ğŸ“‹ Running 5 simulation cases...\n",
            "\n",
            "ğŸ§ª Case 1/5: distillation_ethanol_water\n",
            "   âœ… Success - Duration: 0.00s\n",
            "\n",
            "ğŸ§ª Case 2/5: reactor_methane_combustion\n",
            "   âœ… Success - Duration: 0.00s\n",
            "\n",
            "ğŸ§ª Case 3/5: heat_exchanger_steam\n",
            "   âœ… Success - Duration: 0.00s\n",
            "\n",
            "ğŸ§ª Case 4/5: absorber_co2_capture\n",
            "   âœ… Success - Duration: 0.00s\n",
            "\n",
            "ğŸ§ª Case 5/5: crystallizer_salt\n",
            "   âœ… Success - Duration: 0.00s\n",
            "\n",
            "ğŸ“Š Simulation Summary:\n",
            "   â€¢ Successful simulations: 5/5\n",
            "   â€¢ Failed simulations: 0/5\n",
            "ğŸ’¾ Saved 5 DWSIM results to disk\n",
            "ğŸ“Š DWSIM Statistics:\n",
            "   â€¢ Total Simulations: 5\n",
            "   â€¢ Success Rate: 100.0%\n",
            "   â€¢ Average Duration: 0.00s\n",
            "âœ… DWSIM: 5 simulations completed\n",
            "\n",
            "ğŸ“š Step 2: Running RAG pipeline with DWSIM integration...\n",
            "ğŸ“š Starting RAG Pipeline...\n",
            "Step 1: Processing source documents...\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Engineering Economics and Economic Design for Process Engineers.pdf - already processed (output exists: Engineering Economics and Economic Design for Process Engineers.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping The Landscape of Modular Chemical Plants in Africa- Opportunities, Challenges, and Strategic Imperatives for Industrialization.txt - already processed (output exists: The Landscape of Modular Chemical Plants in Africa- Opportunities, Challenges, and Strategic Imperatives for Industrialization.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Process Simulation Specifications.pdf - already processed (output exists: Process Simulation Specifications.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Engineering-Economy.pdf - already processed (output exists: Engineering-Economy.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Modular Chemical Plant Development Guide.pdf - already processed (output exists: Modular Chemical Plant Development Guide.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Modular Chemical Plant Feasibility Parameters.pdf - already processed (output exists: Modular Chemical Plant Feasibility Parameters.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx - already processed (output exists: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping 20180329160017017.pdf - already processed (output exists: 20180329160017017.txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: â­ï¸ Skipping J.M. Smith, Hendrick Van Ness, Michael Abbott, Mark Swihart - Introduction to Chemical Engineering Thermodynamics-McGraw-Hill Education (2018).pdf - already processed (output exists: J.M. Smith, Hendrick Van Ness, Michael Abbott, Mark Swihart - Introduction to Chemical Engineering Thermodynamics-McGraw-Hill Education (2018).txt)\n",
            "2025-06-18T00:06:48 [INFO] pynucleus.rag.document_processor: âœ… No new files to process in data/01_raw/source_documents (all files already processed)\n",
            "\n",
            "Step 2: Scraping Wikipedia articles...\n",
            "ğŸ” Starting Wikipedia article search for 5 keywords...\n",
            "â–¶ï¸  Searching for: modular design\n",
            "âœ…  Saved article to: /Users/mohammadalmusaiteer/PyNucleus-Model/data/01_raw/web_sources/wikipedia_modular_design.txt\n",
            "â–¶ï¸  Searching for: software architecture\n",
            "âœ…  Saved article to: /Users/mohammadalmusaiteer/PyNucleus-Model/data/01_raw/web_sources/wikipedia_software_architecture.txt\n",
            "â–¶ï¸  Searching for: system design\n",
            "âœ…  Saved article to: /Users/mohammadalmusaiteer/PyNucleus-Model/data/01_raw/web_sources/wikipedia_system_design.txt\n",
            "â–¶ï¸  Searching for: industrial design\n",
            "âœ…  Saved article to: /Users/mohammadalmusaiteer/PyNucleus-Model/data/01_raw/web_sources/wikipedia_industrial_design.txt\n",
            "â–¶ï¸  Searching for: supply chain\n",
            "âœ…  Saved article to: /Users/mohammadalmusaiteer/PyNucleus-Model/data/01_raw/web_sources/wikipedia_supply_chain.txt\n",
            "\n",
            "âœ¨ Article scraping complete!\n",
            "\n",
            "Step 3: Processing and chunking documents...\n",
            "ğŸ“° Found 5 Wikipedia articles\n",
            "ğŸ“„ Found 9 converted documents\n",
            "ğŸ“‹ Total documents loaded: 14\n",
            "âœ‚ï¸ Split into 7136 chunks\n",
            "\n",
            "\n",
            "Step 4: Integrating DWSIM simulation data...\n",
            "   ğŸ“Š Using provided DWSIM data: 5 simulations\n",
            "   âœ… Created 5 simulation chunks\n",
            "   ğŸ“Š DWSIM data integrated successfully\n",
            "\n",
            "ğŸ” Enhanced Query Capabilities:\n",
            "   1. What are the performance metrics for the distillation simulation?\n",
            "   2. How do the reactor conversion rates compare across simulations?\n",
            "   3. Which simulation showed the highest efficiency?\n",
            "   ğŸ“Š Combined 7141 total chunks (documents + simulations)\n",
            "   ğŸ”— Simulation chunks properly formatted as Document objects\n",
            "   ğŸ’¾ Saving combined chunk data...\n",
            "   âœ… Chunk data saved successfully\n",
            "\n",
            "Step 5: Building FAISS vector store...\n",
            "=== FAISS VectorDB Analysis ===\n",
            "Started: 2025-06-18 00:06:53\n",
            "   ğŸ”„ Loading documents from saved JSON file...\n",
            "âœ… Loaded 7141 documents from /Users/mohammadalmusaiteer/PyNucleus-Model/data/03_intermediate/converted_chunked_data/chunked_data_full.json\n",
            "   âœ… Documents properly loaded: 7141 Document objects\n",
            "ğŸ” Building FAISS index with 7141 documents\n",
            "   First document type: <class 'langchain_core.documents.base.Document'>\n",
            "   Has page_content: âœ…\n",
            "âœ… FAISS index built successfully with langchain\n",
            "Embedding device â†’ cpu   | dim=384\n",
            "Docs indexed : 7141\n",
            "\n",
            "-- Files in chunk_reports/ --\n",
            "  Â· pynucleus_enhanced.faiss.faiss\n",
            "  Â· faiss_analysis_20250618_000542.txt\n",
            "  Â· embeddings.pkl\n",
            "  Â· faiss_analysis_20250618_000653.txt\n",
            "  Â· document_hashes.json\n",
            "  Â· qwen_embeddings.pkl\n",
            "  Â· pynucleus_enhanced.faiss.pkl\n",
            "  Â· pynucleus_mcp.faiss\n",
            "  Â· performance_metrics.jsonl\n",
            "  Â· faiss_analysis_20250618_000417.txt\n",
            "\n",
            "=== Evaluation (Recall@3) ===\n",
            "Q: what are the benefits of modular design  âœ—   top-score=0.4899   time=0.012s\n",
            "Q: how does modular design work in vehicles âœ—   top-score=0.4153   time=0.005s\n",
            "\n",
            "Recall@3: 0/2  â†’  0.0%\n",
            "Avg Similarity Score: 0.4526\n",
            "Avg Response Time: 0.008s\n",
            "âš ï¸ FAISS build encountered an issue: Object of type float32 is not JSON serializable\n",
            "   ğŸ”§ Continuing with basic document processing...\n",
            "=== FAISS VectorDB Analysis ===\n",
            "Started: 2025-06-18 00:08:17\n",
            "âœ… Loaded 7141 documents from /Users/mohammadalmusaiteer/PyNucleus-Model/data/03_intermediate/converted_chunked_data/chunked_data_full.json\n",
            "âœ… RAG Pipeline completed with fallback processing\n",
            "ğŸ” Testing RAG queries...\n",
            "\n",
            "ğŸ“ Query: What are the key challenges in implementing modular chemical plants?\n",
            "   âŒ Query failed: No search index available. Call build() first.\n",
            "\n",
            "ğŸ“ Query: How does supply chain management affect modular design?\n",
            "   âŒ Query failed: No search index available. Call build() first.\n",
            "\n",
            "ğŸ“ Query: What are the economic benefits of modular construction?\n",
            "   âŒ Query failed: No search index available. Call build() first.\n",
            "\n",
            "ğŸ“ Query: How does software architecture relate to modular design?\n",
            "   âŒ Query failed: No search index available. Call build() first.\n",
            "\n",
            "ğŸ“ Query: What are the environmental impacts of modular manufacturing?\n",
            "   âŒ Query failed: No search index available. Call build() first.\n",
            "âœ… Query testing completed! 0 results collected.\n",
            "âœ… RAG: 7141 chunks processed\n",
            "\n",
            "ğŸ’¾ Step 3: Exporting results...\n",
            "\n",
            "ğŸ’¾ Exporting DWSIM simulation results...\n",
            "âœ… DWSIM results exported: data/05_output/results/dwsim_simulation_results.csv\n",
            "   ğŸ“Š 5 simulation results exported\n",
            "   ğŸ“‹ Columns: Case Name, Type, Components, Status, Performance Metrics\n",
            "âœ… DWSIM summary exported: data/05_output/results/dwsim_summary.csv\n",
            "\n",
            "ğŸ‰ Export completed successfully!\n",
            "ğŸ“ All results saved in: data/05_output/results\n",
            "ğŸ“ˆ Files created:\n",
            "   â€¢ dwsim_simulation_results.csv (918 bytes)\n",
            "   â€¢ dwsim_summary.csv (360 bytes)\n",
            "âœ… Export: 2 files created\n",
            "\n",
            "âœ… Complete pipeline finished in 90.5 seconds!\n",
            "ğŸ“Š Components completed: 3/3\n",
            "\n",
            "ğŸ‰ Pipeline completed in 90.5 seconds!\n",
            "\n",
            "ğŸ“Š Detailed Results:\n",
            "   â€¢ RAG Queries: 0\n",
            "   â€¢ DWSIM Simulations: 5\n",
            "   â€¢ Export Files: 2\n",
            "âœ… Pipeline data ready for enhanced analysis\n",
            "\n",
            "âœ… Enhanced pipeline testing complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 2.2: Run Enhanced Pipeline with Full Analysis\n",
        "# =====================================================\n",
        "\n",
        "print(\"ğŸš€ Running Enhanced Pipeline for Development Testing...\")\n",
        "\n",
        "# Run complete pipeline with detailed logging\n",
        "try:\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Execute pipeline\n",
        "    results = pipeline.run_complete_pipeline()\n",
        "    \n",
        "    if results:\n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"\\nğŸ‰ Pipeline completed in {duration:.1f} seconds!\")\n",
        "        \n",
        "        # Detailed results analysis\n",
        "        print(f\"\\nğŸ“Š Detailed Results:\")\n",
        "        print(f\"   â€¢ RAG Queries: {len(results.get('rag_data', []))}\")\n",
        "        print(f\"   â€¢ DWSIM Simulations: {len(results.get('dwsim_data', []))}\")\n",
        "        print(f\"   â€¢ Export Files: {len(results.get('exported_files', []))}\")\n",
        "        \n",
        "        # Set up integrator with pipeline data\n",
        "        if hasattr(pipeline, 'rag_pipeline'):\n",
        "            dwsim_rag_integrator.rag_pipeline = pipeline.rag_pipeline\n",
        "        \n",
        "        print(\"âœ… Pipeline data ready for enhanced analysis\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ Pipeline execution failed\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Enhanced pipeline error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nâœ… Enhanced pipeline testing complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 3: Advanced Analysis & Integration** ğŸ”¬\n",
        "\n",
        "DWSIM-RAG integration, financial analysis, and enhanced reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¬ Advanced DWSIM-RAG Integration Analysis...\n",
            "ğŸ“Š Processing 5 DWSIM simulations...\n",
            "âœ… Enhanced 5 simulations with RAG insights\n",
            "âœ… Enhanced integration complete:\n",
            "   â€¢ Integrated simulations: 5\n",
            "   â€¢ Export file: data/05_output/results/integrated_dwsim_rag_results_20250618_000819.json\n",
            "\\nğŸ“‹ Sample Analysis (First Simulation):\n",
            "   â€¢ Case: distillation_ethanol_water\n",
            "   ğŸ“Š Performance Metrics:\n",
            "      â€¢ Overall Performance: Good\n",
            "      â€¢ Efficiency Rating: High\n",
            "      â€¢ Reliability Score: High\n",
            "      â€¢ Performance Indicators: 4 items\n",
            "        - Process Type: distillation\n",
            "        - Success Status: True\n",
            "        - Duration Seconds: 0.0003\n",
            "        - Timestamp: 2025-06-18 00:06:48\n",
            "      â€¢ Conversion: 88.900\n",
            "      â€¢ Selectivity: 97.600\n",
            "      â€¢ Yield: 89.800\n",
            "      â€¢ Recovery Rate: 86.8%\n",
            "   ğŸ“‹ Recommendations: 1 items\n",
            "      1. Simulation completed successfully - results are ready for analysis\n",
            "   ğŸ“‹ Optimization Opportunities: 1 items\n",
            "      1. Consider heat integration for energy efficiency\n",
            "\\nâœ… Advanced integration analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 3.1: DWSIM-RAG Integration and Enhanced Analysis\n",
        "# ========================================================\n",
        "\n",
        "print(\"ğŸ”¬ Advanced DWSIM-RAG Integration Analysis...\")\n",
        "\n",
        "try:\n",
        "    # Get DWSIM results\n",
        "    dwsim_results = pipeline.dwsim_pipeline.get_results()\n",
        "    \n",
        "    if dwsim_results:\n",
        "        print(f\"ğŸ“Š Processing {len(dwsim_results)} DWSIM simulations...\")\n",
        "        \n",
        "        # Perform enhanced integration\n",
        "        integrated_results = dwsim_rag_integrator.integrate_simulation_results(\n",
        "            dwsim_results, perform_rag_analysis=True\n",
        "        )\n",
        "        \n",
        "        # Export integrated results\n",
        "        integrated_export_file = dwsim_rag_integrator.export_integrated_results()\n",
        "        \n",
        "        print(f\"âœ… Enhanced integration complete:\")\n",
        "        print(f\"   â€¢ Integrated simulations: {len(integrated_results)}\")\n",
        "        print(f\"   â€¢ Export file: {integrated_export_file}\")\n",
        "        \n",
        "        # Show detailed analysis for first simulation\n",
        "        if integrated_results:\n",
        "            sample = integrated_results[0]\n",
        "            print(f\"\\\\nğŸ“‹ Sample Analysis (First Simulation):\")\n",
        "            \n",
        "            # Safely access original simulation data\n",
        "            original_sim = sample.get('original_simulation', {})\n",
        "            print(f\"   â€¢ Case: {original_sim.get('case_name', 'Unknown')}\")\n",
        "            \n",
        "            # Dynamically display all available performance metrics\n",
        "            perf_metrics = sample.get('performance_metrics', {})\n",
        "            if perf_metrics:\n",
        "                print(f\"   ğŸ“Š Performance Metrics:\")\n",
        "                for key, value in perf_metrics.items():\n",
        "                    # Format the key for display (convert snake_case to Title Case)\n",
        "                    display_key = key.replace('_', ' ').title()\n",
        "                    \n",
        "                    # Handle different value types appropriately\n",
        "                    if isinstance(value, (int, float)):\n",
        "                        if 'rate' in key.lower() or 'percentage' in key.lower():\n",
        "                            print(f\"      â€¢ {display_key}: {value:.1f}%\")\n",
        "                        elif isinstance(value, float):\n",
        "                            print(f\"      â€¢ {display_key}: {value:.3f}\")\n",
        "                        else:\n",
        "                            print(f\"      â€¢ {display_key}: {value}\")\n",
        "                    elif isinstance(value, dict):\n",
        "                        print(f\"      â€¢ {display_key}: {len(value)} items\")\n",
        "                        # Show nested dict items if not too many\n",
        "                        if len(value) <= 5:\n",
        "                            for sub_key, sub_value in value.items():\n",
        "                                sub_display_key = sub_key.replace('_', ' ').title()\n",
        "                                print(f\"        - {sub_display_key}: {sub_value}\")\n",
        "                    elif isinstance(value, list):\n",
        "                        print(f\"      â€¢ {display_key}: {len(value)} items\")\n",
        "                    else:\n",
        "                        print(f\"      â€¢ {display_key}: {value}\")\n",
        "            else:\n",
        "                print(f\"   âš ï¸ No performance metrics available\")\n",
        "            \n",
        "            # Show other analysis results dynamically\n",
        "            analysis_sections = [\n",
        "                ('potential_issues', 'Potential Issues'),\n",
        "                ('recommendations', 'Recommendations'), \n",
        "                ('optimization_opportunities', 'Optimization Opportunities'),\n",
        "                ('rag_insights', 'RAG Insights')\n",
        "            ]\n",
        "            \n",
        "            for section_key, section_title in analysis_sections:\n",
        "                section_data = sample.get(section_key, [])\n",
        "                if section_data:\n",
        "                    print(f\"   ğŸ“‹ {section_title}: {len(section_data)} items\")\n",
        "                    # Show first few items as examples\n",
        "                    for i, item in enumerate(section_data[:3]):\n",
        "                        if isinstance(item, dict):\n",
        "                            # For RAG insights or complex items\n",
        "                            item_summary = item.get('content', item.get('query', str(item)))[:100]\n",
        "                            print(f\"      {i+1}. {item_summary}...\")\n",
        "                        else:\n",
        "                            # For simple string items\n",
        "                            print(f\"      {i+1}. {item}\")\n",
        "                    if len(section_data) > 3:\n",
        "                        print(f\"      ... and {len(section_data) - 3} more\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸ No DWSIM results available. Run Section 2.2 first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Integration error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\nâœ… Advanced integration analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’° Advanced Financial Analysis and LLM Report Generation...\n",
            "ğŸ“„ Generating LLM reports for 5 simulations...\n",
            "   âœ… Report 1: distillation_ethanol_water_summary.md\n",
            "   âœ… Report 2: reactor_methane_combustion_summary.md\n",
            "   âœ… Report 3: heat_exchanger_steam_summary.md\n",
            "   âœ… Report 4: absorber_co2_capture_summary.md\n",
            "   âœ… Report 5: crystallizer_salt_summary.md\n",
            "\\nğŸ’° Comprehensive Financial Metrics:\n",
            "   â€¢ Average Recovery Rate: 83.9%\n",
            "   â€¢ Estimated Daily Revenue: $151,056.00\n",
            "   â€¢ Estimated Daily Profit: $61,056.00\n",
            "   â€¢ Return on Investment: 6.8%\n",
            "   â€¢ Financial Analysis File: data/05_output/llm_reports/financial_analysis_20250618_000819.csv\n",
            "\\nğŸ“Š Performance Summary:\n",
            "   â€¢ High Performance Simulations: 2/5\n",
            "   â€¢ High Efficiency Rate: 40.0%\n",
            "\\nğŸ“„ Generated Files:\n",
            "   â€¢ LLM Reports: 5 files\n",
            "   â€¢ Financial Analysis: 1 file\n",
            "\\nâœ… Advanced analysis and reporting complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 3.2: LLM Report Generation and Financial Analysis\n",
        "# =========================================================\n",
        "\n",
        "print(\"ğŸ’° Advanced Financial Analysis and LLM Report Generation...\")\n",
        "\n",
        "try:\n",
        "    if 'integrated_results' in locals() and integrated_results:\n",
        "        \n",
        "        # Generate LLM reports for all simulations\n",
        "        print(f\"ğŸ“„ Generating LLM reports for {len(integrated_results)} simulations...\")\n",
        "        \n",
        "        llm_report_files = []\n",
        "        for i, result in enumerate(integrated_results):\n",
        "            try:\n",
        "                report_file = llm_generator.export_llm_ready_text(result)\n",
        "                llm_report_files.append(report_file)\n",
        "                print(f\"   âœ… Report {i+1}: {Path(report_file).name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Report {i+1} failed: {e}\")\n",
        "        \n",
        "        # Generate comprehensive financial analysis\n",
        "        financial_file = llm_generator.export_financial_analysis(integrated_results)\n",
        "        metrics = llm_generator._calculate_key_metrics(integrated_results)\n",
        "        \n",
        "        print(f\"\\\\nğŸ’° Comprehensive Financial Metrics:\")\n",
        "        print(f\"   â€¢ Average Recovery Rate: {metrics['avg_recovery']:.1f}%\")\n",
        "        print(f\"   â€¢ Estimated Daily Revenue: ${metrics['estimated_revenue']:,.2f}\")\n",
        "        print(f\"   â€¢ Estimated Daily Profit: ${metrics['net_profit']:,.2f}\")\n",
        "        print(f\"   â€¢ Return on Investment: {metrics['roi']:.1f}%\")\n",
        "        print(f\"   â€¢ Financial Analysis File: {financial_file}\")\n",
        "        \n",
        "        # Performance summary\n",
        "        print(f\"\\\\nğŸ“Š Performance Summary:\")\n",
        "        good_performance = sum(1 for r in integrated_results \n",
        "                             if r['performance_metrics']['overall_performance'] == 'Good')\n",
        "        print(f\"   â€¢ High Performance Simulations: {good_performance}/{len(integrated_results)}\")\n",
        "        \n",
        "        avg_efficiency = sum(1 for r in integrated_results \n",
        "                           if r['performance_metrics']['efficiency_rating'] == 'High') / len(integrated_results)\n",
        "        print(f\"   â€¢ High Efficiency Rate: {avg_efficiency:.1%}\")\n",
        "        \n",
        "        print(f\"\\\\nğŸ“„ Generated Files:\")\n",
        "        print(f\"   â€¢ LLM Reports: {len(llm_report_files)} files\")\n",
        "        print(f\"   â€¢ Financial Analysis: 1 file\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸ No integrated results available. Run Section 3.1 first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ LLM/Financial analysis error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\nâœ… Advanced analysis and reporting complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 4: LLM Development & Testing** ğŸ¤–\n",
        "\n",
        "LLM model testing, prompt engineering, and query development.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– LLM Development Environment Initialization...\n",
            "Loading tokenizer for gpt2...\n",
            "Loading model gpt2 on cpu...\n",
            "Model loaded successfully on cpu\n",
            "Loading tokenizer for gpt2...\n",
            "Loading model gpt2 on cpu...\n",
            "Model loaded successfully on cpu\n",
            "âœ… LLM Runner initialized\n",
            "âœ… LLM Query Manager initialized\n",
            "   â€¢ Template directory: /Users/mohammadalmusaiteer/PyNucleus-Model/prompts\n",
            "   â€¢ Template exists: True\n",
            "\\nğŸ”§ Model Information:\n",
            "   â€¢ Model ID: gpt2\n",
            "   â€¢ Vocabulary Size: 50,257\n",
            "   â€¢ Device: cpu\n",
            "\\nğŸ“‹ Prompt System Test:\n",
            "   â€¢ Template rendering: âœ… Success\n",
            "   â€¢ Prompt length: 289 characters\n",
            "\\nâœ… LLM development environment ready!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 4.1: LLM Model Testing and Initialization\n",
        "# =================================================\n",
        "\n",
        "print(\"ğŸ¤– LLM Development Environment Initialization...\")\n",
        "\n",
        "try:\n",
        "    # Initialize LLM components\n",
        "    llm_runner = LLMRunner()\n",
        "    llm_query_manager = LLMQueryManager(max_tokens=2048)\n",
        "    \n",
        "    # Test LLM functionality\n",
        "    print(f\"âœ… LLM Runner initialized\")\n",
        "    print(f\"âœ… LLM Query Manager initialized\")\n",
        "    print(f\"   â€¢ Template directory: {llm_query_manager.template_dir}\")\n",
        "    print(f\"   â€¢ Template exists: {llm_query_manager.template_dir.exists()}\")\n",
        "    \n",
        "    # Get model information\n",
        "    model_info = llm_runner.get_model_info()\n",
        "    print(f\"\\\\nğŸ”§ Model Information:\")\n",
        "    print(f\"   â€¢ Model ID: {model_info['model_id']}\")\n",
        "    print(f\"   â€¢ Vocabulary Size: {model_info['vocab_size']:,}\")\n",
        "    print(f\"   â€¢ Device: {model_info['device']}\")\n",
        "    \n",
        "    # Test basic prompt rendering\n",
        "    test_prompt = llm_query_manager.render_prompt(\n",
        "        user_query=\"Test chemical process optimization query\",\n",
        "        system_message=\"You are a chemical engineering expert.\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nğŸ“‹ Prompt System Test:\")\n",
        "    print(f\"   â€¢ Template rendering: âœ… Success\")\n",
        "    print(f\"   â€¢ Prompt length: {len(test_prompt)} characters\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ LLM initialization error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\nâœ… LLM development environment ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Advanced Prompt Engineering and Testing...\n",
            "ğŸ§ª Testing 3 prompt scenarios...\n",
            "\\nğŸ“‹ Scenario 1: Process Optimization\n",
            "   âœ… Prompt rendered successfully (341 chars)\n",
            "   âœ… LLM response generated (153 chars)\n",
            "   ğŸ“ Preview: In the Distillation column, there are two ways to do it:\n",
            "\n",
            "Use a distillation column for the same amo...\n",
            "\\nğŸ“‹ Scenario 2: Safety Analysis\n",
            "   âœ… Prompt rendered successfully (357 chars)\n",
            "   âœ… LLM response generated (194 chars)\n",
            "   ğŸ“ Preview: A: Safety concerns are the most important. The safety of a chemical plant is the most important thin...\n",
            "\\nğŸ“‹ Scenario 3: Economic Assessment\n",
            "   âœ… Prompt rendered successfully (345 chars)\n",
            "   âœ… LLM response generated (222 chars)\n",
            "   ğŸ“ Preview: The modular plant design provides a high-efficiency, high-volume manufacturing capability that enabl...\n",
            "\\nğŸ” Running Prompt System Validation...\n",
            "âš ï¸ Prompt system module not available for validation\n",
            "\\nâœ… Prompt engineering and testing complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 4.2: Advanced Prompt Engineering and Testing\n",
        "# ====================================================\n",
        "\n",
        "print(\"ğŸ¯ Advanced Prompt Engineering and Testing...\")\n",
        "\n",
        "try:\n",
        "    # Test different prompt scenarios\n",
        "    test_scenarios = [\n",
        "        {\n",
        "            \"name\": \"Process Optimization\",\n",
        "            \"query\": \"How can we optimize the distillation column efficiency?\",\n",
        "            \"system\": \"You are a process optimization expert specializing in distillation systems.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Safety Analysis\", \n",
        "            \"query\": \"What safety considerations are important for modular chemical plants?\",\n",
        "            \"system\": \"You are a chemical safety engineer with expertise in process hazard analysis.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Economic Assessment\",\n",
        "            \"query\": \"Analyze the economic benefits of modular plant design.\",\n",
        "            \"system\": \"You are a chemical engineering economist specializing in plant design economics.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(f\"ğŸ§ª Testing {len(test_scenarios)} prompt scenarios...\")\n",
        "    \n",
        "    for i, scenario in enumerate(test_scenarios):\n",
        "        print(f\"\\\\nğŸ“‹ Scenario {i+1}: {scenario['name']}\")\n",
        "        \n",
        "        try:\n",
        "            # Render prompt\n",
        "            prompt = llm_query_manager.render_prompt(\n",
        "                user_query=scenario['query'],\n",
        "                system_message=scenario['system']\n",
        "            )\n",
        "            \n",
        "            print(f\"   âœ… Prompt rendered successfully ({len(prompt)} chars)\")\n",
        "            \n",
        "            # Quick test with LLM (generate a short response)\n",
        "            response = llm_runner.ask(\n",
        "                scenario['query'],\n",
        "                max_length=50,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            \n",
        "            print(f\"   âœ… LLM response generated ({len(response)} chars)\")\n",
        "            print(f\"   ğŸ“ Preview: {response[:100]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Scenario {i+1} failed: {e}\")\n",
        "    \n",
        "    # Test prompt system validation\n",
        "    print(f\"\\\\nğŸ” Running Prompt System Validation...\")\n",
        "    try:\n",
        "        # Import prompt system if available\n",
        "        from src.pynucleus.llm.prompt_system import PyNucleusPromptSystem\n",
        "        \n",
        "        prompt_system = PyNucleusPromptSystem(template_dir=\"prompts\")\n",
        "        validation_result = prompt_system.validate_prompts()\n",
        "        \n",
        "        print(\"âœ… Prompt system validation completed\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ Prompt system module not available for validation\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Prompt validation error: {e}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Prompt engineering error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\\\nâœ… Prompt engineering and testing complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 5: Performance & Debugging** ğŸ”\n",
        "\n",
        "Performance analysis, debugging tools, and system optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ˆ Performance Analysis and System Benchmarking...\n",
            "ğŸ§ª Running Performance Benchmarks...\n",
            "ğŸ”§ Setting up RAG imports...\n",
            "Warning: wikipedia package not available. Wikipedia scraping disabled.\n",
            "âœ… RAG imports ready!\n",
            "ğŸ“‚ Loaded 5 existing DWSIM results from disk\n",
            "ğŸ”§ Setting up DWSIM imports...\n",
            "âœ… DWSIM modules imported successfully\n",
            "ğŸ“ Results directory: data/05_output/results\n",
            "ğŸ”§ Pipeline Utils initialized with results dir: data/05_output/results\n",
            "ğŸ”— DWSIM-RAG integration enabled\n",
            "ğŸ§ª Quick Pipeline Test\n",
            "------------------------------\n",
            "ğŸ“š RAG: 7141 chunks available\n",
            "ğŸ”— Integration: âšª Documents only\n",
            "ğŸ“Š DWSIM Statistics:\n",
            "   â€¢ Total Simulations: 5\n",
            "   â€¢ Success Rate: 100.0%\n",
            "   â€¢ Average Duration: 0.00s\n",
            "ğŸ”¬ DWSIM: 5 simulations\n",
            "ğŸ“ Output: 2 CSV files\n",
            "\n",
            "ğŸ“Š Performance Benchmark Results:\n",
            "------------------------------------------------------------\n",
            "âœ… Pipeline Initialization           0.016s      4.1MB\n",
            "âœ… Configuration Template Creation    0.000s      0.0MB\n",
            "âœ… Quick Status Check                0.001s      0.0MB\n",
            "\n",
            "ğŸ’» Current System Resources:\n",
            "   â€¢ CPU Usage: 34.2%\n",
            "   â€¢ Memory Usage: 83.3%\n",
            "   â€¢ Available Memory: 2.7 GB\n",
            "\n",
            "âœ… Performance analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 5.1: Performance Analysis and Benchmarking\n",
        "# ==================================================\n",
        "\n",
        "print(\"ğŸ“ˆ Performance Analysis and System Benchmarking...\")\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "def measure_performance(func, name, *args, **kwargs):\n",
        "    \"\"\"Measure function performance\"\"\"\n",
        "    gc.collect()  # Clean memory before measurement\n",
        "    \n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    try:\n",
        "        result = func(*args, **kwargs)\n",
        "        success = True\n",
        "        error = None\n",
        "    except Exception as e:\n",
        "        result = None\n",
        "        success = False\n",
        "        error = str(e)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'duration': end_time - start_time,\n",
        "        'memory_used': end_memory - start_memory,\n",
        "        'success': success,\n",
        "        'error': error,\n",
        "        'result': result\n",
        "    }\n",
        "\n",
        "# Performance benchmarks\n",
        "benchmarks = []\n",
        "\n",
        "print(\"ğŸ§ª Running Performance Benchmarks...\")\n",
        "\n",
        "# Benchmark 1: Pipeline initialization\n",
        "bench1 = measure_performance(\n",
        "    lambda: PipelineUtils(results_dir=\"data/05_output/results\"),\n",
        "    \"Pipeline Initialization\"\n",
        ")\n",
        "benchmarks.append(bench1)\n",
        "\n",
        "# Benchmark 2: Configuration template creation\n",
        "bench2 = measure_performance(\n",
        "    lambda: config_manager.create_template_json(\"perf_test.json\"),\n",
        "    \"Configuration Template Creation\"\n",
        ")\n",
        "benchmarks.append(bench2)\n",
        "\n",
        "# Benchmark 3: Quick status check\n",
        "bench3 = measure_performance(\n",
        "    pipeline.quick_test,\n",
        "    \"Quick Status Check\"\n",
        ")\n",
        "benchmarks.append(bench3)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nğŸ“Š Performance Benchmark Results:\")\n",
        "print(\"-\" * 60)\n",
        "for bench in benchmarks:\n",
        "    status = \"âœ…\" if bench['success'] else \"âŒ\"\n",
        "    print(f\"{status} {bench['name']:<30} {bench['duration']:>8.3f}s {bench['memory_used']:>8.1f}MB\")\n",
        "    if not bench['success']:\n",
        "        print(f\"   Error: {bench['error']}\")\n",
        "\n",
        "# System resource usage\n",
        "print(f\"\\nğŸ’» Current System Resources:\")\n",
        "print(f\"   â€¢ CPU Usage: {psutil.cpu_percent():.1f}%\")\n",
        "print(f\"   â€¢ Memory Usage: {psutil.virtual_memory().percent:.1f}%\")\n",
        "print(f\"   â€¢ Available Memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} GB\")\n",
        "\n",
        "print(\"\\nâœ… Performance analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Debug Tools and System Maintenance...\n",
            "ğŸ—‘ï¸ Running System Cleanup...\n",
            "   â€¢ Cleaned 0 temporary files\n",
            "\n",
            "ğŸ“‹ Checking Log Files...\n",
            "   â€¢ Found 14 log files:\n",
            "     - ingestion.log (51304 bytes, 0.0h old)\n",
            "     - system_diagnostic_20250617_220244.log (13404 bytes, 2.1h old)\n",
            "     - system_diagnostic_20250617_232907.log (16884 bytes, 0.7h old)\n",
            "     - system_diagnostic_20250617_220903.log (13408 bytes, 2.0h old)\n",
            "     - system_diagnostic_20250617_215819.log (13404 bytes, 2.2h old)\n",
            "\n",
            "ğŸ’¾ Memory Cleanup...\n",
            "   â€¢ Garbage collection completed\n",
            "\n",
            "ğŸ“Š Development Session Summary:\n",
            "   â€¢ Session Duration: 95.2 seconds\n",
            "   â€¢ Benchmarks Run: 3\n",
            "   â€¢ Components Tested: âœ…\n",
            "\n",
            "âœ… Debug tools and cleanup complete!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 5.2: Debug Tools and System Cleanup\n",
        "# ============================================\n",
        "\n",
        "print(\"ğŸ”§ Debug Tools and System Maintenance...\")\n",
        "\n",
        "# System cleanup functions\n",
        "def cleanup_temp_files():\n",
        "    \"\"\"Remove temporary files\"\"\"\n",
        "    temp_patterns = [\"perf_test.json\", \"dev_simulation_config.*\"]\n",
        "    cleaned = 0\n",
        "    \n",
        "    for pattern in temp_patterns:\n",
        "        if \"*\" in pattern:\n",
        "            import glob\n",
        "            files = glob.glob(pattern)\n",
        "            for file in files:\n",
        "                try:\n",
        "                    Path(file).unlink()\n",
        "                    cleaned += 1\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            try:\n",
        "                if Path(pattern).exists():\n",
        "                    Path(pattern).unlink()\n",
        "                    cleaned += 1\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def check_log_files():\n",
        "    \"\"\"Check system log files\"\"\"\n",
        "    log_dirs = [\"logs\", \"data/05_output/logs\"]\n",
        "    log_files = []\n",
        "    \n",
        "    for log_dir in log_dirs:\n",
        "        if Path(log_dir).exists():\n",
        "            for log_file in Path(log_dir).glob(\"*.log\"):\n",
        "                size = log_file.stat().st_size\n",
        "                log_files.append({\n",
        "                    'file': str(log_file),\n",
        "                    'size': size,\n",
        "                    'age': time.time() - log_file.stat().st_mtime\n",
        "                })\n",
        "    \n",
        "    return log_files\n",
        "\n",
        "# Run debug tools\n",
        "print(\"ğŸ—‘ï¸ Running System Cleanup...\")\n",
        "cleaned_files = cleanup_temp_files()\n",
        "print(f\"   â€¢ Cleaned {cleaned_files} temporary files\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Checking Log Files...\")\n",
        "log_files = check_log_files()\n",
        "if log_files:\n",
        "    print(f\"   â€¢ Found {len(log_files)} log files:\")\n",
        "    for log in log_files[-5:]:  # Show last 5\n",
        "        age_hours = log['age'] / 3600\n",
        "        print(f\"     - {Path(log['file']).name} ({log['size']} bytes, {age_hours:.1f}h old)\")\n",
        "else:\n",
        "    print(\"   â€¢ No log files found\")\n",
        "\n",
        "# Memory cleanup\n",
        "print(\"\\nğŸ’¾ Memory Cleanup...\")\n",
        "gc.collect()\n",
        "print(\"   â€¢ Garbage collection completed\")\n",
        "\n",
        "# Final status\n",
        "print(f\"\\nğŸ“Š Development Session Summary:\")\n",
        "print(f\"   â€¢ Session Duration: {(datetime.now() - start_time).total_seconds():.1f} seconds\")\n",
        "print(f\"   â€¢ Benchmarks Run: {len(benchmarks)}\")\n",
        "print(f\"   â€¢ Components Tested: {'âœ…' if all(b['success'] for b in benchmarks) else 'âš ï¸'}\")\n",
        "\n",
        "print(\"\\nâœ… Debug tools and cleanup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "## **SECTION 6: Version Control & Maintenance** ğŸ“š\n",
        "\n",
        "Optional version control, documentation updates, and system maintenance tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Version control tools ready (uncomment to use)\n",
            "ğŸ’¡ Manual operations available:\n"
          ]
        }
      ],
      "source": [
        "# SECTION 6.1: Version Control and Documentation\n",
        "# ===============================================\n",
        "# Uncomment and run these cells for version control operations\n",
        "\n",
        "# from datetime import datetime\n",
        "# import subprocess\n",
        "\n",
        "# def update_github():\n",
        "#     \"\"\"Update GitHub repository with changes\"\"\"\n",
        "#     print(\"ğŸ“¦ Starting GitHub update...\")\n",
        "#     \n",
        "#     try:\n",
        "#         # Add all changes\n",
        "#         subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
        "#         print(\"   âœ… Files added to staging\")\n",
        "#         \n",
        "#         # Commit with timestamp\n",
        "#         commit_msg = f\"Developer update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "#         subprocess.run([\"git\", \"commit\", \"-m\", commit_msg], check=True)\n",
        "#         print(\"   âœ… Changes committed\")\n",
        "#         \n",
        "#         # Push to origin\n",
        "#         subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "#         print(\"   âœ… Changes pushed to GitHub\")\n",
        "#         \n",
        "#         return True\n",
        "#         \n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         print(f\"   âŒ Git operation failed: {e}\")\n",
        "#         return False\n",
        "\n",
        "# def log_development_session():\n",
        "#     \"\"\"Log this development session\"\"\"\n",
        "#     log_file = \"update_log.txt\"\n",
        "#     \n",
        "#     with open(log_file, \"a\") as f:\n",
        "#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "#         f.write(f\"\\n{timestamp}: Developer session - System testing and validation\\n\")\n",
        "#     \n",
        "#     print(f\"âœ… Development session logged to {log_file}\")\n",
        "\n",
        "# # Uncomment to run version control operations:\n",
        "# # log_development_session()\n",
        "# # update_github()\n",
        "\n",
        "print(\"ğŸ”§ Version control tools ready (uncomment to use)\")\n",
        "print(\"ğŸ’¡ Manual operations available:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
