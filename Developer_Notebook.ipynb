{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# **PyNucleus Developer Notebook** 🛠️\n",
    "\n",
    "## **Advanced Development Environment**\n",
    "\n",
    "This notebook contains advanced features, diagnostics, and development tools for PyNucleus.\n",
    "\n",
    "### **🔧 Developer Features:**\n",
    "- **System Diagnostics**: Comprehensive health checks and validation\n",
    "- **Enhanced Pipeline**: Advanced configuration and integration\n",
    "- **LLM Development**: Prompt engineering and model testing\n",
    "- **Performance Analysis**: Detailed metrics and benchmarking\n",
    "- **Configuration Management**: Custom templates and settings\n",
    "- **Debug Tools**: Logging, tracing, and error analysis\n",
    "\n",
    "### **📋 Notebook Sections:**\n",
    "1. **System Initialization & Diagnostics** (Cells 1-3)\n",
    "2. **Enhanced Pipeline Configuration** (Cells 4-6)\n",
    "3. **Advanced Analysis & Integration** (Cells 7-9)\n",
    "4. **LLM Development & Testing** (Cells 10-12)\n",
    "5. **Performance & Debugging** (Cells 13-15)\n",
    "\n",
    "---\n",
    "**⚠️ Warning**: This notebook is intended for developers and advanced users. For basic usage, use `Capstone Project.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 PyNucleus Developer Environment - Starting Initialization...\n",
      "📅 Session started: 2025-06-18 14:51:38\n",
      "✅ All PyNucleus modules imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mohammadalmusaiteer/PyNucleus-Model/src/pynucleus/rag/vector_store.py:336: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core components initialized\n",
      "🎯 Ready for development and testing!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1: System Initialization & Diagnostics\n",
    "# ===============================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔧 PyNucleus Developer Environment - Starting Initialization...\")\n",
    "print(f\"📅 Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = str(Path().resolve() / 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import all PyNucleus components\n",
    "try:\n",
    "    from pynucleus.pipeline import RAGPipeline, DWSIMPipeline, ResultsExporter, PipelineUtils\n",
    "    from pynucleus.integration.config_manager import ConfigManager\n",
    "    from pynucleus.integration.dwsim_rag_integrator import DWSIMRAGIntegrator\n",
    "    from pynucleus.integration.llm_output_generator import LLMOutputGenerator\n",
    "    from pynucleus.llm import LLMRunner\n",
    "    from pynucleus.llm.query_llm import LLMQueryManager\n",
    "    \n",
    "    print(\"✅ All PyNucleus modules imported successfully\")\n",
    "    \n",
    "    # Initialize components\n",
    "    pipeline = PipelineUtils(results_dir=\"data/05_output/results\")\n",
    "    config_manager = ConfigManager(config_dir=\"configs\")\n",
    "    \n",
    "    # Fixed: DWSIMRAGIntegrator only accepts results_dir parameter\n",
    "    dwsim_rag_integrator = DWSIMRAGIntegrator(results_dir=\"data/05_output/results\")\n",
    "    \n",
    "    llm_generator = LLMOutputGenerator(results_dir=\"data/05_output/llm_reports\")\n",
    "    \n",
    "    print(\"✅ Core components initialized\")\n",
    "    print(\"🎯 Ready for development and testing!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Initialization error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Comprehensive System Diagnostic...\n",
      "⚠️ System diagnostic issues detected:\n",
      "/Users/mohammadalmusaiteer/PyNucleus-Model/scripts/comprehensive_system_diagnostic.py:87: DeprecationWarning: scripts/comprehensive_system_diagnostic.py is deprecated. Use 'python -m pynucleus.diagnostics.runner --full' instead.\n",
      "  main()\n",
      "\n",
      "\n",
      "🔍 Manual System Checks:\n",
      "   ✅ data/01_raw\n",
      "   ✅ data/02_processed\n",
      "   ✅ data/03_intermediate\n",
      "   ✅ data/04_models\n",
      "   ✅ data/05_output\n",
      "   ✅ src/pynucleus/pipeline\n",
      "   ✅ src/pynucleus/rag\n",
      "   ✅ src/pynucleus/integration\n",
      "   ✅ src/pynucleus/llm\n",
      "\n",
      "🎯 System diagnostic complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1.2: Comprehensive System Diagnostic\n",
    "# ============================================\n",
    "\n",
    "print(\"🔍 Running Comprehensive System Diagnostic...\")\n",
    "\n",
    "try:\n",
    "    # Run system diagnostic\n",
    "    import subprocess\n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"scripts/comprehensive_system_diagnostic.py\", \"--quiet\"\n",
    "    ], capture_output=True, text=True, cwd=\".\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ System diagnostic completed successfully\")\n",
    "        # Extract key metrics from output\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        for line in lines[-10:]:  # Show last 10 lines for summary\n",
    "            if any(keyword in line for keyword in ['Health:', 'Status:', 'EXCELLENT', 'GOOD', 'passed']):\n",
    "                print(f\"   {line}\")\n",
    "    else:\n",
    "        print(\"⚠️ System diagnostic issues detected:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not run system diagnostic: {e}\")\n",
    "    print(\"💡 Continuing with manual checks...\")\n",
    "\n",
    "# Manual system checks\n",
    "print(\"\\n🔍 Manual System Checks:\")\n",
    "\n",
    "# Check data directories\n",
    "data_dirs = ['data/01_raw', 'data/02_processed', 'data/03_intermediate', 'data/04_models', 'data/05_output']\n",
    "for dir_path in data_dirs:\n",
    "    exists = Path(dir_path).exists()\n",
    "    print(f\"   {'✅' if exists else '❌'} {dir_path}\")\n",
    "\n",
    "# Check src structure\n",
    "src_dirs = ['src/pynucleus/pipeline', 'src/pynucleus/rag', 'src/pynucleus/integration', 'src/pynucleus/llm']\n",
    "for dir_path in src_dirs:\n",
    "    exists = Path(dir_path).exists()\n",
    "    print(f\"   {'✅' if exists else '❌'} {dir_path}\")\n",
    "\n",
    "print(\"\\n🎯 System diagnostic complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Detailed Pipeline Status Check...\n",
      "\n",
      "==================================================\n",
      "📁 Results Directory: data/05_output/results\n",
      "📄 CSV Files: 2\n",
      "\n",
      "📋 Existing Files:\n",
      "   • dev_simulation_config.csv (171 bytes)\n",
      "   • bulk_modular_plants_template.csv (1372 bytes)\n",
      "\n",
      "🔧 Component Health:\n",
      "   • Pipeline Utils: ✅\n",
      "   • RAG Pipeline: ✅\n",
      "   • DWSIM Pipeline: ✅\n",
      "   • Config Manager: ✅\n",
      "   • DWSIM-RAG Integrator: ✅\n",
      "   • LLM Generator: ✅\n",
      "\n",
      "🔍 Pipeline Component Status:\n",
      "   • RAG Pipeline: ❌\n",
      "   • DWSIM Pipeline: ❌\n",
      "   • Results Exporter: ❌\n",
      "\n",
      "🔗 Integration Status:\n",
      "   • Total RAG Chunks: 0\n",
      "   • Simulation Chunks: 0\n",
      "   • Integration Active: ⚪\n",
      "\n",
      "✅ Status check complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1.3: Pipeline Status and Health Check\n",
    "# =============================================\n",
    "\n",
    "print(\"📊 Detailed Pipeline Status Check...\")\n",
    "\n",
    "try:\n",
    "    # Quick test with validation (this method exists)\n",
    "    test_results = pipeline.quick_test()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Validate test_results\n",
    "    if test_results is None:\n",
    "        print(\"❌ Quick test returned None - pipeline may not be properly initialized\")\n",
    "        test_results = {\n",
    "            'results_dir': 'data/05_output/results',\n",
    "            'csv_files_count': 0,\n",
    "            'csv_files': []\n",
    "        }\n",
    "    \n",
    "    # Safely access results with fallbacks\n",
    "    results_dir = test_results.get('results_dir', 'data/05_output/results')\n",
    "    csv_files_count = test_results.get('csv_files_count', 0)\n",
    "    csv_files = test_results.get('csv_files', [])\n",
    "    \n",
    "    print(f\"📁 Results Directory: {results_dir}\")\n",
    "    print(f\"📄 CSV Files: {csv_files_count}\")\n",
    "    \n",
    "    if csv_files_count > 0:\n",
    "        print(\"\\n📋 Existing Files:\")\n",
    "        for file_info in csv_files:\n",
    "            if isinstance(file_info, dict):\n",
    "                name = file_info.get('name', 'Unknown')\n",
    "                size = file_info.get('size', 0)\n",
    "                print(f\"   • {name} ({size} bytes)\")\n",
    "            else:\n",
    "                print(f\"   • {file_info}\")\n",
    "    \n",
    "    # Component health check\n",
    "    print(f\"\\n🔧 Component Health:\")\n",
    "    print(f\"   • Pipeline Utils: {'✅' if pipeline else '❌'}\")\n",
    "    print(f\"   • RAG Pipeline: {'✅' if hasattr(pipeline, 'rag_pipeline') and pipeline.rag_pipeline else '❌'}\")\n",
    "    print(f\"   • DWSIM Pipeline: {'✅' if hasattr(pipeline, 'dwsim_pipeline') and pipeline.dwsim_pipeline else '❌'}\")\n",
    "    print(f\"   • Config Manager: {'✅' if 'config_manager' in locals() and config_manager else '❌'}\")\n",
    "    print(f\"   • DWSIM-RAG Integrator: {'✅' if 'dwsim_rag_integrator' in locals() and dwsim_rag_integrator else '❌'}\")\n",
    "    print(f\"   • LLM Generator: {'✅' if 'llm_generator' in locals() and llm_generator else '❌'}\")\n",
    "    \n",
    "    # Additional pipeline status info\n",
    "    component_status = test_results.get('component_status', {})\n",
    "    print(f\"\\n🔍 Pipeline Component Status:\")\n",
    "    print(f\"   • RAG Pipeline: {'✅' if component_status.get('rag_pipeline', False) else '❌'}\")\n",
    "    print(f\"   • DWSIM Pipeline: {'✅' if component_status.get('dwsim_pipeline', False) else '❌'}\")\n",
    "    print(f\"   • Results Exporter: {'✅' if component_status.get('exporter', False) else '❌'}\")\n",
    "    \n",
    "    # Integration status\n",
    "    integration_enabled = test_results.get('integration_enabled', False)\n",
    "    rag_chunks = test_results.get('rag_chunks', 0)\n",
    "    simulation_chunks = test_results.get('simulation_chunks', 0)\n",
    "    \n",
    "    print(f\"\\n🔗 Integration Status:\")\n",
    "    print(f\"   • Total RAG Chunks: {rag_chunks:,}\")\n",
    "    print(f\"   • Simulation Chunks: {simulation_chunks:,}\")\n",
    "    print(f\"   • Integration Active: {'✅' if integration_enabled else '⚪'}\")\n",
    "    \n",
    "    print(\"\\n✅ Status check complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Status check error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Provide troubleshooting tips\n",
    "    print(\"\\n🔧 Troubleshooting Tips:\")\n",
    "    print(\"   1. Try restarting the notebook kernel\")\n",
    "    print(\"   2. Re-run Cell 1 to reinitialize components\")\n",
    "    print(\"   3. Check if all required directories exist\")\n",
    "    print(\"   4. Verify PyNucleus installation is complete\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **SECTION 2: Enhanced Pipeline Configuration** 🔧\n",
    "\n",
    "Advanced configuration management and template generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Advanced Configuration Management...\n",
      "📋 Existing Configuration Files: 5\n",
      "   • bulk_modular_plants_template.csv\n",
      "   • bulk_modular_plants_template.json\n",
      "   • dev_simulation_config.csv\n",
      "   • dev_simulation_config.json\n",
      "   • perf_test.json\n",
      "✅ JSON template created: configs/dev_simulation_config.json\n",
      "✅ CSV template created: configs/dev_simulation_config.csv\n",
      "\n",
      "📋 JSON Template Preview:\n",
      "{\n",
      "  \"simulations\": [\n",
      "    {\n",
      "      \"case_name\": \"dev_test_case_1\",\n",
      "      \"temperature\": 350.0,\n",
      "      \"pressure\": 2.5,\n",
      "      \"feed_rate\": 100.0,\n",
      "      \"catalyst_type\": \"Pt/Al2O3\",\n",
      "      \"process_type\": \"distillation\"\n",
      "    },\n",
      "    {\n",
      "      \"case_name\": \"dev_test_case_2\",\n",
      "      \"temperature\": 375.0,\n",
      "      \"...\n",
      "\n",
      "🔄 Template validation: 2 simulations loaded\n",
      "\n",
      "✅ Configuration management ready!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 2.1: Configuration Templates and Management\n",
    "# ===================================================\n",
    "\n",
    "print(\"🔧 Advanced Configuration Management...\")\n",
    "\n",
    "# Create configuration templates\n",
    "try:\n",
    "    # List existing configuration files\n",
    "    existing_configs = config_manager.list_configs()\n",
    "    print(f\"📋 Existing Configuration Files: {len(existing_configs)}\")\n",
    "    for config_file in existing_configs:\n",
    "        print(f\"   • {config_file}\")\n",
    "    \n",
    "    # Create a sample configuration template\n",
    "    sample_config = {\n",
    "        \"simulations\": [\n",
    "            {\n",
    "                \"case_name\": \"dev_test_case_1\",\n",
    "                \"temperature\": 350.0,\n",
    "                \"pressure\": 2.5,\n",
    "                \"feed_rate\": 100.0,\n",
    "                \"catalyst_type\": \"Pt/Al2O3\",\n",
    "                \"process_type\": \"distillation\"\n",
    "            },\n",
    "            {\n",
    "                \"case_name\": \"dev_test_case_2\", \n",
    "                \"temperature\": 375.0,\n",
    "                \"pressure\": 3.0,\n",
    "                \"feed_rate\": 120.0,\n",
    "                \"catalyst_type\": \"Pd/C\",\n",
    "                \"process_type\": \"reaction\"\n",
    "            }\n",
    "        ],\n",
    "        \"metadata\": {\n",
    "            \"created_by\": \"Developer_Notebook\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"description\": \"Development configuration template\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save JSON template\n",
    "    json_template_path = config_manager.save(sample_config, \"dev_simulation_config.json\")\n",
    "    print(f\"✅ JSON template created: {json_template_path}\")\n",
    "    \n",
    "    # Save CSV template (will extract simulations data)\n",
    "    csv_template_path = config_manager.save(sample_config, \"dev_simulation_config.csv\")\n",
    "    print(f\"✅ CSV template created: {csv_template_path}\")\n",
    "    \n",
    "    # Show template contents (first few lines)\n",
    "    if json_template_path.exists():\n",
    "        with open(json_template_path, 'r') as f:\n",
    "            content = f.read()[:300]\n",
    "            print(f\"\\n📋 JSON Template Preview:\")\n",
    "            print(content + \"...\" if len(content) >= 300 else content)\n",
    "    \n",
    "    # Test loading the configuration back\n",
    "    loaded_config = config_manager.load(\"dev_simulation_config.json\")\n",
    "    print(f\"\\n🔄 Template validation: {len(loaded_config['simulations'])} simulations loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Configuration management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Enhanced Pipeline for Development Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export RAG results: Object of type float32 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Pipeline completed in 14.4 seconds!\n",
      "\n",
      "📊 Detailed Results:\n",
      "   • RAG Queries: 3\n",
      "   • DWSIM Simulations: 3\n",
      "   • Export Files: 1\n",
      "✅ Pipeline data ready for enhanced analysis\n",
      "\n",
      "✅ Enhanced pipeline testing complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 2.2: Run Enhanced Pipeline with Full Analysis\n",
    "# =====================================================\n",
    "\n",
    "print(\"🚀 Running Enhanced Pipeline for Development Testing...\")\n",
    "\n",
    "# Run complete pipeline with detailed logging\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Execute pipeline\n",
    "    results = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    if results:\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\n🎉 Pipeline completed in {duration:.1f} seconds!\")\n",
    "        \n",
    "        # Detailed results analysis\n",
    "        print(f\"\\n📊 Detailed Results:\")\n",
    "        print(f\"   • RAG Queries: {len(results.get('rag_data', []))}\")\n",
    "        print(f\"   • DWSIM Simulations: {len(results.get('dwsim_data', []))}\")\n",
    "        print(f\"   • Export Files: {len(results.get('exported_files', []))}\")\n",
    "        \n",
    "        # Set up integrator with pipeline data\n",
    "        if hasattr(pipeline, 'rag_pipeline'):\n",
    "            dwsim_rag_integrator.rag_pipeline = pipeline.rag_pipeline\n",
    "        \n",
    "        print(\"✅ Pipeline data ready for enhanced analysis\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Pipeline execution failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Enhanced pipeline error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Enhanced pipeline testing complete!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **SECTION 3: Advanced Analysis & Integration** 🔬\n",
    "\n",
    "DWSIM-RAG integration, financial analysis, and enhanced reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Advanced DWSIM-RAG Integration Analysis...\n",
      "📊 Processing 3 DWSIM simulations...\n",
      "✅ Enhanced integration complete:\n",
      "   • Integrated simulations: 3\n",
      "   • Export file: data/05_output/results/integrated_results_20250618_145159.json\n",
      "\n",
      "📋 Sample Analysis (First Simulation):\n",
      "   • Case: distillation_ethanol_water\n",
      "   📊 Performance Metrics:\n",
      "      • Conversion: 0.850\n",
      "      • Selectivity: 0.920\n",
      "      • Yield: 0.782\n",
      "      • Overall Performance: Good\n",
      "      • Efficiency Rating: High\n",
      "      • Temperature Rating: Suboptimal\n",
      "      • Pressure Rating: Optimal\n",
      "\n",
      "✅ Advanced integration analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 3.1: DWSIM-RAG Integration and Enhanced Analysis\n",
    "# ========================================================\n",
    "\n",
    "print(\"🔬 Advanced DWSIM-RAG Integration Analysis...\")\n",
    "\n",
    "try:\n",
    "    # Get DWSIM results\n",
    "    dwsim_results = pipeline.dwsim_pipeline.get_results()\n",
    "    \n",
    "    if dwsim_results:\n",
    "        print(f\"📊 Processing {len(dwsim_results)} DWSIM simulations...\")\n",
    "        \n",
    "        # Perform enhanced integration\n",
    "        integrated_results = dwsim_rag_integrator.integrate_simulation_results(\n",
    "            dwsim_results, perform_rag_analysis=True\n",
    "        )\n",
    "        \n",
    "        # Export integrated results\n",
    "        integrated_export_file = dwsim_rag_integrator.export_integrated_results()\n",
    "        \n",
    "        print(f\"✅ Enhanced integration complete:\")\n",
    "        print(f\"   • Integrated simulations: {len(integrated_results)}\")\n",
    "        print(f\"   • Export file: {integrated_export_file}\")\n",
    "        \n",
    "        # Show detailed analysis for first simulation\n",
    "        if integrated_results:\n",
    "            sample = integrated_results[0]\n",
    "            print(f\"\\n📋 Sample Analysis (First Simulation):\")\n",
    "            \n",
    "            # Safely access original simulation data\n",
    "            original_sim = sample.get('original_simulation', {})\n",
    "            print(f\"   • Case: {original_sim.get('case_name', 'Unknown')}\")\n",
    "            \n",
    "            # Performance metrics\n",
    "            perf_metrics = sample.get('performance_metrics', {})\n",
    "            if perf_metrics:\n",
    "                print(f\"   📊 Performance Metrics:\")\n",
    "                for key, value in perf_metrics.items():\n",
    "                    display_key = key.replace('_', ' ').title()\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        if 'rate' in key.lower() or 'percentage' in key.lower():\n",
    "                            print(f\"      • {display_key}: {value:.1f}%\")\n",
    "                        elif isinstance(value, float):\n",
    "                            print(f\"      • {display_key}: {value:.3f}\")\n",
    "                        else:\n",
    "                            print(f\"      • {display_key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"      • {display_key}: {value}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ No performance metrics available\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ No DWSIM results available. Run Section 2.2 first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Integration error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Advanced integration analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Advanced Financial Analysis and LLM Report Generation...\n",
      "📄 Generating LLM reports for 3 simulations...\n",
      "   ✅ Report 1: llm_analysis_distillation_ethanol_water_20250618_145159.md\n",
      "   ✅ Report 2: llm_analysis_methanol_synthesis_20250618_145159.md\n",
      "   ✅ Report 3: llm_analysis_heat_exchanger_optimization_20250618_145159.md\n",
      "\n",
      "💰 Comprehensive Financial Metrics:\n",
      "   • Average Recovery Rate: 85.0%\n",
      "   • Estimated Daily Revenue: $997,050.00\n",
      "   • Estimated Daily Profit: $-2,950.00\n",
      "   • Return on Investment: -43.2%\n",
      "   • Financial Analysis File: data/05_output/llm_reports/financial_analysis_20250618_145159.md\n",
      "\n",
      "📄 Generated Files:\n",
      "   • LLM Reports: 3 files\n",
      "   • Financial Analysis: 1 file\n",
      "\n",
      "✅ Advanced analysis and reporting complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 3.2: LLM Report Generation and Financial Analysis\n",
    "# =========================================================\n",
    "\n",
    "print(\"💰 Advanced Financial Analysis and LLM Report Generation...\")\n",
    "\n",
    "try:\n",
    "    if 'integrated_results' in locals() and integrated_results:\n",
    "        \n",
    "        # Generate LLM reports for all simulations\n",
    "        print(f\"📄 Generating LLM reports for {len(integrated_results)} simulations...\")\n",
    "        \n",
    "        llm_report_files = []\n",
    "        for i, result in enumerate(integrated_results):\n",
    "            try:\n",
    "                report_file = llm_generator.export_llm_ready_text(result)\n",
    "                llm_report_files.append(report_file)\n",
    "                print(f\"   ✅ Report {i+1}: {Path(report_file).name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Report {i+1} failed: {e}\")\n",
    "        \n",
    "        # Generate comprehensive financial analysis\n",
    "        financial_file = llm_generator.export_financial_analysis(integrated_results)\n",
    "        metrics = llm_generator._calculate_key_metrics(integrated_results)\n",
    "        \n",
    "        print(f\"\\n💰 Comprehensive Financial Metrics:\")\n",
    "        print(f\"   • Average Recovery Rate: {metrics['avg_recovery']:.1f}%\")\n",
    "        print(f\"   • Estimated Daily Revenue: ${metrics['estimated_revenue']:,.2f}\")\n",
    "        print(f\"   • Estimated Daily Profit: ${metrics['net_profit']:,.2f}\")\n",
    "        print(f\"   • Return on Investment: {metrics['roi']:.1f}%\")\n",
    "        print(f\"   • Financial Analysis File: {financial_file}\")\n",
    "        \n",
    "        print(f\"\\n📄 Generated Files:\")\n",
    "        print(f\"   • LLM Reports: {len(llm_report_files)} files\")\n",
    "        print(f\"   • Financial Analysis: 1 file\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ No integrated results available. Run Section 3.1 first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ LLM/Financial analysis error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Advanced analysis and reporting complete!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **SECTION 4: LLM Development & Testing** 🤖\n",
    "\n",
    "LLM model testing, prompt engineering, and query development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM Development Environment Initialization...\n",
      "✅ LLM Runner initialized\n",
      "✅ LLM Query Manager initialized\n",
      "\n",
      "🔧 Model Information:\n",
      "   • Model ID: microsoft/DialoGPT-medium\n",
      "   • Parameters: 354,823,168\n",
      "   • Device: cpu\n",
      "   • Data Type: torch.float32\n",
      "   • Memory: 1377.5 MB\n",
      "\n",
      "📋 Prompt System Test:\n",
      "   • Template rendering: ✅ Success\n",
      "   • Prompt length: 108 characters\n",
      "   • Vocabulary Size: 50,257\n",
      "\n",
      "✅ LLM development environment ready!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 4.1: LLM Model Testing and Initialization\n",
    "# =================================================\n",
    "\n",
    "print(\"🤖 LLM Development Environment Initialization...\")\n",
    "\n",
    "try:\n",
    "    # Initialize LLM components\n",
    "    llm_runner = LLMRunner()\n",
    "    llm_query_manager = LLMQueryManager()  # Fixed: removed max_tokens parameter\n",
    "    \n",
    "    # Test LLM functionality\n",
    "    print(f\"✅ LLM Runner initialized\")\n",
    "    print(f\"✅ LLM Query Manager initialized\")\n",
    "    \n",
    "    # Get model information\n",
    "    model_info = llm_runner.get_model_info()\n",
    "    print(f\"\\n🔧 Model Information:\")\n",
    "    print(f\"   • Model ID: {model_info['model_id']}\")\n",
    "    \n",
    "    # Handle the actual keys returned by get_model_info()\n",
    "    if 'num_parameters' in model_info:\n",
    "        print(f\"   • Parameters: {model_info['num_parameters']:,}\")\n",
    "    if 'device' in model_info:\n",
    "        print(f\"   • Device: {model_info['device']}\")\n",
    "    if 'dtype' in model_info:\n",
    "        print(f\"   • Data Type: {model_info['dtype']}\")\n",
    "    if 'memory_footprint' in model_info:\n",
    "        memory = model_info['memory_footprint']\n",
    "        if isinstance(memory, int):\n",
    "            print(f\"   • Memory: {memory / 1024 / 1024:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   • Memory: {memory}\")\n",
    "    \n",
    "    # Check for errors in model info\n",
    "    if 'error' in model_info:\n",
    "        print(f\"   ⚠️ Model Info Error: {model_info['error']}\")\n",
    "    \n",
    "    # Test basic prompt rendering\n",
    "    test_prompt = llm_query_manager._create_general_query_prompt(\n",
    "        question=\"Test chemical process optimization query\",\n",
    "        context=\"You are a chemical engineering expert.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Prompt System Test:\")\n",
    "    print(f\"   • Template rendering: ✅ Success\")\n",
    "    print(f\"   • Prompt length: {len(test_prompt)} characters\")\n",
    "    \n",
    "    # Test tokenizer access if available\n",
    "    if hasattr(llm_runner, 'tokenizer') and llm_runner.tokenizer:\n",
    "        vocab_size = len(llm_runner.tokenizer)\n",
    "        print(f\"   • Vocabulary Size: {vocab_size:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ LLM initialization error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ LLM development environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Advanced Prompt Engineering and Testing...\n",
      "🧪 Testing 3 prompt scenarios...\n",
      "\n",
      "📋 Scenario 1: Process Optimization\n",
      "   ✅ Prompt rendered successfully (160 chars)\n",
      "   ✅ LLM response generated (0 chars)\n",
      "   📝 Preview: ...\n",
      "\n",
      "📋 Scenario 2: Safety Analysis\n",
      "   ✅ Prompt rendered successfully (176 chars)\n",
      "   ✅ LLM response generated (0 chars)\n",
      "   📝 Preview: ...\n",
      "\n",
      "📋 Scenario 3: Economic Assessment\n",
      "   ✅ Prompt rendered successfully (164 chars)\n",
      "   ✅ LLM response generated (0 chars)\n",
      "   📝 Preview: ...\n",
      "\n",
      "✅ Prompt engineering and testing complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 4.2: Advanced Prompt Engineering and Testing\n",
    "# ====================================================\n",
    "\n",
    "print(\"🎯 Advanced Prompt Engineering and Testing...\")\n",
    "\n",
    "try:\n",
    "    # Test different prompt scenarios\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Process Optimization\",\n",
    "            \"query\": \"How can we optimize the distillation column efficiency?\",\n",
    "            \"system\": \"You are a process optimization expert specializing in distillation systems.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Safety Analysis\", \n",
    "            \"query\": \"What safety considerations are important for modular chemical plants?\",\n",
    "            \"system\": \"You are a chemical safety engineer with expertise in process hazard analysis.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Economic Assessment\",\n",
    "            \"query\": \"Analyze the economic benefits of modular plant design.\",\n",
    "            \"system\": \"You are a chemical engineering economist specializing in plant design economics.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"🧪 Testing {len(test_scenarios)} prompt scenarios...\")\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        print(f\"\\n📋 Scenario {i+1}: {scenario['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Render prompt\n",
    "            prompt = llm_query_manager._create_general_query_prompt(\n",
    "                question=scenario['query'],\n",
    "                context=scenario['system']\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Prompt rendered successfully ({len(prompt)} chars)\")\n",
    "            \n",
    "            # Quick test with LLM (generate a short response) - FIXED: use generate_response\n",
    "            response_result = llm_runner.generate_response(\n",
    "                prompt=scenario['query'],\n",
    "                max_length=100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            # Extract the actual response text\n",
    "            response_text = response_result.get('response', 'No response generated')\n",
    "            print(f\"   ✅ LLM response generated ({len(response_text)} chars)\")\n",
    "            print(f\"   📝 Preview: {response_text[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Scenario {i+1} failed: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Prompt engineering error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Prompt engineering and testing complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **SECTION 5: Performance & Debugging** 🔍\n",
    "\n",
    "Performance analysis, debugging tools, and system optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Performance Analysis and System Benchmarking...\n",
      "🧪 Running Performance Benchmarks...\n",
      "\n",
      "📊 Performance Benchmark Results:\n",
      "------------------------------------------------------------\n",
      "✅ Pipeline Initialization           1.949s   -977.9MB\n",
      "✅ Configuration Template Creation    0.000s      0.0MB\n",
      "✅ Quick Status Check                0.001s      0.9MB\n",
      "\n",
      "💻 Current System Resources:\n",
      "   • CPU Usage: 29.3%\n",
      "   • Memory Usage: 84.2%\n",
      "   • Available Memory: 2.5 GB\n",
      "\n",
      "✅ Performance analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 5.1: Performance Analysis and Benchmarking\n",
    "# ==================================================\n",
    "\n",
    "print(\"📈 Performance Analysis and System Benchmarking...\")\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def measure_performance(func, name, *args, **kwargs):\n",
    "    \"\"\"Measure function performance\"\"\"\n",
    "    gc.collect()  # Clean memory before measurement\n",
    "    \n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        success = True\n",
    "        error = None\n",
    "    except Exception as e:\n",
    "        result = None\n",
    "        success = False\n",
    "        error = str(e)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'duration': end_time - start_time,\n",
    "        'memory_used': end_memory - start_memory,\n",
    "        'success': success,\n",
    "        'error': error,\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# Performance benchmarks\n",
    "benchmarks = []\n",
    "\n",
    "print(\"🧪 Running Performance Benchmarks...\")\n",
    "\n",
    "# Benchmark 1: Pipeline initialization\n",
    "bench1 = measure_performance(\n",
    "    lambda: PipelineUtils(results_dir=\"data/05_output/results\"),\n",
    "    \"Pipeline Initialization\"\n",
    ")\n",
    "benchmarks.append(bench1)\n",
    "\n",
    "# Benchmark 2: Configuration template creation\n",
    "bench2 = measure_performance(\n",
    "    lambda: config_manager.create_template_json(\"perf_test.json\"),\n",
    "    \"Configuration Template Creation\"\n",
    ")\n",
    "benchmarks.append(bench2)\n",
    "\n",
    "# Benchmark 3: Quick status check\n",
    "bench3 = measure_performance(\n",
    "    pipeline.quick_test,\n",
    "    \"Quick Status Check\"\n",
    ")\n",
    "benchmarks.append(bench3)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📊 Performance Benchmark Results:\")\n",
    "print(\"-\" * 60)\n",
    "for bench in benchmarks:\n",
    "    status = \"✅\" if bench['success'] else \"❌\"\n",
    "    print(f\"{status} {bench['name']:<30} {bench['duration']:>8.3f}s {bench['memory_used']:>8.1f}MB\")\n",
    "    if not bench['success']:\n",
    "        print(f\"   Error: {bench['error']}\")\n",
    "\n",
    "# System resource usage\n",
    "print(f\"\\n💻 Current System Resources:\")\n",
    "print(f\"   • CPU Usage: {psutil.cpu_percent():.1f}%\")\n",
    "print(f\"   • Memory Usage: {psutil.virtual_memory().percent:.1f}%\")\n",
    "print(f\"   • Available Memory: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} GB\")\n",
    "\n",
    "print(\"\\n✅ Performance analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Debug Tools and System Maintenance...\n",
      "🗑️ Running System Cleanup...\n",
      "   • Cleaned 0 temporary files\n",
      "\n",
      "📋 Checking Log Files...\n",
      "   • Found 61 log files:\n",
      "     - system_diagnostic_20250617_220903.log (13408 bytes, 16.7h old)\n",
      "     - system_diagnostic_20250618_135859.log (16805 bytes, 0.9h old)\n",
      "     - system_diagnostic_20250618_135654.log (17202 bytes, 0.9h old)\n",
      "     - system_validation_20250618_010327.log (10283 bytes, 13.8h old)\n",
      "     - system_diagnostic_20250617_215819.log (13404 bytes, 16.9h old)\n",
      "\n",
      "💾 Memory Cleanup...\n",
      "   • Garbage collection completed\n",
      "\n",
      "📊 Development Session Summary:\n",
      "   • Session Duration: 19.6 seconds\n",
      "   • Benchmarks Run: 3\n",
      "   • Components Tested: ✅\n",
      "\n",
      "✅ Debug tools and cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 5.2: Debug Tools and System Cleanup\n",
    "# ============================================\n",
    "\n",
    "print(\"🔧 Debug Tools and System Maintenance...\")\n",
    "\n",
    "# System cleanup functions\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Remove temporary files\"\"\"\n",
    "    temp_patterns = [\"perf_test.json\", \"dev_simulation_config.*\"]\n",
    "    cleaned = 0\n",
    "    \n",
    "    for pattern in temp_patterns:\n",
    "        if \"*\" in pattern:\n",
    "            import glob\n",
    "            files = glob.glob(pattern)\n",
    "            for file in files:\n",
    "                try:\n",
    "                    Path(file).unlink()\n",
    "                    cleaned += 1\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            try:\n",
    "                if Path(pattern).exists():\n",
    "                    Path(pattern).unlink()\n",
    "                    cleaned += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def check_log_files():\n",
    "    \"\"\"Check system log files\"\"\"\n",
    "    log_dirs = [\"logs\", \"data/05_output/logs\"]\n",
    "    log_files = []\n",
    "    \n",
    "    for log_dir in log_dirs:\n",
    "        if Path(log_dir).exists():\n",
    "            for log_file in Path(log_dir).glob(\"*.log\"):\n",
    "                size = log_file.stat().st_size\n",
    "                log_files.append({\n",
    "                    'file': str(log_file),\n",
    "                    'size': size,\n",
    "                    'age': time.time() - log_file.stat().st_mtime\n",
    "                })\n",
    "    \n",
    "    return log_files\n",
    "\n",
    "# Run debug tools\n",
    "print(\"🗑️ Running System Cleanup...\")\n",
    "cleaned_files = cleanup_temp_files()\n",
    "print(f\"   • Cleaned {cleaned_files} temporary files\")\n",
    "\n",
    "print(\"\\n📋 Checking Log Files...\")\n",
    "log_files = check_log_files()\n",
    "if log_files:\n",
    "    print(f\"   • Found {len(log_files)} log files:\")\n",
    "    for log in log_files[-5:]:  # Show last 5\n",
    "        age_hours = log['age'] / 3600\n",
    "        print(f\"     - {Path(log['file']).name} ({log['size']} bytes, {age_hours:.1f}h old)\")\n",
    "else:\n",
    "    print(\"   • No log files found\")\n",
    "\n",
    "# Memory cleanup\n",
    "print(\"\\n💾 Memory Cleanup...\")\n",
    "gc.collect()\n",
    "print(\"   • Garbage collection completed\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n📊 Development Session Summary:\")\n",
    "if 'start_time' in locals():\n",
    "    print(f\"   • Session Duration: {(datetime.now() - start_time).total_seconds():.1f} seconds\")\n",
    "print(f\"   • Benchmarks Run: {len(benchmarks)}\")\n",
    "print(f\"   • Components Tested: {'✅' if all(b['success'] for b in benchmarks) else '⚠️'}\")\n",
    "\n",
    "print(\"\\n✅ Debug tools and cleanup complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
