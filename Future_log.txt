1) image processing is not available right now but we can add another VLLM model that can process images at later stages 
2)
If you later want RLHF, here’s the roadmap
Collect feedback
Add a thumbs-up / thumbs-down button in the UI.
Store (question, answer, docs_shown, user_rating).
Build a tiny reward dataset
After ~500–1 000 rated pairs, split 80 / 20 train-val.
Train a reward model (single-layer head on top of your 1.3 B LM)
# pseudo-code
import trl
rm = trl.RewardTrainer(base_model="phi-1_5", dataset=feedback_df)
rm.train()
Run DPO or PPO
Use trl.PPOTrainer or trl.DPOTrainer for 1–3 epochs.
Keep KL penalty high so you don’t ruin the factual core.
Plug new weights into your vLLM container
Update docker/Dockerfile.vllm to download phi1_5_rlhf.
DSPy and RAG remain unchanged.


3) lets implement an algorith that ranks the answers and chooses the best one based on probability--- right now top k is being used... is there a better more accurate approach? 
