{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Model - Clean Architecture** üöÄ\n",
        "\n",
        "## **Welcome to PyNucleus Clean!** \n",
        "\n",
        "This notebook provides a streamlined interface to run the PyNucleus pipeline using the new clean architecture with ChromaDB and Qwen models.\n",
        "\n",
        "### **What PyNucleus Clean Does:**\n",
        "- **üìö Document Analysis**: Processes chemical engineering documents using ChromaDB vector store\n",
        "- **ü§ñ AI Generation**: Uses quantized Qwen models for intelligent responses  \n",
        "- **üìä Results Export**: Automatically exports comprehensive results to structured formats\n",
        "- **üí° RAG Integration**: Combines document retrieval with AI generation for enhanced analysis\n",
        "- **üîó Modular Design**: Clean, maintainable architecture with minimal dependencies\n",
        "\n",
        "### **‚ú® Clean Architecture Features:**\n",
        "- ‚úÖ **ChromaDB Integration**: Modern vector database with persistent storage\n",
        "- ‚úÖ **Qwen Model**: Efficient 0.5B parameter model with 4-bit quantization\n",
        "- ‚úÖ **Pydantic Settings**: Type-safe configuration with validation\n",
        "- ‚úÖ **Loguru Logging**: Beautiful structured logging with colors\n",
        "- ‚úÖ **Minimal Dependencies**: Streamlined requirements for better performance\n",
        "- ‚úÖ **Golden Dataset Validation**: Automated accuracy testing and evaluation\n",
        "\n",
        "### **üìã How to Use This Notebook:**\n",
        "0. **üîç Analyze Performance** (Cell 0): Review ChromaDB and chunking performance\n",
        "1. **üîß Initialize System** (Cell 1): Set up PyNucleus with automatic validation\n",
        "2. **üìö Ingest Documents** (Cell 2): Process documents into ChromaDB\n",
        "3. **üöÄ Ask Questions** (Cell 3): Query the system with natural language\n",
        "4. **üìä View Results** (Cell 4): Explore responses and sources\n",
        "5. **üîç Run Evaluation** (Cell 5): Test system accuracy with golden dataset\n",
        "\n",
        "**‚ö° Simple 6-step process for powerful chemical engineering Q&A!**\n",
        "\n",
        "---\n",
        "**üîß For developers**: Advanced features available in `Developer_Notebook_Clean.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: ChromaDB Performance & Chunking Analysis\n",
        "# =================================================\n",
        "# This cell analyzes ChromaDB performance and chunking strategies\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"üîç ChromaDB Performance & Chunking Analysis\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìÖ Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "def analyze_data_structure():\n",
        "    \"\"\"Analyze current data directory structure\"\"\"\n",
        "    print(\"\\nüìÅ Data Structure Analysis:\")\n",
        "    \n",
        "    data_dirs = {\n",
        "        'data/01_raw/source_documents': 'Source documents for processing',\n",
        "        'data/01_raw/web_sources': 'Web-scraped content',\n",
        "        'data/03_intermediate/converted_chunked_data': 'Text chunks from processing',\n",
        "        'data/03_intermediate/vector_db': 'ChromaDB database storage',\n",
        "        'data/04_models/chunk_reports': 'Chunking performance analysis',\n",
        "        'data/04_models/recall_evaluation': 'RAG system recall metrics',\n",
        "        'data/validation': 'Golden dataset and validation results'\n",
        "    }\n",
        "    \n",
        "    for dir_path, description in data_dirs.items():\n",
        "        path = Path(dir_path)\n",
        "        if path.exists():\n",
        "            if path.is_dir():\n",
        "                file_count = len([f for f in path.iterdir() if f.is_file()])\n",
        "                dir_count = len([d for d in path.iterdir() if d.is_dir()])\n",
        "                print(f\"   ‚úÖ {dir_path}: {file_count} files, {dir_count} subdirs - {description}\")\n",
        "            else:\n",
        "                print(f\"   üìÑ {dir_path}: File exists - {description}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {dir_path}: Missing - {description}\")\n",
        "\n",
        "def analyze_chunking_strategy():\n",
        "    \"\"\"Analyze current chunking configuration\"\"\"\n",
        "    print(\"\\nüî§ Chunking Strategy Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        print(f\"   ‚Ä¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "        print(f\"   ‚Ä¢ Retrieval Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "        \n",
        "        # Check if chunk data exists\n",
        "        chunk_dir = Path(\"data/03_intermediate/converted_chunked_data\")\n",
        "        if chunk_dir.exists():\n",
        "            chunk_files = list(chunk_dir.glob(\"*.json\"))\n",
        "            print(f\"   ‚Ä¢ Chunk Files: {len(chunk_files)} files found\")\n",
        "            \n",
        "            if chunk_files:\n",
        "                # Analyze a sample chunk file\n",
        "                with open(chunk_files[0], 'r') as f:\n",
        "                    sample_chunk = json.load(f)\n",
        "                    print(f\"   ‚Ä¢ Sample chunk keys: {list(sample_chunk.keys())}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è No chunk data found in {chunk_dir}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error analyzing chunking: {e}\")\n",
        "\n",
        "def analyze_chromadb_performance():\n",
        "    \"\"\"Analyze ChromaDB setup and performance\"\"\"\n",
        "    print(\"\\nüóÑÔ∏è ChromaDB Performance Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        \n",
        "        chroma_path = Path(settings.CHROMA_PATH)\n",
        "        print(f\"   ‚Ä¢ ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   ‚Ä¢ Database Exists: {'‚úÖ' if chroma_path.exists() else '‚ùå'}\")\n",
        "        \n",
        "        if chroma_path.exists():\n",
        "            # Check database size\n",
        "            db_files = list(chroma_path.rglob(\"*\"))\n",
        "            total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "            print(f\"   ‚Ä¢ Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "            print(f\"   ‚Ä¢ Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "            \n",
        "        # Test retrieval if possible\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            print(f\"   ‚Ä¢ Retrieval Engine: ‚úÖ Available\")\n",
        "            \n",
        "            # Test basic retrieval\n",
        "            test_docs = retrieve(\"chemical engineering\", k=1)\n",
        "            if test_docs and len(test_docs) > 0:\n",
        "                print(f\"   ‚Ä¢ Test Retrieval: ‚úÖ {len(test_docs)} documents found\")\n",
        "                sample_length = len(test_docs[0]) if test_docs[0] else 0\n",
        "                print(f\"   ‚Ä¢ Sample Document Length: {sample_length} characters\")\n",
        "            else:\n",
        "                print(f\"   ‚Ä¢ Test Retrieval: ‚ö†Ô∏è No documents found\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚Ä¢ Retrieval Engine: ‚ùå Error - {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå ChromaDB analysis failed: {e}\")\n",
        "\n",
        "def performance_recommendations():\n",
        "    \"\"\"Provide performance optimization recommendations\"\"\"\n",
        "    print(\"\\nüí° Performance Optimization Recommendations:\")\n",
        "    \n",
        "    recommendations = [\n",
        "        \"üî§ Chunking: Ensure optimal chunk size (512-1024 tokens) for your documents\",\n",
        "        \"üßÆ Embeddings: Use 'all-MiniLM-L6-v2' for faster processing or 'all-mpnet-base-v2' for better quality\",\n",
        "        \"üóÑÔ∏è ChromaDB: Enable persistence and consider indexing parameters for large datasets\",\n",
        "        \"üîç Retrieval: Tune top-k value (4-8) based on your accuracy requirements\",\n",
        "        \"üíæ Storage: Monitor database size and consider compression for large document sets\",\n",
        "        \"‚ö° Performance: First query is slower (model loading), subsequent queries are faster\"\n",
        "    ]\n",
        "    \n",
        "    for rec in recommendations:\n",
        "        print(f\"   {rec}\")\n",
        "\n",
        "# Run all analysis functions\n",
        "try:\n",
        "    analyze_data_structure()\n",
        "    analyze_chunking_strategy()\n",
        "    analyze_chromadb_performance()\n",
        "    performance_recommendations()\n",
        "    \n",
        "    print(f\"\\n‚úÖ ChromaDB Performance Analysis Complete!\")\n",
        "    print(f\"üìù Next: Run Cell 2 to initialize the PyNucleus system\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Analysis Error: {e}\")\n",
        "    print(\"\\nüí° Troubleshooting:\")\n",
        "    print(\"   ‚Ä¢ Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   ‚Ä¢ Check that the data directory structure exists\")\n",
        "    print(\"   ‚Ä¢ Try running this cell again after initializing the system\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: System Initialization & Validation\n",
        "# ==========================================\n",
        "# This cell sets up PyNucleus Clean and validates all components\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üîß Initializing PyNucleus Clean Architecture...\")\n",
        "print(f\"üìÖ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "try:\n",
        "    # Import PyNucleus Clean components\n",
        "    from pynucleus.settings import settings\n",
        "    from pynucleus.utils.logger import logger\n",
        "    from pynucleus.rag.collector import ingest\n",
        "    from pynucleus.rag.engine import ask, retrieve\n",
        "    from pynucleus.llm.qwen_loader import generate\n",
        "    \n",
        "    print(\"‚úÖ PyNucleus Clean modules imported successfully\")\n",
        "    \n",
        "    # Validate configuration\n",
        "    print(f\"üìã Configuration:\")\n",
        "    print(f\"   ‚Ä¢ ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   ‚Ä¢ Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   ‚Ä¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   ‚Ä¢ Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   ‚Ä¢ Retrieve Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "    print(f\"   ‚Ä¢ Log Level: {settings.LOG_LEVEL}\")\n",
        "    print(f\"   ‚Ä¢ Use CUDA: {settings.USE_CUDA}\")\n",
        "    \n",
        "    # Test logging\n",
        "    logger.info(\"PyNucleus Clean initialization successful\")\n",
        "    \n",
        "    print(\"‚úÖ Configuration validated\")\n",
        "    print(\"‚úÖ Logging system active\")\n",
        "    \n",
        "    print(\"\\nüìã System Components Ready:\")\n",
        "    print(\"   ‚Ä¢ üìö ChromaDB Vector Store - Modern document indexing\")\n",
        "    print(\"   ‚Ä¢ ü§ñ Qwen Model - Efficient quantized AI generation\")\n",
        "    print(\"   ‚Ä¢ üìä Document Ingestion - Text processing and embedding\")\n",
        "    print(\"   ‚Ä¢ üí° RAG Engine - Retrieval-augmented generation\")\n",
        "    print(\"   ‚Ä¢ üîç Golden Dataset Evaluation - Validation and testing\")\n",
        "    \n",
        "    # Check for existing vector database\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        print(f\"\\nüìÅ Vector Database: {settings.CHROMA_PATH} (‚úÖ Exists)\")\n",
        "        print(\"üéØ System ready! You can skip to Cell 4 if documents are already ingested.\")\n",
        "    else:\n",
        "        print(f\"\\nüìÅ Vector Database: {settings.CHROMA_PATH} (‚ùå Not Found)\")\n",
        "        print(\"üéØ System ready! Execute Cell 3 to ingest documents.\")\n",
        "    \n",
        "    # Store initialization status for other cells\n",
        "    globals()['system_initialized'] = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import Error: {e}\")\n",
        "    print(\"\\nüí° Troubleshooting:\")\n",
        "    print(\"   ‚Ä¢ Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   ‚Ä¢ Try: pip install -e .\")\n",
        "    print(\"   ‚Ä¢ Check dependencies: pip install tiktoken sentence-transformers chromadb\")\n",
        "    print(\"   ‚Ä¢ Try restarting the kernel\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Initialization Error: {e}\")\n",
        "    print(\"\\nüí° Troubleshooting:\")\n",
        "    print(\"   ‚Ä¢ Check your Python environment setup\")\n",
        "    print(\"   ‚Ä¢ Verify all required directories exist\")\n",
        "    print(\"   ‚Ä¢ For advanced diagnostics, see Developer_Notebook_Clean.ipynb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Document Ingestion\n",
        "# ===========================\n",
        "# This cell processes documents into the ChromaDB vector store\n",
        "\n",
        "print(\"üìö Starting Document Ingestion...\")\n",
        "print(\"\\nüìä Processing Pipeline:\")\n",
        "print(\"   1. üìÅ Scan source directory for text files\")\n",
        "print(\"   2. üî§ Extract and clean text content\")\n",
        "print(\"   3. üßÆ Generate embeddings using SentenceTransformers\")\n",
        "print(\"   4. üíæ Store in ChromaDB with persistent storage\")\n",
        "\n",
        "# Check if system is initialized\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"\\n‚ö†Ô∏è Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    print(\"\\n‚è≥ Please wait... Document processing may take 30-60 seconds.\")\n",
        "    \n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Run document ingestion\n",
        "        logger.info(\"Starting document ingestion process\")\n",
        "        \n",
        "        # Check for documents in source directory\n",
        "        source_dirs = [\"data/01_raw/source_documents\", \"data/01_raw\"]\n",
        "        docs_found = False\n",
        "        \n",
        "        for source_dir in source_dirs:\n",
        "            if Path(source_dir).exists():\n",
        "                files = list(Path(source_dir).glob(\"*.txt\"))\n",
        "                if files:\n",
        "                    print(f\"   üìÑ Found {len(files)} .txt files in {source_dir}\")\n",
        "                    ingest(source_dir=source_dir)\n",
        "                    docs_found = True\n",
        "                    break\n",
        "        \n",
        "        if not docs_found:\n",
        "            print(\"   ‚ö†Ô∏è No .txt files found in source directories\")\n",
        "            print(\"   üí° Please add documents to data/01_raw/source_documents/\")\n",
        "            raise FileNotFoundError(\"No source documents found\")\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"\\nüéâ Document ingestion completed in {duration:.1f} seconds!\")\n",
        "        \n",
        "        # Test the vector store\n",
        "        test_docs = retrieve(\"chemical engineering\")\n",
        "        doc_count = len(test_docs) if test_docs else 0\n",
        "        \n",
        "        print(f\"\\nüìä Ingestion Results:\")\n",
        "        print(f\"   ‚Ä¢ Vector Database: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   ‚Ä¢ Collection: docs\") \n",
        "        print(f\"   ‚Ä¢ Test Query Results: {doc_count} documents retrieved\")\n",
        "        print(f\"   ‚Ä¢ Processing Time: {duration:.1f} seconds\")\n",
        "        \n",
        "        if doc_count > 0:\n",
        "            print(f\"\\nüìã Sample Retrieved Content:\")\n",
        "            sample_doc = test_docs[0][:200] + \"...\" if len(test_docs[0]) > 200 else test_docs[0]\n",
        "            print(f\"   '{sample_doc}'\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Document ingestion complete! Run Cell 4 to start asking questions.\")\n",
        "        \n",
        "        # Store status for next cells\n",
        "        globals()['ingestion_completed'] = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ingestion Error: {e}\")\n",
        "        print(\"\\nüí° Troubleshooting:\")\n",
        "        print(\"   ‚Ä¢ Ensure Cell 2 completed successfully\")\n",
        "        print(\"   ‚Ä¢ Check that data/01_raw/ contains .txt files\")\n",
        "        print(\"   ‚Ä¢ Verify sufficient disk space for vector database\")\n",
        "        print(\"   ‚Ä¢ Try restarting the kernel and re-running Cell 2\")\n",
        "        \n",
        "        import traceback\n",
        "        print(f\"\\nüîß Technical details (for developers):\")\n",
        "        print(f\"   Error type: {type(e).__name__}\")\n",
        "        # Only show first few lines of traceback\n",
        "        tb_lines = traceback.format_exc().split('\\n')[:5]\n",
        "        for line in tb_lines:\n",
        "            if line.strip():\n",
        "                print(f\"   {line}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Interactive Q&A System\n",
        "# ================================\n",
        "# This cell demonstrates the RAG system with sample questions\n",
        "\n",
        "print(\"üöÄ PyNucleus Clean Q&A System\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if ingestion was completed\n",
        "if 'ingestion_completed' not in globals() and 'system_initialized' not in globals():\n",
        "    print(\"‚ö†Ô∏è Please run Cell 2 (System Initialization) and Cell 3 (Document Ingestion) first.\")\n",
        "elif 'ingestion_completed' not in globals():\n",
        "    print(\"‚ö†Ô∏è Please run Cell 3 (Document Ingestion) first, or check if documents are already loaded.\")\n",
        "    # Try to test if retrieval works anyway\n",
        "    try:\n",
        "        test_retrieve = retrieve(\"test\", top_k=1)\n",
        "        if test_retrieve:\n",
        "            print(\"‚úÖ Vector database appears to be loaded. Proceeding with Q&A...\")\n",
        "            globals()['ingestion_completed'] = True\n",
        "        else:\n",
        "            print(\"‚ùå No documents found in vector database.\")\n",
        "    except:\n",
        "        print(\"‚ùå Cannot access vector database.\")\n",
        "\n",
        "if 'ingestion_completed' in globals() or 'system_initialized' in globals():\n",
        "    print(\"üéØ Ask questions about chemical engineering!\")\n",
        "    print(\"\\nüìã Sample Questions to Try:\")\n",
        "    sample_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work in chemical processes?\",\n",
        "        \"What factors affect reactor conversion efficiency?\",\n",
        "        \"Why do modular plants reduce costs?\",\n",
        "        \"What are the key principles of process safety?\",\n",
        "        \"How do heat exchangers improve energy efficiency?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(sample_questions, 1):\n",
        "        print(f\"   {i}. {question}\")\n",
        "    \n",
        "    print(f\"\\nüöÄ Testing with sample questions...\")\n",
        "    \n",
        "    # Test with a few sample questions\n",
        "    test_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work?\",\n",
        "        \"Why do modular plants reduce costs?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nüîç Question {i}: {question}\")\n",
        "        \n",
        "        try:\n",
        "            start_time = datetime.now()\n",
        "            result = ask(question)\n",
        "            duration = (datetime.now() - start_time).total_seconds()\n",
        "            \n",
        "            answer = result.get(\"answer\", \"No answer generated\")\n",
        "            sources = result.get(\"sources\", [])\n",
        "            \n",
        "            print(f\"‚è±Ô∏è Response time: {duration:.2f} seconds\")\n",
        "            print(f\"üìù Answer: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
        "            print(f\"üìö Sources: {len(sources)} documents used\")\n",
        "            \n",
        "            if sources:\n",
        "                print(f\"üîó Source preview: '{sources[0][:100]}...'\" if len(sources[0]) > 100 else f\"üîó Source preview: '{sources[0]}'\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"üí° To ask custom questions, use:\")\n",
        "    print(\"   result = ask('Your question here')\")\n",
        "    print(\"   print(result['answer'])\")\n",
        "    print(\"   print(result['sources'])\")\n",
        "    \n",
        "    print(f\"\\nüéØ System Status:\")\n",
        "    print(f\"   ‚Ä¢ Vector Database: ‚úÖ Ready\")\n",
        "    print(f\"   ‚Ä¢ AI Model: ‚úÖ Loaded\")\n",
        "    print(f\"   ‚Ä¢ Q&A System: ‚úÖ Active\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Interactive Q&A ready! Run Cell 5 to view detailed results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: System Status & Results Dashboard\n",
        "# ==========================================\n",
        "# This cell shows comprehensive system status and usage examples\n",
        "\n",
        "print(\"üìä PyNucleus Clean Results Dashboard\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # System health check\n",
        "    print(\"üîç System Health Check:\")\n",
        "    \n",
        "    # Check vector database\n",
        "    try:\n",
        "        test_retrieval = retrieve(\"test\", k=1)\n",
        "        db_status = \"‚úÖ Ready\" if test_retrieval is not None else \"‚ö†Ô∏è Empty\"\n",
        "        print(f\"   ‚Ä¢ ChromaDB Vector Store: {db_status}\")\n",
        "        \n",
        "        if test_retrieval:\n",
        "            # Get some statistics\n",
        "            sample_retrieval = retrieve(\"chemical engineering\", k=10)\n",
        "            print(f\"   ‚Ä¢ Sample Retrieval: {len(sample_retrieval)} documents found\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚Ä¢ ChromaDB Vector Store: ‚ùå Error - {e}\")\n",
        "    \n",
        "    # Check AI model\n",
        "    try:\n",
        "        test_generation = generate(\"Test prompt\", max_tokens=10)\n",
        "        model_status = \"‚úÖ Ready\" if test_generation else \"‚ùå Error\"\n",
        "        print(f\"   ‚Ä¢ Qwen AI Model: {model_status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚Ä¢ Qwen AI Model: ‚ùå Error - {e}\")\n",
        "    \n",
        "    # Configuration status\n",
        "    print(f\"\\n‚öôÔ∏è Configuration:\")\n",
        "    print(f\"   ‚Ä¢ Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   ‚Ä¢ Vector DB: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   ‚Ä¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   ‚Ä¢ Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   ‚Ä¢ Top-K Retrieval: {settings.RETRIEVE_TOP_K}\")\n",
        "    \n",
        "    # Database statistics\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        db_files = list(chroma_path.rglob(\"*\"))\n",
        "        total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "        print(f\"\\nüìä Database Statistics:\")\n",
        "        print(f\"   ‚Ä¢ Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "        print(f\"   ‚Ä¢ Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "    \n",
        "    # Usage examples\n",
        "    print(f\"\\nüí° Usage Examples:\")\n",
        "    print(f\"\")\n",
        "    print(f\"üìù Basic Question:\")\n",
        "    print(f\"   result = ask('What is distillation?')\")\n",
        "    print(f\"   print(result['answer'])\")\n",
        "    print(f\"\")\n",
        "    print(f\"üîç Document Retrieval:\")\n",
        "    print(f\"   docs = retrieve('modular plants', k=3)\")\n",
        "    print(f\"   for doc in docs:\")\n",
        "    print(f\"       print(doc[:100])\")\n",
        "    print(f\"\")\n",
        "    print(f\"ü§ñ Direct Generation:\")\n",
        "    print(f\"   response = generate('Explain chemical processes', max_tokens=100)\")\n",
        "    print(f\"   print(response)\")\n",
        "    \n",
        "    # CLI usage\n",
        "    print(f\"\\nüîß Command Line Interface:\")\n",
        "    print(f\"   # Ingest documents\")\n",
        "    print(f\"   pynucleus ingest_docs --source-dir data/01_raw\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Ask questions\")\n",
        "    print(f\"   pynucleus ask 'What are the benefits of modular plants?'\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Run evaluation\")\n",
        "    print(f\"   pynucleus eval_golden\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(f\"\\nüìà Performance Tips:\")\n",
        "    print(f\"   ‚Ä¢ First question may be slower (model loading)\")\n",
        "    print(f\"   ‚Ä¢ Subsequent questions are faster (~1-2 seconds)\")\n",
        "    print(f\"   ‚Ä¢ Use specific technical terms for better retrieval\")\n",
        "    print(f\"   ‚Ä¢ ChromaDB persists between sessions\")\n",
        "    print(f\"   ‚Ä¢ Optimal chunk size: 512-1024 tokens\")\n",
        "    \n",
        "    # Next steps\n",
        "    print(f\"\\nüöÄ Next Steps:\")\n",
        "    print(f\"   ‚Ä¢ üìö Add more documents to data/01_raw/source_documents/\")\n",
        "    print(f\"   ‚Ä¢ üîÑ Re-run Cell 3 to update vector database\")\n",
        "    print(f\"   ‚Ä¢ üîß Use Developer_Notebook_Clean.ipynb for advanced features\")\n",
        "    print(f\"   ‚Ä¢ ‚öôÔ∏è Modify settings.py for custom configuration\")\n",
        "    print(f\"   ‚Ä¢ üîç Run Cell 6 for golden dataset evaluation\")\n",
        "    \n",
        "    print(f\"\\nüéØ System Summary:\")\n",
        "    if 'ingestion_completed' in globals():\n",
        "        print(f\"   ‚úÖ Documents processed and indexed\")\n",
        "        print(f\"   ‚úÖ Q&A system ready\")\n",
        "        print(f\"   ‚úÖ All components functional\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Run Cell 3 to complete document ingestion\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Dashboard Error: {e}\")\n",
        "    print(\"\\nüí° Troubleshooting:\")\n",
        "    print(\"   ‚Ä¢ Ensure all previous cells completed successfully\")\n",
        "    print(\"   ‚Ä¢ Check system configuration and dependencies\")\n",
        "    print(\"   ‚Ä¢ Try restarting the kernel and re-running all cells\")\n",
        "\n",
        "print(f\"\\n‚úÖ PyNucleus Clean system analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Golden Dataset Evaluation\n",
        "# ==================================\n",
        "# This cell runs validation tests using the golden dataset\n",
        "\n",
        "print(\"üîç Golden Dataset Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if system is ready\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"‚ö†Ô∏è Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    try:\n",
        "        from pynucleus.eval.golden_eval import run_eval\n",
        "        \n",
        "        print(\"üéØ Running Golden Dataset Evaluation...\")\n",
        "        print(\"\\nüìã Evaluation Process:\")\n",
        "        print(\"   1. üìÑ Load golden dataset from data/validation/golden_dataset.csv\")\n",
        "        print(\"   2. üîç Query each question through the RAG system\")\n",
        "        print(\"   3. üéØ Check if expected keywords appear in responses\")\n",
        "        print(\"   4. üìä Calculate overall accuracy score\")\n",
        "        \n",
        "        # Check if golden dataset exists\n",
        "        golden_path = Path(\"data/validation/golden_dataset.csv\")\n",
        "        if not golden_path.exists():\n",
        "            print(f\"\\n‚ùå Golden dataset not found at {golden_path}\")\n",
        "            print(\"üí° Please ensure the golden dataset file exists\")\n",
        "        else:\n",
        "            # Count questions in dataset\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(golden_path)\n",
        "            question_count = len(df)\n",
        "            \n",
        "            print(f\"\\nüìä Dataset Info:\")\n",
        "            print(f\"   ‚Ä¢ Questions: {question_count}\")\n",
        "            print(f\"   ‚Ä¢ Domains: {len(df['domain'].unique())} unique domains\")\n",
        "            print(f\"   ‚Ä¢ Difficulty levels: {list(df['difficulty'].unique())}\")\n",
        "            \n",
        "            print(f\"\\n‚è≥ Running evaluation... This may take 1-2 minutes.\")\n",
        "            \n",
        "            start_time = datetime.now()\n",
        "            \n",
        "            # Run evaluation with different thresholds\n",
        "            thresholds = [0.6, 0.7, 0.8]\n",
        "            results = {}\n",
        "            \n",
        "            for threshold in thresholds:\n",
        "                success = run_eval(threshold=threshold)\n",
        "                results[threshold] = success\n",
        "                print(f\"   üìä Threshold {threshold:.0%}: {'‚úÖ PASSED' if success else '‚ùå FAILED'}\")\n",
        "            \n",
        "            end_time = datetime.now()\n",
        "            duration = (end_time - start_time).total_seconds()\n",
        "            \n",
        "            print(f\"\\nüéâ Evaluation completed in {duration:.1f} seconds!\")\n",
        "            \n",
        "            # Summary\n",
        "            print(f\"\\nüìä Evaluation Summary:\")\n",
        "            print(f\"   ‚Ä¢ Total Questions: {question_count}\")\n",
        "            print(f\"   ‚Ä¢ Evaluation Time: {duration:.1f} seconds\")\n",
        "            print(f\"   ‚Ä¢ Average Time per Question: {duration/question_count:.1f} seconds\")\n",
        "            \n",
        "            # Recommendations based on results\n",
        "            if results.get(0.8, False):\n",
        "                print(f\"\\nüéØ Performance Assessment: EXCELLENT\")\n",
        "                print(f\"   ‚úÖ System exceeds 80% accuracy threshold\")\n",
        "                print(f\"   ‚úÖ Ready for production use\")\n",
        "            elif results.get(0.7, False):\n",
        "                print(f\"\\nüéØ Performance Assessment: GOOD\")\n",
        "                print(f\"   ‚úÖ System meets 70% accuracy threshold\")\n",
        "                print(f\"   üí° Consider adding more domain-specific documents\")\n",
        "            elif results.get(0.6, False):\n",
        "                print(f\"\\nüéØ Performance Assessment: ACCEPTABLE\")\n",
        "                print(f\"   ‚ö†Ô∏è System meets 60% accuracy threshold\")\n",
        "                print(f\"   üí° Recommendations:\")\n",
        "                print(f\"      ‚Ä¢ Add more comprehensive source documents\")\n",
        "                print(f\"      ‚Ä¢ Review chunking strategy\")\n",
        "                print(f\"      ‚Ä¢ Consider fine-tuning retrieval parameters\")\n",
        "            else:\n",
        "                print(f\"\\nüéØ Performance Assessment: NEEDS IMPROVEMENT\")\n",
        "                print(f\"   ‚ùå System below 60% accuracy threshold\")\n",
        "                print(f\"   üí° Action Items:\")\n",
        "                print(f\"      ‚Ä¢ Review and expand document collection\")\n",
        "                print(f\"      ‚Ä¢ Verify document quality and relevance\")\n",
        "                print(f\"      ‚Ä¢ Check embedding model configuration\")\n",
        "                print(f\"      ‚Ä¢ Consider adjusting chunk size and overlap\")\n",
        "            \n",
        "            print(f\"\\nüîß Advanced Analysis:\")\n",
        "            print(f\"   ‚Ä¢ For detailed per-question analysis, see Developer_Notebook_Clean.ipynb\")\n",
        "            print(f\"   ‚Ä¢ For system diagnostics, run: python scripts/system_validator.py\")\n",
        "            print(f\"   ‚Ä¢ For comprehensive testing, run: python scripts/comprehensive_system_diagnostic.py\")\n",
        "            \n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Import Error: {e}\")\n",
        "        print(\"üí° Ensure golden evaluation module is available\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation Error: {e}\")\n",
        "        print(\"\\nüí° Troubleshooting:\")\n",
        "        print(\"   ‚Ä¢ Ensure documents are ingested (run Cell 3)\")\n",
        "        print(\"   ‚Ä¢ Check that golden dataset exists\")\n",
        "        print(\"   ‚Ä¢ Verify system is properly initialized\")\n",
        "\n",
        "print(f\"\\n‚úÖ Golden dataset evaluation complete!\")\n",
        "print(f\"üéØ PyNucleus Clean system fully validated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ========================================\n",
        "# # VERSION CONTROL (Optional - For Maintainers Only)\n",
        "# # ========================================\n",
        "# # Uncomment the lines below if you need to update the repository:\n",
        "\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Log end time\n",
        "# with open(\"update_log.txt\", \"a\") as f:\n",
        "#     f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
        "\n",
        "# # Simple GitHub update function\n",
        "# def update_github():\n",
        "#     print(\" Starting GitHub update...\")\n",
        "    \n",
        "#     # Check for large files first\n",
        "#     import subprocess\n",
        "#     result = subprocess.run(['git', 'add', '.'], capture_output=True, text=True)\n",
        "#     if result.returncode != 0:\n",
        "#         print(f\" ‚ùå Error adding files: {result.stderr}\")\n",
        "#         return False\n",
        "#     print(\" Files added to staging\")\n",
        "    \n",
        "#     # Commit changes\n",
        "#     result = subprocess.run(['git', 'commit', '-m', f\"Update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"], \n",
        "#                           capture_output=True, text=True)\n",
        "#     if result.returncode != 0:\n",
        "#         print(f\" ‚ùå Error committing: {result.stderr}\")\n",
        "#         return False\n",
        "#     print(\" Changes committed\")\n",
        "    \n",
        "#     # Push to GitHub\n",
        "#     result = subprocess.run(['git', 'push', 'origin', 'main'], capture_output=True, text=True)\n",
        "#     if result.returncode == 0:\n",
        "#         print(\" ‚úÖ Changes pushed to GitHub successfully!\")\n",
        "#         return True\n",
        "#     else:\n",
        "#         print(f\" ‚ùå Push failed: {result.stderr}\")\n",
        "#         print(\"\\nüí° Common solutions:\")\n",
        "#         print(\"   ‚Ä¢ Large files: Add to .gitignore and remove from tracking\")\n",
        "#         print(\"   ‚Ä¢ Authentication: Check GitHub credentials\")\n",
        "#         print(\"   ‚Ä¢ Network: Verify internet connection\")\n",
        "#         return False\n",
        "\n",
        "# # Note: This function now properly checks for push success/failure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Starting GitHub update...\n",
            " Files added to staging\n",
            "[main e786cfb] Update: 2025-06-24 21:26:33\n",
            " 156 files changed, 9071 insertions(+), 5036 deletions(-)\n",
            " delete mode 100644 FIXES_IMPLEMENTED.md\n",
            " delete mode 100644 capture_flask_issue.py\n",
            " create mode 100644 data/validation/results/enhanced_evaluation_20250622_183346.json\n",
            " create mode 100644 data/validation/results/enhanced_evaluation_20250622_183805.json\n",
            " create mode 100644 data/validation/results/enhanced_evaluation_20250622_185607.json\n",
            " create mode 100644 data/validation/results/system_validation_20250622_221149.json\n",
            " create mode 100644 data/validation/results/system_validation_20250622_221518.json\n",
            " create mode 100644 data/validation/results/system_validation_20250624_164356.json\n",
            " create mode 100644 data/validation/results/system_validation_20250624_165233.json\n",
            " create mode 100644 data/validation/results/system_validation_20250624_171450.json\n",
            " create mode 100644 data/validation/results/system_validation_20250624_211728.json\n",
            " create mode 100644 logs/diagnostics_output_20250624_160824.log\n",
            " create mode 100644 logs/failure_reproduction_20250624_160824.log\n",
            " create mode 100644 logs/pynucleus_20250622_090036.log\n",
            " create mode 100644 logs/pynucleus_20250622_182957.log\n",
            " create mode 100644 logs/pynucleus_20250622_183215.log\n",
            " create mode 100644 logs/pynucleus_20250622_183414.log\n",
            " create mode 100644 logs/pynucleus_20250622_183727.log\n",
            " create mode 100644 logs/pynucleus_20250622_184004.log\n",
            " create mode 100644 logs/pynucleus_20250622_184154.log\n",
            " create mode 100644 logs/pynucleus_20250622_184205.log\n",
            " create mode 100644 logs/pynucleus_20250622_185510.log\n",
            " create mode 100644 logs/pynucleus_20250622_185922.log\n",
            " create mode 100644 logs/pynucleus_20250622_201119.log\n",
            " create mode 100644 logs/pynucleus_20250622_212850.log\n",
            " create mode 100644 logs/pynucleus_20250622_212929.log\n",
            " create mode 100644 logs/pynucleus_20250622_213021.log\n",
            " create mode 100644 logs/pynucleus_20250622_214022.log\n",
            " create mode 100644 logs/pynucleus_20250622_214325.log\n",
            " create mode 100644 logs/pynucleus_20250622_214511.log\n",
            " create mode 100644 logs/pynucleus_20250622_215041.log\n",
            " create mode 100644 logs/pynucleus_20250622_220337.log\n",
            " create mode 100644 logs/pynucleus_20250622_220727.log\n",
            " create mode 100644 logs/pynucleus_20250622_220920.log\n",
            " create mode 100644 logs/pynucleus_20250622_221139.log\n",
            " create mode 100644 logs/pynucleus_20250622_221416.log\n",
            " create mode 100644 logs/pynucleus_20250622_221509.log\n",
            " create mode 100644 logs/pynucleus_20250622_222103.log\n",
            " create mode 100644 logs/pynucleus_20250622_222321.log\n",
            " create mode 100644 logs/pynucleus_20250622_222322.log\n",
            " create mode 100644 logs/pynucleus_20250622_222405.log\n",
            " create mode 100644 logs/pynucleus_20250622_222406.log\n",
            " create mode 100644 logs/pynucleus_20250622_223008.log\n",
            " create mode 100644 logs/pynucleus_20250622_223048.log\n",
            " create mode 100644 logs/pynucleus_20250622_223633.log\n",
            " create mode 100644 logs/pynucleus_20250622_223702.log\n",
            " create mode 100644 logs/pynucleus_20250623_105603.log\n",
            " create mode 100644 logs/pynucleus_20250623_110151.log\n",
            " create mode 100644 logs/pynucleus_20250623_110359.log\n",
            " create mode 100644 logs/pynucleus_20250623_112904.log\n",
            " create mode 100644 logs/pynucleus_20250623_114905.log\n",
            " create mode 100644 logs/pynucleus_20250623_115203.log\n",
            " create mode 100644 logs/pynucleus_20250623_115531.log\n",
            " create mode 100644 logs/pynucleus_20250623_115640.log\n",
            " create mode 100644 logs/pynucleus_20250623_120037.log\n",
            " create mode 100644 logs/pynucleus_20250623_121314.log\n",
            " create mode 100644 logs/pynucleus_20250623_121919.log\n",
            " create mode 100644 logs/pynucleus_20250623_121943.log\n",
            " create mode 100644 logs/pynucleus_20250623_122031.log\n",
            " create mode 100644 logs/pynucleus_20250623_122521.log\n",
            " create mode 100644 logs/pynucleus_20250623_122728.log\n",
            " create mode 100644 logs/pynucleus_20250623_122755.log\n",
            " create mode 100644 logs/pynucleus_20250623_123201.log\n",
            " create mode 100644 logs/pynucleus_20250623_123304.log\n",
            " create mode 100644 logs/pynucleus_20250623_124042.log\n",
            " create mode 100644 logs/pynucleus_20250623_124152.log\n",
            " create mode 100644 logs/pynucleus_20250623_124858.log\n",
            " create mode 100644 logs/pynucleus_20250623_125103.log\n",
            " create mode 100644 logs/pynucleus_20250623_130514.log\n",
            " create mode 100644 logs/pynucleus_20250623_130704.log\n",
            " create mode 100644 logs/pynucleus_20250623_131100.log\n",
            " create mode 100644 logs/pynucleus_20250623_131129.log\n",
            " create mode 100644 logs/pynucleus_20250623_131557.log\n",
            " create mode 100644 logs/pynucleus_20250623_132328.log\n",
            " create mode 100644 logs/pynucleus_20250623_132358.log\n",
            " create mode 100644 logs/pynucleus_20250623_132359.log\n",
            " create mode 100644 logs/pynucleus_20250623_132812.log\n",
            " create mode 100644 logs/pynucleus_20250623_132841.log\n",
            " create mode 100644 logs/pynucleus_20250623_133414.log\n",
            " create mode 100644 logs/pynucleus_20250623_133447.log\n",
            " create mode 100644 logs/pynucleus_20250623_133449.log\n",
            " create mode 100644 logs/pynucleus_20250623_133545.log\n",
            " create mode 100644 logs/pynucleus_20250623_133547.log\n",
            " create mode 100644 logs/pynucleus_20250623_134736.log\n",
            " create mode 100644 logs/pynucleus_20250623_134817.log\n",
            " create mode 100644 logs/pynucleus_20250623_135053.log\n",
            " create mode 100644 logs/pynucleus_20250623_135054.log\n",
            " create mode 100644 logs/pynucleus_20250624_143951.log\n",
            " create mode 100644 logs/pynucleus_20250624_143952.log\n",
            " create mode 100644 logs/pynucleus_20250624_150606.log\n",
            " create mode 100644 logs/pynucleus_20250624_151133.log\n",
            " create mode 100644 logs/pynucleus_20250624_151136.log\n",
            " create mode 100644 logs/pynucleus_20250624_151815.log\n",
            " create mode 100644 logs/pynucleus_20250624_152138.log\n",
            " create mode 100644 logs/pynucleus_20250624_152913.log\n",
            " create mode 100644 logs/pynucleus_20250624_153042.log\n",
            " create mode 100644 logs/pynucleus_20250624_153705.log\n",
            " create mode 100644 logs/pynucleus_20250624_154215.log\n",
            " create mode 100644 logs/pynucleus_20250624_154918.log\n",
            " create mode 100644 logs/pynucleus_20250624_160840.log\n",
            " create mode 100644 logs/pynucleus_20250624_160841.log\n",
            " create mode 100644 logs/pynucleus_20250624_161805.log\n",
            " create mode 100644 logs/pynucleus_20250624_161806.log\n",
            " create mode 100644 logs/pynucleus_20250624_164120.log\n",
            " create mode 100644 logs/pynucleus_20250624_164355.log\n",
            " create mode 100644 logs/pynucleus_20250624_164401.log\n",
            " create mode 100644 logs/pynucleus_20250624_165232.log\n",
            " create mode 100644 logs/pynucleus_20250624_165240.log\n",
            " create mode 100644 logs/pynucleus_20250624_171449.log\n",
            " create mode 100644 logs/pynucleus_20250624_171455.log\n",
            " create mode 100644 logs/pynucleus_20250624_173306.log\n",
            " create mode 100644 logs/pynucleus_20250624_173357.log\n",
            " create mode 100644 logs/pynucleus_20250624_173536.log\n",
            " create mode 100644 logs/pynucleus_20250624_173949.log\n",
            " create mode 100644 logs/pynucleus_20250624_174206.log\n",
            " create mode 100644 logs/pynucleus_20250624_175451.log\n",
            " create mode 100644 logs/pynucleus_20250624_191055.log\n",
            " create mode 100644 logs/pynucleus_20250624_192513.log\n",
            " create mode 100644 logs/pynucleus_20250624_192616.log\n",
            " create mode 100644 logs/pynucleus_20250624_194323.log\n",
            " create mode 100644 logs/pynucleus_20250624_195108.log\n",
            " create mode 100644 logs/pynucleus_20250624_200229.log\n",
            " create mode 100644 logs/pynucleus_20250624_202109.log\n",
            " create mode 100644 logs/pynucleus_20250624_202617.log\n",
            " create mode 100644 logs/pynucleus_20250624_203044.log\n",
            " create mode 100644 logs/pynucleus_20250624_204908.log\n",
            " create mode 100644 logs/pynucleus_20250624_205931.log\n",
            " create mode 100644 logs/pynucleus_20250624_210225.log\n",
            " create mode 100644 logs/pynucleus_20250624_211329.log\n",
            " delete mode 100644 restart_web_server.py\n",
            " delete mode 100755 run_web_app.py\n",
            " create mode 100644 scripts/demo_enhanced_statistics.py\n",
            " delete mode 100644 src/pynucleus/api/__init__.py\n",
            " delete mode 100644 src/pynucleus/api/app.py\n",
            " delete mode 100644 src/pynucleus/api/static/developer_dashboard.html\n",
            " delete mode 100644 src/pynucleus/api/static/index.html\n",
            " delete mode 100644 src/pynucleus/api/wsgi.py\n",
            " create mode 100644 src/pynucleus/metrics/__init__.py\n",
            " create mode 100644 src/pynucleus/rag/answer_processing.py\n",
            " delete mode 100644 tests/test_dev_console.py\n",
            " Changes committed\n",
            "Enumerating objects: 215, done.\n",
            "Counting objects: 100% (215/215), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (181/181), done.\n",
            "Writing objects: 100% (184/184), 145.14 KiB | 11.16 MiB/s, done.\n",
            "Total 184 (delta 114), reused 0 (delta 0), pack-reused 0 (from 0)\n",
            "remote: Resolving deltas: 100% (114/114), completed with 17 local objects.\u001b[K\n",
            "To https://github.com/Saytor20/PyNucleus-Model.git\n",
            "   3bcfd11..e786cfb  main -> main\n",
            " Changes pushed to GitHub successfully!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# VERSION CONTROL (Optional - For Maintainers Only)\n",
        "# ========================================\n",
        "# Uncomment the lines below if you need to update the repository:\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Log end time\n",
        "with open(\"update_log.txt\", \"a\") as f:\n",
        "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
        "\n",
        "# Simple GitHub update function\n",
        "def update_github():\n",
        "    print(\" Starting GitHub update...\")\n",
        "    !git add .\n",
        "    print(\" Files added to staging\")\n",
        "    !git commit -m \"Update: $(date +'%Y-%m-%d %H:%M:%S')\"\n",
        "    print(\" Changes committed\")\n",
        "    !git push origin main\n",
        "    print(\" Changes pushed to GitHub successfully!\")\n",
        "\n",
        "# To use it, just run:\n",
        "update_github()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python run_pipeline.py chat\n",
        "\n",
        "python run_pipeline.py build"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
