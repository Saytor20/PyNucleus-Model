{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Model - Clean Architecture** 🚀\n",
        "\n",
        "## **Welcome to PyNucleus Clean!** \n",
        "\n",
        "This notebook provides a streamlined interface to run the PyNucleus pipeline using the new clean architecture with ChromaDB and Qwen models.\n",
        "\n",
        "### **What PyNucleus Clean Does:**\n",
        "- **📚 Document Analysis**: Processes chemical engineering documents using ChromaDB vector store\n",
        "- **🤖 AI Generation**: Uses quantized Qwen models for intelligent responses  \n",
        "- **📊 Results Export**: Automatically exports comprehensive results to structured formats\n",
        "- **💡 RAG Integration**: Combines document retrieval with AI generation for enhanced analysis\n",
        "- **🔗 Modular Design**: Clean, maintainable architecture with minimal dependencies\n",
        "\n",
        "### **✨ Clean Architecture Features:**\n",
        "- ✅ **ChromaDB Integration**: Modern vector database with persistent storage\n",
        "- ✅ **Qwen Model**: Efficient 0.5B parameter model with 4-bit quantization\n",
        "- ✅ **Pydantic Settings**: Type-safe configuration with validation\n",
        "- ✅ **Loguru Logging**: Beautiful structured logging with colors\n",
        "- ✅ **Minimal Dependencies**: Streamlined requirements for better performance\n",
        "- ✅ **Golden Dataset Validation**: Automated accuracy testing and evaluation\n",
        "\n",
        "### **📋 How to Use This Notebook:**\n",
        "0. **🔍 Analyze Performance** (Cell 0): Review ChromaDB and chunking performance\n",
        "1. **🔧 Initialize System** (Cell 1): Set up PyNucleus with automatic validation\n",
        "2. **📚 Ingest Documents** (Cell 2): Process documents into ChromaDB\n",
        "3. **🚀 Ask Questions** (Cell 3): Query the system with natural language\n",
        "4. **📊 View Results** (Cell 4): Explore responses and sources\n",
        "5. **🔍 Run Evaluation** (Cell 5): Test system accuracy with golden dataset\n",
        "\n",
        "**⚡ Simple 6-step process for powerful chemical engineering Q&A!**\n",
        "\n",
        "---\n",
        "**🔧 For developers**: Advanced features available in `Developer_Notebook_Clean.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: ChromaDB Performance & Chunking Analysis\n",
        "# =================================================\n",
        "# This cell analyzes ChromaDB performance and chunking strategies\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"🔍 ChromaDB Performance & Chunking Analysis\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"📅 Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "def analyze_data_structure():\n",
        "    \"\"\"Analyze current data directory structure\"\"\"\n",
        "    print(\"\\n📁 Data Structure Analysis:\")\n",
        "    \n",
        "    data_dirs = {\n",
        "        'data/01_raw/source_documents': 'Source documents for processing',\n",
        "        'data/01_raw/web_sources': 'Web-scraped content',\n",
        "        'data/03_intermediate/converted_chunked_data': 'Text chunks from processing',\n",
        "        'data/03_intermediate/vector_db': 'ChromaDB database storage',\n",
        "        'data/04_models/chunk_reports': 'Chunking performance analysis',\n",
        "        'data/04_models/recall_evaluation': 'RAG system recall metrics',\n",
        "        'data/validation': 'Golden dataset and validation results'\n",
        "    }\n",
        "    \n",
        "    for dir_path, description in data_dirs.items():\n",
        "        path = Path(dir_path)\n",
        "        if path.exists():\n",
        "            if path.is_dir():\n",
        "                file_count = len([f for f in path.iterdir() if f.is_file()])\n",
        "                dir_count = len([d for d in path.iterdir() if d.is_dir()])\n",
        "                print(f\"   ✅ {dir_path}: {file_count} files, {dir_count} subdirs - {description}\")\n",
        "            else:\n",
        "                print(f\"   📄 {dir_path}: File exists - {description}\")\n",
        "        else:\n",
        "            print(f\"   ❌ {dir_path}: Missing - {description}\")\n",
        "\n",
        "def analyze_chunking_strategy():\n",
        "    \"\"\"Analyze current chunking configuration\"\"\"\n",
        "    print(\"\\n🔤 Chunking Strategy Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        print(f\"   • Embedding Model: {settings.EMB_MODEL}\")\n",
        "        print(f\"   • Retrieval Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "        \n",
        "        # Check if chunk data exists\n",
        "        chunk_dir = Path(\"data/03_intermediate/converted_chunked_data\")\n",
        "        if chunk_dir.exists():\n",
        "            chunk_files = list(chunk_dir.glob(\"*.json\"))\n",
        "            print(f\"   • Chunk Files: {len(chunk_files)} files found\")\n",
        "            \n",
        "            if chunk_files:\n",
        "                # Analyze a sample chunk file\n",
        "                with open(chunk_files[0], 'r') as f:\n",
        "                    sample_chunk = json.load(f)\n",
        "                    print(f\"   • Sample chunk keys: {list(sample_chunk.keys())}\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ No chunk data found in {chunk_dir}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error analyzing chunking: {e}\")\n",
        "\n",
        "def analyze_chromadb_performance():\n",
        "    \"\"\"Analyze ChromaDB setup and performance\"\"\"\n",
        "    print(\"\\n🗄️ ChromaDB Performance Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        \n",
        "        chroma_path = Path(settings.CHROMA_PATH)\n",
        "        print(f\"   • ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   • Database Exists: {'✅' if chroma_path.exists() else '❌'}\")\n",
        "        \n",
        "        if chroma_path.exists():\n",
        "            # Check database size\n",
        "            db_files = list(chroma_path.rglob(\"*\"))\n",
        "            total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "            print(f\"   • Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "            print(f\"   • Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "            \n",
        "        # Test retrieval if possible\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            print(f\"   • Retrieval Engine: ✅ Available\")\n",
        "            \n",
        "            # Test basic retrieval\n",
        "            test_docs = retrieve(\"chemical engineering\", top_k=1)\n",
        "            if test_docs and len(test_docs) > 0:\n",
        "                print(f\"   • Test Retrieval: ✅ {len(test_docs)} documents found\")\n",
        "                sample_length = len(test_docs[0]) if test_docs[0] else 0\n",
        "                print(f\"   • Sample Document Length: {sample_length} characters\")\n",
        "            else:\n",
        "                print(f\"   • Test Retrieval: ⚠️ No documents found\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   • Retrieval Engine: ❌ Error - {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ ChromaDB analysis failed: {e}\")\n",
        "\n",
        "def performance_recommendations():\n",
        "    \"\"\"Provide performance optimization recommendations\"\"\"\n",
        "    print(\"\\n💡 Performance Optimization Recommendations:\")\n",
        "    \n",
        "    recommendations = [\n",
        "        \"🔤 Chunking: Ensure optimal chunk size (512-1024 tokens) for your documents\",\n",
        "        \"🧮 Embeddings: Use 'all-MiniLM-L6-v2' for faster processing or 'all-mpnet-base-v2' for better quality\",\n",
        "        \"🗄️ ChromaDB: Enable persistence and consider indexing parameters for large datasets\",\n",
        "        \"🔍 Retrieval: Tune top-k value (4-8) based on your accuracy requirements\",\n",
        "        \"💾 Storage: Monitor database size and consider compression for large document sets\",\n",
        "        \"⚡ Performance: First query is slower (model loading), subsequent queries are faster\"\n",
        "    ]\n",
        "    \n",
        "    for rec in recommendations:\n",
        "        print(f\"   {rec}\")\n",
        "\n",
        "# Run all analysis functions\n",
        "try:\n",
        "    analyze_data_structure()\n",
        "    analyze_chunking_strategy()\n",
        "    analyze_chromadb_performance()\n",
        "    performance_recommendations()\n",
        "    \n",
        "    print(f\"\\n✅ ChromaDB Performance Analysis Complete!\")\n",
        "    print(f\"📝 Next: Run Cell 2 to initialize the PyNucleus system\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Analysis Error: {e}\")\n",
        "    print(\"\\n💡 Troubleshooting:\")\n",
        "    print(\"   • Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   • Check that the data directory structure exists\")\n",
        "    print(\"   • Try running this cell again after initializing the system\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: System Initialization & Validation\n",
        "# ==========================================\n",
        "# This cell sets up PyNucleus Clean and validates all components\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"🔧 Initializing PyNucleus Clean Architecture...\")\n",
        "print(f\"📅 Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "try:\n",
        "    # Import PyNucleus Clean components\n",
        "    from pynucleus.settings import settings\n",
        "    from pynucleus.utils.logger import logger\n",
        "    from pynucleus.rag.collector import ingest\n",
        "    from pynucleus.rag.engine import ask, retrieve\n",
        "    from pynucleus.llm.qwen_loader import generate\n",
        "    \n",
        "    print(\"✅ PyNucleus Clean modules imported successfully\")\n",
        "    \n",
        "    # Validate configuration\n",
        "    print(f\"📋 Configuration:\")\n",
        "    print(f\"   • ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   • Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   • Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   • Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   • Retrieve Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "    print(f\"   • Log Level: {settings.LOG_LEVEL}\")\n",
        "    print(f\"   • Use CUDA: {settings.USE_CUDA}\")\n",
        "    \n",
        "    # Test logging\n",
        "    logger.info(\"PyNucleus Clean initialization successful\")\n",
        "    \n",
        "    print(\"✅ Configuration validated\")\n",
        "    print(\"✅ Logging system active\")\n",
        "    \n",
        "    print(\"\\n📋 System Components Ready:\")\n",
        "    print(\"   • 📚 ChromaDB Vector Store - Modern document indexing\")\n",
        "    print(\"   • 🤖 Qwen Model - Efficient quantized AI generation\")\n",
        "    print(\"   • 📊 Document Ingestion - Text processing and embedding\")\n",
        "    print(\"   • 💡 RAG Engine - Retrieval-augmented generation\")\n",
        "    print(\"   • 🔍 Golden Dataset Evaluation - Validation and testing\")\n",
        "    \n",
        "    # Check for existing vector database\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        print(f\"\\n📁 Vector Database: {settings.CHROMA_PATH} (✅ Exists)\")\n",
        "        print(\"🎯 System ready! You can skip to Cell 4 if documents are already ingested.\")\n",
        "    else:\n",
        "        print(f\"\\n📁 Vector Database: {settings.CHROMA_PATH} (❌ Not Found)\")\n",
        "        print(\"🎯 System ready! Execute Cell 3 to ingest documents.\")\n",
        "    \n",
        "    # Store initialization status for other cells\n",
        "    globals()['system_initialized'] = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"❌ Import Error: {e}\")\n",
        "    print(\"\\n💡 Troubleshooting:\")\n",
        "    print(\"   • Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   • Try: pip install -e .\")\n",
        "    print(\"   • Check dependencies: pip install tiktoken sentence-transformers chromadb\")\n",
        "    print(\"   • Try restarting the kernel\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Initialization Error: {e}\")\n",
        "    print(\"\\n💡 Troubleshooting:\")\n",
        "    print(\"   • Check your Python environment setup\")\n",
        "    print(\"   • Verify all required directories exist\")\n",
        "    print(\"   • For advanced diagnostics, see Developer_Notebook_Clean.ipynb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Document Ingestion\n",
        "# ===========================\n",
        "# This cell processes documents into the ChromaDB vector store\n",
        "\n",
        "print(\"📚 Starting Document Ingestion...\")\n",
        "print(\"\\n📊 Processing Pipeline:\")\n",
        "print(\"   1. 📁 Scan source directory for text files\")\n",
        "print(\"   2. 🔤 Extract and clean text content\")\n",
        "print(\"   3. 🧮 Generate embeddings using SentenceTransformers\")\n",
        "print(\"   4. 💾 Store in ChromaDB with persistent storage\")\n",
        "\n",
        "# Check if system is initialized\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"\\n⚠️ Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    print(\"\\n⏳ Please wait... Document processing may take 30-60 seconds.\")\n",
        "    \n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Run document ingestion\n",
        "        logger.info(\"Starting document ingestion process\")\n",
        "        \n",
        "        # Check for documents in source directory\n",
        "        source_dirs = [\"data/01_raw/source_documents\", \"data/01_raw\"]\n",
        "        docs_found = False\n",
        "        \n",
        "        for source_dir in source_dirs:\n",
        "            if Path(source_dir).exists():\n",
        "                files = list(Path(source_dir).glob(\"*.txt\"))\n",
        "                if files:\n",
        "                    print(f\"   📄 Found {len(files)} .txt files in {source_dir}\")\n",
        "                    ingest(source_dir=source_dir)\n",
        "                    docs_found = True\n",
        "                    break\n",
        "        \n",
        "        if not docs_found:\n",
        "            print(\"   ⚠️ No .txt files found in source directories\")\n",
        "            print(\"   💡 Please add documents to data/01_raw/source_documents/\")\n",
        "            raise FileNotFoundError(\"No source documents found\")\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"\\n🎉 Document ingestion completed in {duration:.1f} seconds!\")\n",
        "        \n",
        "        # Test the vector store\n",
        "        test_docs = retrieve(\"chemical engineering\")\n",
        "        doc_count = len(test_docs) if test_docs else 0\n",
        "        \n",
        "        print(f\"\\n📊 Ingestion Results:\")\n",
        "        print(f\"   • Vector Database: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   • Collection: docs\") \n",
        "        print(f\"   • Test Query Results: {doc_count} documents retrieved\")\n",
        "        print(f\"   • Processing Time: {duration:.1f} seconds\")\n",
        "        \n",
        "        if doc_count > 0:\n",
        "            print(f\"\\n📋 Sample Retrieved Content:\")\n",
        "            sample_doc = test_docs[0][:200] + \"...\" if len(test_docs[0]) > 200 else test_docs[0]\n",
        "            print(f\"   '{sample_doc}'\")\n",
        "        \n",
        "        print(f\"\\n✅ Document ingestion complete! Run Cell 4 to start asking questions.\")\n",
        "        \n",
        "        # Store status for next cells\n",
        "        globals()['ingestion_completed'] = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ingestion Error: {e}\")\n",
        "        print(\"\\n💡 Troubleshooting:\")\n",
        "        print(\"   • Ensure Cell 2 completed successfully\")\n",
        "        print(\"   • Check that data/01_raw/ contains .txt files\")\n",
        "        print(\"   • Verify sufficient disk space for vector database\")\n",
        "        print(\"   • Try restarting the kernel and re-running Cell 2\")\n",
        "        \n",
        "        import traceback\n",
        "        print(f\"\\n🔧 Technical details (for developers):\")\n",
        "        print(f\"   Error type: {type(e).__name__}\")\n",
        "        # Only show first few lines of traceback\n",
        "        tb_lines = traceback.format_exc().split('\\n')[:5]\n",
        "        for line in tb_lines:\n",
        "            if line.strip():\n",
        "                print(f\"   {line}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Interactive Q&A System\n",
        "# ================================\n",
        "# This cell demonstrates the RAG system with sample questions\n",
        "\n",
        "print(\"🚀 PyNucleus Clean Q&A System\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if ingestion was completed\n",
        "if 'ingestion_completed' not in globals() and 'system_initialized' not in globals():\n",
        "    print(\"⚠️ Please run Cell 2 (System Initialization) and Cell 3 (Document Ingestion) first.\")\n",
        "elif 'ingestion_completed' not in globals():\n",
        "    print(\"⚠️ Please run Cell 3 (Document Ingestion) first, or check if documents are already loaded.\")\n",
        "    # Try to test if retrieval works anyway\n",
        "    try:\n",
        "        test_retrieve = retrieve(\"test\", top_k=1)\n",
        "        if test_retrieve:\n",
        "            print(\"✅ Vector database appears to be loaded. Proceeding with Q&A...\")\n",
        "            globals()['ingestion_completed'] = True\n",
        "        else:\n",
        "            print(\"❌ No documents found in vector database.\")\n",
        "    except:\n",
        "        print(\"❌ Cannot access vector database.\")\n",
        "\n",
        "if 'ingestion_completed' in globals() or 'system_initialized' in globals():\n",
        "    print(\"🎯 Ask questions about chemical engineering!\")\n",
        "    print(\"\\n📋 Sample Questions to Try:\")\n",
        "    sample_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work in chemical processes?\",\n",
        "        \"What factors affect reactor conversion efficiency?\",\n",
        "        \"Why do modular plants reduce costs?\",\n",
        "        \"What are the key principles of process safety?\",\n",
        "        \"How do heat exchangers improve energy efficiency?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(sample_questions, 1):\n",
        "        print(f\"   {i}. {question}\")\n",
        "    \n",
        "    print(f\"\\n🚀 Testing with sample questions...\")\n",
        "    \n",
        "    # Test with a few sample questions\n",
        "    test_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work?\",\n",
        "        \"Why do modular plants reduce costs?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n🔍 Question {i}: {question}\")\n",
        "        \n",
        "        try:\n",
        "            start_time = datetime.now()\n",
        "            result = ask(question)\n",
        "            duration = (datetime.now() - start_time).total_seconds()\n",
        "            \n",
        "            answer = result.get(\"answer\", \"No answer generated\")\n",
        "            sources = result.get(\"sources\", [])\n",
        "            \n",
        "            print(f\"⏱️ Response time: {duration:.2f} seconds\")\n",
        "            print(f\"📝 Answer: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
        "            print(f\"📚 Sources: {len(sources)} documents used\")\n",
        "            \n",
        "            if sources:\n",
        "                print(f\"🔗 Source preview: '{sources[0][:100]}...'\" if len(sources[0]) > 100 else f\"🔗 Source preview: '{sources[0]}'\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"💡 To ask custom questions, use:\")\n",
        "    print(\"   result = ask('Your question here')\")\n",
        "    print(\"   print(result['answer'])\")\n",
        "    print(\"   print(result['sources'])\")\n",
        "    \n",
        "    print(f\"\\n🎯 System Status:\")\n",
        "    print(f\"   • Vector Database: ✅ Ready\")\n",
        "    print(f\"   • AI Model: ✅ Loaded\")\n",
        "    print(f\"   • Q&A System: ✅ Active\")\n",
        "    \n",
        "    print(f\"\\n✅ Interactive Q&A ready! Run Cell 5 to view detailed results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: System Status & Results Dashboard\n",
        "# ==========================================\n",
        "# This cell shows comprehensive system status and usage examples\n",
        "\n",
        "print(\"📊 PyNucleus Clean Results Dashboard\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # System health check\n",
        "    print(\"🔍 System Health Check:\")\n",
        "    \n",
        "    # Check vector database\n",
        "    try:\n",
        "        test_retrieval = retrieve(\"test\", top_k=1)\n",
        "        db_status = \"✅ Ready\" if test_retrieval is not None else \"⚠️ Empty\"\n",
        "        print(f\"   • ChromaDB Vector Store: {db_status}\")\n",
        "        \n",
        "        if test_retrieval:\n",
        "            # Get some statistics\n",
        "            sample_retrieval = retrieve(\"chemical engineering\", top_k=10)\n",
        "            print(f\"   • Sample Retrieval: {len(sample_retrieval)} documents found\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   • ChromaDB Vector Store: ❌ Error - {e}\")\n",
        "    \n",
        "    # Check AI model\n",
        "    try:\n",
        "        test_generation = generate(\"Test prompt\", max_tokens=10)\n",
        "        model_status = \"✅ Ready\" if test_generation else \"❌ Error\"\n",
        "        print(f\"   • Qwen AI Model: {model_status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   • Qwen AI Model: ❌ Error - {e}\")\n",
        "    \n",
        "    # Configuration status\n",
        "    print(f\"\\n⚙️ Configuration:\")\n",
        "    print(f\"   • Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   • Vector DB: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   • Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   • Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   • Top-K Retrieval: {settings.RETRIEVE_TOP_K}\")\n",
        "    \n",
        "    # Database statistics\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        db_files = list(chroma_path.rglob(\"*\"))\n",
        "        total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "        print(f\"\\n📊 Database Statistics:\")\n",
        "        print(f\"   • Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "        print(f\"   • Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "    \n",
        "    # Usage examples\n",
        "    print(f\"\\n💡 Usage Examples:\")\n",
        "    print(f\"\")\n",
        "    print(f\"📝 Basic Question:\")\n",
        "    print(f\"   result = ask('What is distillation?')\")\n",
        "    print(f\"   print(result['answer'])\")\n",
        "    print(f\"\")\n",
        "    print(f\"🔍 Document Retrieval:\")\n",
        "    print(f\"   docs = retrieve('modular plants', top_k=3)\")\n",
        "    print(f\"   for doc in docs:\")\n",
        "    print(f\"       print(doc[:100])\")\n",
        "    print(f\"\")\n",
        "    print(f\"🤖 Direct Generation:\")\n",
        "    print(f\"   response = generate('Explain chemical processes', max_tokens=100)\")\n",
        "    print(f\"   print(response)\")\n",
        "    \n",
        "    # CLI usage\n",
        "    print(f\"\\n🔧 Command Line Interface:\")\n",
        "    print(f\"   # Ingest documents\")\n",
        "    print(f\"   pynucleus ingest_docs --source-dir data/01_raw\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Ask questions\")\n",
        "    print(f\"   pynucleus ask 'What are the benefits of modular plants?'\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Run evaluation\")\n",
        "    print(f\"   pynucleus eval_golden\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(f\"\\n📈 Performance Tips:\")\n",
        "    print(f\"   • First question may be slower (model loading)\")\n",
        "    print(f\"   • Subsequent questions are faster (~1-2 seconds)\")\n",
        "    print(f\"   • Use specific technical terms for better retrieval\")\n",
        "    print(f\"   • ChromaDB persists between sessions\")\n",
        "    print(f\"   • Optimal chunk size: 512-1024 tokens\")\n",
        "    \n",
        "    # Next steps\n",
        "    print(f\"\\n🚀 Next Steps:\")\n",
        "    print(f\"   • 📚 Add more documents to data/01_raw/source_documents/\")\n",
        "    print(f\"   • 🔄 Re-run Cell 3 to update vector database\")\n",
        "    print(f\"   • 🔧 Use Developer_Notebook_Clean.ipynb for advanced features\")\n",
        "    print(f\"   • ⚙️ Modify settings.py for custom configuration\")\n",
        "    print(f\"   • 🔍 Run Cell 6 for golden dataset evaluation\")\n",
        "    \n",
        "    print(f\"\\n🎯 System Summary:\")\n",
        "    if 'ingestion_completed' in globals():\n",
        "        print(f\"   ✅ Documents processed and indexed\")\n",
        "        print(f\"   ✅ Q&A system ready\")\n",
        "        print(f\"   ✅ All components functional\")\n",
        "    else:\n",
        "        print(f\"   ⚠️ Run Cell 3 to complete document ingestion\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Dashboard Error: {e}\")\n",
        "    print(\"\\n💡 Troubleshooting:\")\n",
        "    print(\"   • Ensure all previous cells completed successfully\")\n",
        "    print(\"   • Check system configuration and dependencies\")\n",
        "    print(\"   • Try restarting the kernel and re-running all cells\")\n",
        "\n",
        "print(f\"\\n✅ PyNucleus Clean system analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Golden Dataset Evaluation\n",
        "# ==================================\n",
        "# This cell runs validation tests using the golden dataset\n",
        "\n",
        "print(\"🔍 Golden Dataset Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if system is ready\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"⚠️ Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    try:\n",
        "        from pynucleus.eval.golden_eval import run_eval\n",
        "        \n",
        "        print(\"🎯 Running Golden Dataset Evaluation...\")\n",
        "        print(\"\\n📋 Evaluation Process:\")\n",
        "        print(\"   1. 📄 Load golden dataset from data/validation/golden_dataset.csv\")\n",
        "        print(\"   2. 🔍 Query each question through the RAG system\")\n",
        "        print(\"   3. 🎯 Check if expected keywords appear in responses\")\n",
        "        print(\"   4. 📊 Calculate overall accuracy score\")\n",
        "        \n",
        "        # Check if golden dataset exists\n",
        "        golden_path = Path(\"data/validation/golden_dataset.csv\")\n",
        "        if not golden_path.exists():\n",
        "            print(f\"\\n❌ Golden dataset not found at {golden_path}\")\n",
        "            print(\"💡 Please ensure the golden dataset file exists\")\n",
        "        else:\n",
        "            # Count questions in dataset\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(golden_path)\n",
        "            question_count = len(df)\n",
        "            \n",
        "            print(f\"\\n📊 Dataset Info:\")\n",
        "            print(f\"   • Questions: {question_count}\")\n",
        "            print(f\"   • Domains: {len(df['domain'].unique())} unique domains\")\n",
        "            print(f\"   • Difficulty levels: {list(df['difficulty'].unique())}\")\n",
        "            \n",
        "            print(f\"\\n⏳ Running evaluation... This may take 1-2 minutes.\")\n",
        "            \n",
        "            start_time = datetime.now()\n",
        "            \n",
        "            # Run evaluation with different thresholds\n",
        "            thresholds = [0.6, 0.7, 0.8]\n",
        "            results = {}\n",
        "            \n",
        "            for threshold in thresholds:\n",
        "                success = run_eval(threshold=threshold)\n",
        "                results[threshold] = success\n",
        "                print(f\"   📊 Threshold {threshold:.0%}: {'✅ PASSED' if success else '❌ FAILED'}\")\n",
        "            \n",
        "            end_time = datetime.now()\n",
        "            duration = (end_time - start_time).total_seconds()\n",
        "            \n",
        "            print(f\"\\n🎉 Evaluation completed in {duration:.1f} seconds!\")\n",
        "            \n",
        "            # Summary\n",
        "            print(f\"\\n📊 Evaluation Summary:\")\n",
        "            print(f\"   • Total Questions: {question_count}\")\n",
        "            print(f\"   • Evaluation Time: {duration:.1f} seconds\")\n",
        "            print(f\"   • Average Time per Question: {duration/question_count:.1f} seconds\")\n",
        "            \n",
        "            # Recommendations based on results\n",
        "            if results.get(0.8, False):\n",
        "                print(f\"\\n🎯 Performance Assessment: EXCELLENT\")\n",
        "                print(f\"   ✅ System exceeds 80% accuracy threshold\")\n",
        "                print(f\"   ✅ Ready for production use\")\n",
        "            elif results.get(0.7, False):\n",
        "                print(f\"\\n🎯 Performance Assessment: GOOD\")\n",
        "                print(f\"   ✅ System meets 70% accuracy threshold\")\n",
        "                print(f\"   💡 Consider adding more domain-specific documents\")\n",
        "            elif results.get(0.6, False):\n",
        "                print(f\"\\n🎯 Performance Assessment: ACCEPTABLE\")\n",
        "                print(f\"   ⚠️ System meets 60% accuracy threshold\")\n",
        "                print(f\"   💡 Recommendations:\")\n",
        "                print(f\"      • Add more comprehensive source documents\")\n",
        "                print(f\"      • Review chunking strategy\")\n",
        "                print(f\"      • Consider fine-tuning retrieval parameters\")\n",
        "            else:\n",
        "                print(f\"\\n🎯 Performance Assessment: NEEDS IMPROVEMENT\")\n",
        "                print(f\"   ❌ System below 60% accuracy threshold\")\n",
        "                print(f\"   💡 Action Items:\")\n",
        "                print(f\"      • Review and expand document collection\")\n",
        "                print(f\"      • Verify document quality and relevance\")\n",
        "                print(f\"      • Check embedding model configuration\")\n",
        "                print(f\"      • Consider adjusting chunk size and overlap\")\n",
        "            \n",
        "            print(f\"\\n🔧 Advanced Analysis:\")\n",
        "            print(f\"   • For detailed per-question analysis, see Developer_Notebook_Clean.ipynb\")\n",
        "            print(f\"   • For system diagnostics, run: python scripts/system_validator.py\")\n",
        "            print(f\"   • For comprehensive testing, run: python scripts/comprehensive_system_diagnostic.py\")\n",
        "            \n",
        "    except ImportError as e:\n",
        "        print(f\"❌ Import Error: {e}\")\n",
        "        print(\"💡 Ensure golden evaluation module is available\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Evaluation Error: {e}\")\n",
        "        print(\"\\n💡 Troubleshooting:\")\n",
        "        print(\"   • Ensure documents are ingested (run Cell 3)\")\n",
        "        print(\"   • Check that golden dataset exists\")\n",
        "        print(\"   • Verify system is properly initialized\")\n",
        "\n",
        "print(f\"\\n✅ Golden dataset evaluation complete!\")\n",
        "print(f\"🎯 PyNucleus Clean system fully validated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Starting GitHub update...\n",
            " Files added to staging\n",
            "[main 449ba67] Update: 2025-06-19 17:54:05\n",
            " 14 files changed, 480 insertions(+), 75 deletions(-)\n",
            " create mode 100644 Fixes.txt\n",
            " create mode 100644 logs/pynucleus_20250619_173708.log\n",
            " create mode 100644 logs/pynucleus_20250619_174101.log\n",
            " create mode 100644 logs/pynucleus_20250619_174244.log\n",
            " create mode 100644 logs/pynucleus_20250619_175156.log\n",
            " create mode 100644 logs/pynucleus_20250619_175157.log\n",
            " Changes committed\n",
            "Enumerating objects: 162, done.\n",
            "Counting objects: 100% (162/162), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (123/123), done.\n",
            "Writing objects: 100% (129/129), 376.41 MiB | 9.39 MiB/s, done.\n",
            "Total 129 (delta 59), reused 0 (delta 0), pack-reused 0 (from 0)\n",
            "remote: Resolving deltas: 100% (59/59), completed with 17 local objects.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: Trace: 6ebc5f3e8d1edc93688a01e2203d0e4dcb0ede607c5c5687136a3b2e9610ddab\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: File models/qwen-0.5b.Q4_K_M.gguf is 388.29 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/Saytor20/PyNucleus-Model.git\n",
            " \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Saytor20/PyNucleus-Model.git'\n",
            "\u001b[m Changes pushed to GitHub successfully!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# VERSION CONTROL (Optional - For Maintainers Only)\n",
        "# ========================================\n",
        "# Uncomment the lines below if you need to update the repository:\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Log end time\n",
        "with open(\"update_log.txt\", \"a\") as f:\n",
        "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
        "\n",
        "# Simple GitHub update function\n",
        "def update_github():\n",
        "    print(\" Starting GitHub update...\")\n",
        "    !git add .\n",
        "    print(\" Files added to staging\")\n",
        "    !git commit -m \"Update: $(date +'%Y-%m-%d %H:%M:%S')\"\n",
        "    print(\" Changes committed\")\n",
        "    !git push origin main\n",
        "    print(\" Changes pushed to GitHub successfully!\")\n",
        "\n",
        "# To use it, just run:\n",
        "update_github()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python run_pipeline.py chat\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
