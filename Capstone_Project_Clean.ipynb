{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **PyNucleus Model - Clean Architecture** ðŸš€\n",
        "\n",
        "## **Welcome to PyNucleus Clean!** \n",
        "\n",
        "This notebook provides a streamlined interface to run the PyNucleus pipeline using the new clean architecture with ChromaDB and Qwen models.\n",
        "\n",
        "### **What PyNucleus Clean Does:**\n",
        "- **ðŸ“š Document Analysis**: Processes chemical engineering documents using ChromaDB vector store\n",
        "- **ðŸ¤– AI Generation**: Uses quantized Qwen models for intelligent responses  \n",
        "- **ðŸ“Š Results Export**: Automatically exports comprehensive results to structured formats\n",
        "- **ðŸ’¡ RAG Integration**: Combines document retrieval with AI generation for enhanced analysis\n",
        "- **ðŸ”— Modular Design**: Clean, maintainable architecture with minimal dependencies\n",
        "\n",
        "### **âœ¨ Clean Architecture Features:**\n",
        "- âœ… **ChromaDB Integration**: Modern vector database with persistent storage\n",
        "- âœ… **Qwen Model**: Efficient 0.5B parameter model with 4-bit quantization\n",
        "- âœ… **Pydantic Settings**: Type-safe configuration with validation\n",
        "- âœ… **Loguru Logging**: Beautiful structured logging with colors\n",
        "- âœ… **Minimal Dependencies**: Streamlined requirements for better performance\n",
        "- âœ… **Golden Dataset Validation**: Automated accuracy testing and evaluation\n",
        "\n",
        "### **ðŸ“‹ How to Use This Notebook:**\n",
        "0. **ðŸ” Analyze Performance** (Cell 0): Review ChromaDB and chunking performance\n",
        "1. **ðŸ”§ Initialize System** (Cell 1): Set up PyNucleus with automatic validation\n",
        "2. **ðŸ“š Ingest Documents** (Cell 2): Process documents into ChromaDB\n",
        "3. **ðŸš€ Ask Questions** (Cell 3): Query the system with natural language\n",
        "4. **ðŸ“Š View Results** (Cell 4): Explore responses and sources\n",
        "5. **ðŸ” Run Evaluation** (Cell 5): Test system accuracy with golden dataset\n",
        "\n",
        "**âš¡ Simple 6-step process for powerful chemical engineering Q&A!**\n",
        "\n",
        "---\n",
        "**ðŸ”§ For developers**: Advanced features available in `Developer_Notebook_Clean.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: ChromaDB Performance & Chunking Analysis\n",
        "# =================================================\n",
        "# This cell analyzes ChromaDB performance and chunking strategies\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"ðŸ” ChromaDB Performance & Chunking Analysis\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ðŸ“… Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "def analyze_data_structure():\n",
        "    \"\"\"Analyze current data directory structure\"\"\"\n",
        "    print(\"\\nðŸ“ Data Structure Analysis:\")\n",
        "    \n",
        "    data_dirs = {\n",
        "        'data/01_raw/source_documents': 'Source documents for processing',\n",
        "        'data/01_raw/web_sources': 'Web-scraped content',\n",
        "        'data/03_intermediate/converted_chunked_data': 'Text chunks from processing',\n",
        "        'data/03_intermediate/vector_db': 'ChromaDB database storage',\n",
        "        'data/04_models/chunk_reports': 'Chunking performance analysis',\n",
        "        'data/04_models/recall_evaluation': 'RAG system recall metrics',\n",
        "        'data/validation': 'Golden dataset and validation results'\n",
        "    }\n",
        "    \n",
        "    for dir_path, description in data_dirs.items():\n",
        "        path = Path(dir_path)\n",
        "        if path.exists():\n",
        "            if path.is_dir():\n",
        "                file_count = len([f for f in path.iterdir() if f.is_file()])\n",
        "                dir_count = len([d for d in path.iterdir() if d.is_dir()])\n",
        "                print(f\"   âœ… {dir_path}: {file_count} files, {dir_count} subdirs - {description}\")\n",
        "            else:\n",
        "                print(f\"   ðŸ“„ {dir_path}: File exists - {description}\")\n",
        "        else:\n",
        "            print(f\"   âŒ {dir_path}: Missing - {description}\")\n",
        "\n",
        "def analyze_chunking_strategy():\n",
        "    \"\"\"Analyze current chunking configuration\"\"\"\n",
        "    print(\"\\nðŸ”¤ Chunking Strategy Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        print(f\"   â€¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "        print(f\"   â€¢ Retrieval Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "        \n",
        "        # Check if chunk data exists\n",
        "        chunk_dir = Path(\"data/03_intermediate/converted_chunked_data\")\n",
        "        if chunk_dir.exists():\n",
        "            chunk_files = list(chunk_dir.glob(\"*.json\"))\n",
        "            print(f\"   â€¢ Chunk Files: {len(chunk_files)} files found\")\n",
        "            \n",
        "            if chunk_files:\n",
        "                # Analyze a sample chunk file\n",
        "                with open(chunk_files[0], 'r') as f:\n",
        "                    sample_chunk = json.load(f)\n",
        "                    print(f\"   â€¢ Sample chunk keys: {list(sample_chunk.keys())}\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸ No chunk data found in {chunk_dir}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error analyzing chunking: {e}\")\n",
        "\n",
        "def analyze_chromadb_performance():\n",
        "    \"\"\"Analyze ChromaDB setup and performance\"\"\"\n",
        "    print(\"\\nðŸ—„ï¸ ChromaDB Performance Analysis:\")\n",
        "    \n",
        "    try:\n",
        "        from pynucleus.settings import settings\n",
        "        \n",
        "        chroma_path = Path(settings.CHROMA_PATH)\n",
        "        print(f\"   â€¢ ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   â€¢ Database Exists: {'âœ…' if chroma_path.exists() else 'âŒ'}\")\n",
        "        \n",
        "        if chroma_path.exists():\n",
        "            # Check database size\n",
        "            db_files = list(chroma_path.rglob(\"*\"))\n",
        "            total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "            print(f\"   â€¢ Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "            print(f\"   â€¢ Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "            \n",
        "        # Test retrieval if possible\n",
        "        try:\n",
        "            from pynucleus.rag.engine import retrieve\n",
        "            print(f\"   â€¢ Retrieval Engine: âœ… Available\")\n",
        "            \n",
        "            # Test basic retrieval\n",
        "            test_docs = retrieve(\"chemical engineering\", top_k=1)\n",
        "            if test_docs and len(test_docs) > 0:\n",
        "                print(f\"   â€¢ Test Retrieval: âœ… {len(test_docs)} documents found\")\n",
        "                sample_length = len(test_docs[0]) if test_docs[0] else 0\n",
        "                print(f\"   â€¢ Sample Document Length: {sample_length} characters\")\n",
        "            else:\n",
        "                print(f\"   â€¢ Test Retrieval: âš ï¸ No documents found\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   â€¢ Retrieval Engine: âŒ Error - {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ ChromaDB analysis failed: {e}\")\n",
        "\n",
        "def performance_recommendations():\n",
        "    \"\"\"Provide performance optimization recommendations\"\"\"\n",
        "    print(\"\\nðŸ’¡ Performance Optimization Recommendations:\")\n",
        "    \n",
        "    recommendations = [\n",
        "        \"ðŸ”¤ Chunking: Ensure optimal chunk size (512-1024 tokens) for your documents\",\n",
        "        \"ðŸ§® Embeddings: Use 'all-MiniLM-L6-v2' for faster processing or 'all-mpnet-base-v2' for better quality\",\n",
        "        \"ðŸ—„ï¸ ChromaDB: Enable persistence and consider indexing parameters for large datasets\",\n",
        "        \"ðŸ” Retrieval: Tune top-k value (4-8) based on your accuracy requirements\",\n",
        "        \"ðŸ’¾ Storage: Monitor database size and consider compression for large document sets\",\n",
        "        \"âš¡ Performance: First query is slower (model loading), subsequent queries are faster\"\n",
        "    ]\n",
        "    \n",
        "    for rec in recommendations:\n",
        "        print(f\"   {rec}\")\n",
        "\n",
        "# Run all analysis functions\n",
        "try:\n",
        "    analyze_data_structure()\n",
        "    analyze_chunking_strategy()\n",
        "    analyze_chromadb_performance()\n",
        "    performance_recommendations()\n",
        "    \n",
        "    print(f\"\\nâœ… ChromaDB Performance Analysis Complete!\")\n",
        "    print(f\"ðŸ“ Next: Run Cell 2 to initialize the PyNucleus system\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Analysis Error: {e}\")\n",
        "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "    print(\"   â€¢ Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   â€¢ Check that the data directory structure exists\")\n",
        "    print(\"   â€¢ Try running this cell again after initializing the system\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: System Initialization & Validation\n",
        "# ==========================================\n",
        "# This cell sets up PyNucleus Clean and validates all components\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ðŸ”§ Initializing PyNucleus Clean Architecture...\")\n",
        "print(f\"ðŸ“… Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = str(Path().resolve() / \"src\")\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "try:\n",
        "    # Import PyNucleus Clean components\n",
        "    from pynucleus.settings import settings\n",
        "    from pynucleus.utils.logger import logger\n",
        "    from pynucleus.rag.collector import ingest\n",
        "    from pynucleus.rag.engine import ask, retrieve\n",
        "    from pynucleus.llm.qwen_loader import generate\n",
        "    \n",
        "    print(\"âœ… PyNucleus Clean modules imported successfully\")\n",
        "    \n",
        "    # Validate configuration\n",
        "    print(f\"ðŸ“‹ Configuration:\")\n",
        "    print(f\"   â€¢ ChromaDB Path: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   â€¢ Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   â€¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   â€¢ Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   â€¢ Retrieve Top-K: {settings.RETRIEVE_TOP_K}\")\n",
        "    print(f\"   â€¢ Log Level: {settings.LOG_LEVEL}\")\n",
        "    print(f\"   â€¢ Use CUDA: {settings.USE_CUDA}\")\n",
        "    \n",
        "    # Test logging\n",
        "    logger.info(\"PyNucleus Clean initialization successful\")\n",
        "    \n",
        "    print(\"âœ… Configuration validated\")\n",
        "    print(\"âœ… Logging system active\")\n",
        "    \n",
        "    print(\"\\nðŸ“‹ System Components Ready:\")\n",
        "    print(\"   â€¢ ðŸ“š ChromaDB Vector Store - Modern document indexing\")\n",
        "    print(\"   â€¢ ðŸ¤– Qwen Model - Efficient quantized AI generation\")\n",
        "    print(\"   â€¢ ðŸ“Š Document Ingestion - Text processing and embedding\")\n",
        "    print(\"   â€¢ ðŸ’¡ RAG Engine - Retrieval-augmented generation\")\n",
        "    print(\"   â€¢ ðŸ” Golden Dataset Evaluation - Validation and testing\")\n",
        "    \n",
        "    # Check for existing vector database\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        print(f\"\\nðŸ“ Vector Database: {settings.CHROMA_PATH} (âœ… Exists)\")\n",
        "        print(\"ðŸŽ¯ System ready! You can skip to Cell 4 if documents are already ingested.\")\n",
        "    else:\n",
        "        print(f\"\\nðŸ“ Vector Database: {settings.CHROMA_PATH} (âŒ Not Found)\")\n",
        "        print(\"ðŸŽ¯ System ready! Execute Cell 3 to ingest documents.\")\n",
        "    \n",
        "    # Store initialization status for other cells\n",
        "    globals()['system_initialized'] = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import Error: {e}\")\n",
        "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "    print(\"   â€¢ Ensure you're in the PyNucleus-Model directory\")\n",
        "    print(\"   â€¢ Try: pip install -e .\")\n",
        "    print(\"   â€¢ Check dependencies: pip install tiktoken sentence-transformers chromadb\")\n",
        "    print(\"   â€¢ Try restarting the kernel\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Initialization Error: {e}\")\n",
        "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "    print(\"   â€¢ Check your Python environment setup\")\n",
        "    print(\"   â€¢ Verify all required directories exist\")\n",
        "    print(\"   â€¢ For advanced diagnostics, see Developer_Notebook_Clean.ipynb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Document Ingestion\n",
        "# ===========================\n",
        "# This cell processes documents into the ChromaDB vector store\n",
        "\n",
        "print(\"ðŸ“š Starting Document Ingestion...\")\n",
        "print(\"\\nðŸ“Š Processing Pipeline:\")\n",
        "print(\"   1. ðŸ“ Scan source directory for text files\")\n",
        "print(\"   2. ðŸ”¤ Extract and clean text content\")\n",
        "print(\"   3. ðŸ§® Generate embeddings using SentenceTransformers\")\n",
        "print(\"   4. ðŸ’¾ Store in ChromaDB with persistent storage\")\n",
        "\n",
        "# Check if system is initialized\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"\\nâš ï¸ Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    print(\"\\nâ³ Please wait... Document processing may take 30-60 seconds.\")\n",
        "    \n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Run document ingestion\n",
        "        logger.info(\"Starting document ingestion process\")\n",
        "        \n",
        "        # Check for documents in source directory\n",
        "        source_dirs = [\"data/01_raw/source_documents\", \"data/01_raw\"]\n",
        "        docs_found = False\n",
        "        \n",
        "        for source_dir in source_dirs:\n",
        "            if Path(source_dir).exists():\n",
        "                files = list(Path(source_dir).glob(\"*.txt\"))\n",
        "                if files:\n",
        "                    print(f\"   ðŸ“„ Found {len(files)} .txt files in {source_dir}\")\n",
        "                    ingest(source_dir=source_dir)\n",
        "                    docs_found = True\n",
        "                    break\n",
        "        \n",
        "        if not docs_found:\n",
        "            print(\"   âš ï¸ No .txt files found in source directories\")\n",
        "            print(\"   ðŸ’¡ Please add documents to data/01_raw/source_documents/\")\n",
        "            raise FileNotFoundError(\"No source documents found\")\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"\\nðŸŽ‰ Document ingestion completed in {duration:.1f} seconds!\")\n",
        "        \n",
        "        # Test the vector store\n",
        "        test_docs = retrieve(\"chemical engineering\")\n",
        "        doc_count = len(test_docs) if test_docs else 0\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Ingestion Results:\")\n",
        "        print(f\"   â€¢ Vector Database: {settings.CHROMA_PATH}\")\n",
        "        print(f\"   â€¢ Collection: docs\") \n",
        "        print(f\"   â€¢ Test Query Results: {doc_count} documents retrieved\")\n",
        "        print(f\"   â€¢ Processing Time: {duration:.1f} seconds\")\n",
        "        \n",
        "        if doc_count > 0:\n",
        "            print(f\"\\nðŸ“‹ Sample Retrieved Content:\")\n",
        "            sample_doc = test_docs[0][:200] + \"...\" if len(test_docs[0]) > 200 else test_docs[0]\n",
        "            print(f\"   '{sample_doc}'\")\n",
        "        \n",
        "        print(f\"\\nâœ… Document ingestion complete! Run Cell 4 to start asking questions.\")\n",
        "        \n",
        "        # Store status for next cells\n",
        "        globals()['ingestion_completed'] = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ingestion Error: {e}\")\n",
        "        print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "        print(\"   â€¢ Ensure Cell 2 completed successfully\")\n",
        "        print(\"   â€¢ Check that data/01_raw/ contains .txt files\")\n",
        "        print(\"   â€¢ Verify sufficient disk space for vector database\")\n",
        "        print(\"   â€¢ Try restarting the kernel and re-running Cell 2\")\n",
        "        \n",
        "        import traceback\n",
        "        print(f\"\\nðŸ”§ Technical details (for developers):\")\n",
        "        print(f\"   Error type: {type(e).__name__}\")\n",
        "        # Only show first few lines of traceback\n",
        "        tb_lines = traceback.format_exc().split('\\n')[:5]\n",
        "        for line in tb_lines:\n",
        "            if line.strip():\n",
        "                print(f\"   {line}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Interactive Q&A System\n",
        "# ================================\n",
        "# This cell demonstrates the RAG system with sample questions\n",
        "\n",
        "print(\"ðŸš€ PyNucleus Clean Q&A System\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if ingestion was completed\n",
        "if 'ingestion_completed' not in globals() and 'system_initialized' not in globals():\n",
        "    print(\"âš ï¸ Please run Cell 2 (System Initialization) and Cell 3 (Document Ingestion) first.\")\n",
        "elif 'ingestion_completed' not in globals():\n",
        "    print(\"âš ï¸ Please run Cell 3 (Document Ingestion) first, or check if documents are already loaded.\")\n",
        "    # Try to test if retrieval works anyway\n",
        "    try:\n",
        "        test_retrieve = retrieve(\"test\", top_k=1)\n",
        "        if test_retrieve:\n",
        "            print(\"âœ… Vector database appears to be loaded. Proceeding with Q&A...\")\n",
        "            globals()['ingestion_completed'] = True\n",
        "        else:\n",
        "            print(\"âŒ No documents found in vector database.\")\n",
        "    except:\n",
        "        print(\"âŒ Cannot access vector database.\")\n",
        "\n",
        "if 'ingestion_completed' in globals() or 'system_initialized' in globals():\n",
        "    print(\"ðŸŽ¯ Ask questions about chemical engineering!\")\n",
        "    print(\"\\nðŸ“‹ Sample Questions to Try:\")\n",
        "    sample_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work in chemical processes?\",\n",
        "        \"What factors affect reactor conversion efficiency?\",\n",
        "        \"Why do modular plants reduce costs?\",\n",
        "        \"What are the key principles of process safety?\",\n",
        "        \"How do heat exchangers improve energy efficiency?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(sample_questions, 1):\n",
        "        print(f\"   {i}. {question}\")\n",
        "    \n",
        "    print(f\"\\nðŸš€ Testing with sample questions...\")\n",
        "    \n",
        "    # Test with a few sample questions\n",
        "    test_questions = [\n",
        "        \"What are the advantages of modular chemical plants?\",\n",
        "        \"How does distillation work?\",\n",
        "        \"Why do modular plants reduce costs?\"\n",
        "    ]\n",
        "    \n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nðŸ” Question {i}: {question}\")\n",
        "        \n",
        "        try:\n",
        "            start_time = datetime.now()\n",
        "            result = ask(question)\n",
        "            duration = (datetime.now() - start_time).total_seconds()\n",
        "            \n",
        "            answer = result.get(\"answer\", \"No answer generated\")\n",
        "            sources = result.get(\"sources\", [])\n",
        "            \n",
        "            print(f\"â±ï¸ Response time: {duration:.2f} seconds\")\n",
        "            print(f\"ðŸ“ Answer: {answer[:300]}{'...' if len(answer) > 300 else ''}\")\n",
        "            print(f\"ðŸ“š Sources: {len(sources)} documents used\")\n",
        "            \n",
        "            if sources:\n",
        "                print(f\"ðŸ”— Source preview: '{sources[0][:100]}...'\" if len(sources[0]) > 100 else f\"ðŸ”— Source preview: '{sources[0]}'\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸ’¡ To ask custom questions, use:\")\n",
        "    print(\"   result = ask('Your question here')\")\n",
        "    print(\"   print(result['answer'])\")\n",
        "    print(\"   print(result['sources'])\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ System Status:\")\n",
        "    print(f\"   â€¢ Vector Database: âœ… Ready\")\n",
        "    print(f\"   â€¢ AI Model: âœ… Loaded\")\n",
        "    print(f\"   â€¢ Q&A System: âœ… Active\")\n",
        "    \n",
        "    print(f\"\\nâœ… Interactive Q&A ready! Run Cell 5 to view detailed results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: System Status & Results Dashboard\n",
        "# ==========================================\n",
        "# This cell shows comprehensive system status and usage examples\n",
        "\n",
        "print(\"ðŸ“Š PyNucleus Clean Results Dashboard\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # System health check\n",
        "    print(\"ðŸ” System Health Check:\")\n",
        "    \n",
        "    # Check vector database\n",
        "    try:\n",
        "        test_retrieval = retrieve(\"test\", top_k=1)\n",
        "        db_status = \"âœ… Ready\" if test_retrieval is not None else \"âš ï¸ Empty\"\n",
        "        print(f\"   â€¢ ChromaDB Vector Store: {db_status}\")\n",
        "        \n",
        "        if test_retrieval:\n",
        "            # Get some statistics\n",
        "            sample_retrieval = retrieve(\"chemical engineering\", top_k=10)\n",
        "            print(f\"   â€¢ Sample Retrieval: {len(sample_retrieval)} documents found\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   â€¢ ChromaDB Vector Store: âŒ Error - {e}\")\n",
        "    \n",
        "    # Check AI model\n",
        "    try:\n",
        "        test_generation = generate(\"Test prompt\", max_tokens=10)\n",
        "        model_status = \"âœ… Ready\" if test_generation else \"âŒ Error\"\n",
        "        print(f\"   â€¢ Qwen AI Model: {model_status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   â€¢ Qwen AI Model: âŒ Error - {e}\")\n",
        "    \n",
        "    # Configuration status\n",
        "    print(f\"\\nâš™ï¸ Configuration:\")\n",
        "    print(f\"   â€¢ Model: {settings.MODEL_ID}\")\n",
        "    print(f\"   â€¢ Vector DB: {settings.CHROMA_PATH}\")\n",
        "    print(f\"   â€¢ Embedding Model: {settings.EMB_MODEL}\")\n",
        "    print(f\"   â€¢ Max Tokens: {settings.MAX_TOKENS}\")\n",
        "    print(f\"   â€¢ Top-K Retrieval: {settings.RETRIEVE_TOP_K}\")\n",
        "    \n",
        "    # Database statistics\n",
        "    chroma_path = Path(settings.CHROMA_PATH)\n",
        "    if chroma_path.exists():\n",
        "        db_files = list(chroma_path.rglob(\"*\"))\n",
        "        total_size = sum(f.stat().st_size for f in db_files if f.is_file())\n",
        "        print(f\"\\nðŸ“Š Database Statistics:\")\n",
        "        print(f\"   â€¢ Database Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "        print(f\"   â€¢ Database Files: {len([f for f in db_files if f.is_file()])} files\")\n",
        "    \n",
        "    # Usage examples\n",
        "    print(f\"\\nðŸ’¡ Usage Examples:\")\n",
        "    print(f\"\")\n",
        "    print(f\"ðŸ“ Basic Question:\")\n",
        "    print(f\"   result = ask('What is distillation?')\")\n",
        "    print(f\"   print(result['answer'])\")\n",
        "    print(f\"\")\n",
        "    print(f\"ðŸ” Document Retrieval:\")\n",
        "    print(f\"   docs = retrieve('modular plants', top_k=3)\")\n",
        "    print(f\"   for doc in docs:\")\n",
        "    print(f\"       print(doc[:100])\")\n",
        "    print(f\"\")\n",
        "    print(f\"ðŸ¤– Direct Generation:\")\n",
        "    print(f\"   response = generate('Explain chemical processes', max_tokens=100)\")\n",
        "    print(f\"   print(response)\")\n",
        "    \n",
        "    # CLI usage\n",
        "    print(f\"\\nðŸ”§ Command Line Interface:\")\n",
        "    print(f\"   # Ingest documents\")\n",
        "    print(f\"   pynucleus ingest_docs --source-dir data/01_raw\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Ask questions\")\n",
        "    print(f\"   pynucleus ask 'What are the benefits of modular plants?'\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # Run evaluation\")\n",
        "    print(f\"   pynucleus eval_golden\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(f\"\\nðŸ“ˆ Performance Tips:\")\n",
        "    print(f\"   â€¢ First question may be slower (model loading)\")\n",
        "    print(f\"   â€¢ Subsequent questions are faster (~1-2 seconds)\")\n",
        "    print(f\"   â€¢ Use specific technical terms for better retrieval\")\n",
        "    print(f\"   â€¢ ChromaDB persists between sessions\")\n",
        "    print(f\"   â€¢ Optimal chunk size: 512-1024 tokens\")\n",
        "    \n",
        "    # Next steps\n",
        "    print(f\"\\nðŸš€ Next Steps:\")\n",
        "    print(f\"   â€¢ ðŸ“š Add more documents to data/01_raw/source_documents/\")\n",
        "    print(f\"   â€¢ ðŸ”„ Re-run Cell 3 to update vector database\")\n",
        "    print(f\"   â€¢ ðŸ”§ Use Developer_Notebook_Clean.ipynb for advanced features\")\n",
        "    print(f\"   â€¢ âš™ï¸ Modify settings.py for custom configuration\")\n",
        "    print(f\"   â€¢ ðŸ” Run Cell 6 for golden dataset evaluation\")\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ System Summary:\")\n",
        "    if 'ingestion_completed' in globals():\n",
        "        print(f\"   âœ… Documents processed and indexed\")\n",
        "        print(f\"   âœ… Q&A system ready\")\n",
        "        print(f\"   âœ… All components functional\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸ Run Cell 3 to complete document ingestion\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Dashboard Error: {e}\")\n",
        "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "    print(\"   â€¢ Ensure all previous cells completed successfully\")\n",
        "    print(\"   â€¢ Check system configuration and dependencies\")\n",
        "    print(\"   â€¢ Try restarting the kernel and re-running all cells\")\n",
        "\n",
        "print(f\"\\nâœ… PyNucleus Clean system analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Golden Dataset Evaluation\n",
        "# ==================================\n",
        "# This cell runs validation tests using the golden dataset\n",
        "\n",
        "print(\"ðŸ” Golden Dataset Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if system is ready\n",
        "if 'system_initialized' not in globals():\n",
        "    print(\"âš ï¸ Please run Cell 2 (System Initialization) first.\")\n",
        "else:\n",
        "    try:\n",
        "        from pynucleus.eval.golden_eval import run_eval\n",
        "        \n",
        "        print(\"ðŸŽ¯ Running Golden Dataset Evaluation...\")\n",
        "        print(\"\\nðŸ“‹ Evaluation Process:\")\n",
        "        print(\"   1. ðŸ“„ Load golden dataset from data/validation/golden_dataset.csv\")\n",
        "        print(\"   2. ðŸ” Query each question through the RAG system\")\n",
        "        print(\"   3. ðŸŽ¯ Check if expected keywords appear in responses\")\n",
        "        print(\"   4. ðŸ“Š Calculate overall accuracy score\")\n",
        "        \n",
        "        # Check if golden dataset exists\n",
        "        golden_path = Path(\"data/validation/golden_dataset.csv\")\n",
        "        if not golden_path.exists():\n",
        "            print(f\"\\nâŒ Golden dataset not found at {golden_path}\")\n",
        "            print(\"ðŸ’¡ Please ensure the golden dataset file exists\")\n",
        "        else:\n",
        "            # Count questions in dataset\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(golden_path)\n",
        "            question_count = len(df)\n",
        "            \n",
        "            print(f\"\\nðŸ“Š Dataset Info:\")\n",
        "            print(f\"   â€¢ Questions: {question_count}\")\n",
        "            print(f\"   â€¢ Domains: {len(df['domain'].unique())} unique domains\")\n",
        "            print(f\"   â€¢ Difficulty levels: {list(df['difficulty'].unique())}\")\n",
        "            \n",
        "            print(f\"\\nâ³ Running evaluation... This may take 1-2 minutes.\")\n",
        "            \n",
        "            start_time = datetime.now()\n",
        "            \n",
        "            # Run evaluation with different thresholds\n",
        "            thresholds = [0.6, 0.7, 0.8]\n",
        "            results = {}\n",
        "            \n",
        "            for threshold in thresholds:\n",
        "                success = run_eval(threshold=threshold)\n",
        "                results[threshold] = success\n",
        "                print(f\"   ðŸ“Š Threshold {threshold:.0%}: {'âœ… PASSED' if success else 'âŒ FAILED'}\")\n",
        "            \n",
        "            end_time = datetime.now()\n",
        "            duration = (end_time - start_time).total_seconds()\n",
        "            \n",
        "            print(f\"\\nðŸŽ‰ Evaluation completed in {duration:.1f} seconds!\")\n",
        "            \n",
        "            # Summary\n",
        "            print(f\"\\nðŸ“Š Evaluation Summary:\")\n",
        "            print(f\"   â€¢ Total Questions: {question_count}\")\n",
        "            print(f\"   â€¢ Evaluation Time: {duration:.1f} seconds\")\n",
        "            print(f\"   â€¢ Average Time per Question: {duration/question_count:.1f} seconds\")\n",
        "            \n",
        "            # Recommendations based on results\n",
        "            if results.get(0.8, False):\n",
        "                print(f\"\\nðŸŽ¯ Performance Assessment: EXCELLENT\")\n",
        "                print(f\"   âœ… System exceeds 80% accuracy threshold\")\n",
        "                print(f\"   âœ… Ready for production use\")\n",
        "            elif results.get(0.7, False):\n",
        "                print(f\"\\nðŸŽ¯ Performance Assessment: GOOD\")\n",
        "                print(f\"   âœ… System meets 70% accuracy threshold\")\n",
        "                print(f\"   ðŸ’¡ Consider adding more domain-specific documents\")\n",
        "            elif results.get(0.6, False):\n",
        "                print(f\"\\nðŸŽ¯ Performance Assessment: ACCEPTABLE\")\n",
        "                print(f\"   âš ï¸ System meets 60% accuracy threshold\")\n",
        "                print(f\"   ðŸ’¡ Recommendations:\")\n",
        "                print(f\"      â€¢ Add more comprehensive source documents\")\n",
        "                print(f\"      â€¢ Review chunking strategy\")\n",
        "                print(f\"      â€¢ Consider fine-tuning retrieval parameters\")\n",
        "            else:\n",
        "                print(f\"\\nðŸŽ¯ Performance Assessment: NEEDS IMPROVEMENT\")\n",
        "                print(f\"   âŒ System below 60% accuracy threshold\")\n",
        "                print(f\"   ðŸ’¡ Action Items:\")\n",
        "                print(f\"      â€¢ Review and expand document collection\")\n",
        "                print(f\"      â€¢ Verify document quality and relevance\")\n",
        "                print(f\"      â€¢ Check embedding model configuration\")\n",
        "                print(f\"      â€¢ Consider adjusting chunk size and overlap\")\n",
        "            \n",
        "            print(f\"\\nðŸ”§ Advanced Analysis:\")\n",
        "            print(f\"   â€¢ For detailed per-question analysis, see Developer_Notebook_Clean.ipynb\")\n",
        "            print(f\"   â€¢ For system diagnostics, run: python scripts/system_validator.py\")\n",
        "            print(f\"   â€¢ For comprehensive testing, run: python scripts/comprehensive_system_diagnostic.py\")\n",
        "            \n",
        "    except ImportError as e:\n",
        "        print(f\"âŒ Import Error: {e}\")\n",
        "        print(\"ðŸ’¡ Ensure golden evaluation module is available\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Evaluation Error: {e}\")\n",
        "        print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "        print(\"   â€¢ Ensure documents are ingested (run Cell 3)\")\n",
        "        print(\"   â€¢ Check that golden dataset exists\")\n",
        "        print(\"   â€¢ Verify system is properly initialized\")\n",
        "\n",
        "print(f\"\\nâœ… Golden dataset evaluation complete!\")\n",
        "print(f\"ðŸŽ¯ PyNucleus Clean system fully validated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Starting GitHub update...\n",
            " Files added to staging\n",
            "[main 449ba67] Update: 2025-06-19 17:54:05\n",
            " 14 files changed, 480 insertions(+), 75 deletions(-)\n",
            " create mode 100644 Fixes.txt\n",
            " create mode 100644 logs/pynucleus_20250619_173708.log\n",
            " create mode 100644 logs/pynucleus_20250619_174101.log\n",
            " create mode 100644 logs/pynucleus_20250619_174244.log\n",
            " create mode 100644 logs/pynucleus_20250619_175156.log\n",
            " create mode 100644 logs/pynucleus_20250619_175157.log\n",
            " Changes committed\n",
            "Enumerating objects: 162, done.\n",
            "Counting objects: 100% (162/162), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (123/123), done.\n",
            "Writing objects: 100% (129/129), 376.41 MiB | 9.39 MiB/s, done.\n",
            "Total 129 (delta 59), reused 0 (delta 0), pack-reused 0 (from 0)\n",
            "remote: Resolving deltas: 100% (59/59), completed with 17 local objects.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: Trace: 6ebc5f3e8d1edc93688a01e2203d0e4dcb0ede607c5c5687136a3b2e9610ddab\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: File models/qwen-0.5b.Q4_K_M.gguf is 388.29 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/Saytor20/PyNucleus-Model.git\n",
            " \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Saytor20/PyNucleus-Model.git'\n",
            "\u001b[m Changes pushed to GitHub successfully!\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# VERSION CONTROL (Optional - For Maintainers Only)\n",
        "# ========================================\n",
        "# Uncomment the lines below if you need to update the repository:\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Log end time\n",
        "with open(\"update_log.txt\", \"a\") as f:\n",
        "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
        "\n",
        "# Simple GitHub update function\n",
        "def update_github():\n",
        "    print(\" Starting GitHub update...\")\n",
        "    !git add .\n",
        "    print(\" Files added to staging\")\n",
        "    !git commit -m \"Update: $(date +'%Y-%m-%d %H:%M:%S')\"\n",
        "    print(\" Changes committed\")\n",
        "    !git push origin main\n",
        "    print(\" Changes pushed to GitHub successfully!\")\n",
        "\n",
        "# To use it, just run:\n",
        "update_github()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python run_pipeline.py chat\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
