{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìÑ Starting processing for 4 file(s) in 'source_documents' ---\n",
      " ‚ñ∂ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " ‚ñ∂ Processing: mcp_basics.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      " ‚ñ∂ Processing: feasibility_factors.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      " ‚ñ∂ Processing: Bist_Madan.pdf\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Bist_Madan.txt\n",
      "\n",
      "\n",
      " All files processed.\n"
     ]
    }
   ],
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"üìÇ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"‚Ñπ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- üìÑ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Skip hidden files like .DS_Store\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "        print(f\" ‚ñ∂ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Handle PDF files differently\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                # Use PyPDF2 for PDF files\n",
    "                reader = PdfReader(input_path)\n",
    "                full_text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\\n\"\n",
    "            else:\n",
    "                # Use UnstructuredLoader for other file types\n",
    "                loader = UnstructuredLoader(input_path)\n",
    "                documents = loader.load()\n",
    "                full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"   ‚Ä¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Ä¢ Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n\\n All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:49:11.411850Z",
     "start_time": "2025-06-04T16:49:11.195022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting Wikipedia article search for 5 keywords...\n",
      "‚ñ∂Ô∏è  Searching for: modular design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_modular_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: software architecture\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_software_architecture.txt\n",
      "‚ñ∂Ô∏è  Searching for: system design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_system_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: industrial design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_industrial_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: supply chain\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_supply_chain.txt\n",
      "\n",
      "‚ú® Article scraping complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Keywords to search for in Wikipedia\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"modular design\",\n",
    "    \"software architecture\",\n",
    "    \"system design\",\n",
    "    \"industrial design\",\n",
    "    \"supply chain\"\n",
    "]\n",
    "\n",
    "# Output directory for saved articles\n",
    "DATA_DIR = \"data_sources\"\n",
    "\n",
    "def search_wikipedia(keyword):\n",
    "    \"\"\"Search Wikipedia for a keyword and return the first result URL\"\"\"\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={quote(keyword)}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['query']['search']:\n",
    "        title = data['query']['search'][0]['title']\n",
    "        return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "    return None\n",
    "\n",
    "def scrape_and_save_article(url, keyword):\n",
    "    \"\"\"Scrape a Wikipedia article and save it as a text file\"\"\"\n",
    "    print(f\"‚ñ∂Ô∏è  Searching for: {keyword}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch the article\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the main content\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        if not content:\n",
    "            print(f\"‚ùå  Could not find article content for: {keyword}\")\n",
    "            return\n",
    "        \n",
    "        # Extract text from paragraphs and headers\n",
    "        article_text = \"\"\n",
    "        for element in content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text:\n",
    "                article_text += text + \"\\n\\n\"\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"wikipedia_{keyword.replace(' ', '_')}.txt\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(article_text)\n",
    "            \n",
    "        print(f\"‚úÖ  Saved article to: {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå  Error processing {keyword}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"üîç Starting Wikipedia article search for {len(SEARCH_KEYWORDS)} keywords...\")\n",
    "    \n",
    "    for keyword in SEARCH_KEYWORDS:\n",
    "        article_url = search_wikipedia(keyword)\n",
    "        if article_url:\n",
    "            scrape_and_save_article(article_url, keyword)\n",
    "        else:\n",
    "            print(f\"‚ùå  No article found for: {keyword}\")\n",
    "    \n",
    "    print(\"\\n‚ú® Article scraping complete!\")\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 9 documents for chunking\n",
      "Split into 883 chunks\n",
      "\n",
      "‚úÖ Successfully saved chunked data to Chuncked_Data/:\n",
      "  ‚Ä¢ chunked_data_full.json - Complete data with metadata\n",
      "  ‚Ä¢ chunked_data_stats.json - Statistical analysis\n",
      "  ‚Ä¢ chunked_data_content.txt - Human-readable content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_chunk_files():\n",
    "    \"\"\"\n",
    "    Load and chunk files from both data_sources and processed_txt_files directories\n",
    "    \"\"\"\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Process files from both directories\n",
    "    directories = ['data_sources', 'processed_txt_files']\n",
    "    \n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"‚ö†Ô∏è Directory {directory} not found\")\n",
    "            continue\n",
    "            \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                        # Create Document object with metadata\n",
    "                        doc = Document(\n",
    "                            page_content=text,\n",
    "                            metadata={\"source\": file_path}\n",
    "                        )\n",
    "                        all_documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(all_documents)} documents for chunking\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunked_documents = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(chunked_documents)} chunks\")\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "def save_chunked_data(chunked_documents, output_dir=\"Chuncked_Data\"):\n",
    "    \"\"\"\n",
    "    Save chunked documents into three separate files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save full content with metadata\n",
    "    full_content = []\n",
    "    for i, chunk in enumerate(chunked_documents):\n",
    "        full_content.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"content\": chunk.page_content,\n",
    "            \"source\": chunk.metadata.get('source', 'N/A'),\n",
    "            \"length\": len(chunk.page_content)\n",
    "        })\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_full.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_content, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Save statistical analysis\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunked_documents),\n",
    "        \"chunk_lengths\": [len(chunk.page_content) for chunk in chunked_documents],\n",
    "        \"sources\": list(set(chunk.metadata.get('source', 'N/A') for chunk in chunked_documents)),\n",
    "        \"generated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_stats.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # 3. Save content-only version (for easy reading)\n",
    "    with open(os.path.join(output_dir, \"chunked_data_content.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for i, chunk in enumerate(chunked_documents):\n",
    "            f.write(f\"=== Chunk {i+1} ===\\n\")\n",
    "            f.write(f\"Source: {chunk.metadata.get('source', 'N/A')}\\n\")\n",
    "            f.write(f\"Length: {len(chunk.page_content)} characters\\n\")\n",
    "            f.write(\"\\nContent:\\n\")\n",
    "            f.write(chunk.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved chunked data to {output_dir}/:\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_full.json - Complete data with metadata\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_stats.json - Statistical analysis\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_content.txt - Human-readable content\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and chunk the documents\n",
    "    chunked_docs = load_and_chunk_files()\n",
    "    \n",
    "    # Save the chunked data\n",
    "    save_chunked_data(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 883\n",
      "Minimum Chunk Size: 4 characters\n",
      "Maximum Chunk Size: 500 characters\n",
      "Average Chunk Size: 365.26 characters\n",
      "Median Chunk Size: 416.00 characters\n",
      "Standard Deviation of Chunk Size: 125.58\n",
      "\n",
      "--- Source Distribution ---\n",
      "Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt: 344 chunks (39.0%)\n",
      "Bist_Madan.txt: 296 chunks (33.5%)\n",
      "wikipedia_supply_chain.txt: 74 chunks (8.4%)\n",
      "wikipedia_industrial_design.txt: 63 chunks (7.1%)\n",
      "wikipedia_software_architecture.txt: 61 chunks (6.9%)\n",
      "wikipedia_modular_design.txt: 34 chunks (3.9%)\n",
      "wikipedia_system_design.txt: 7 chunks (0.8%)\n",
      "mcp_basics.txt: 2 chunks (0.2%)\n",
      "feasibility_factors.txt: 2 chunks (0.2%)\n",
      "\n",
      "[ADVISORY] Found 47 chunks smaller than 100.0 characters.\n",
      "  > The smallest chunk is 4 characters.\n",
      "  > These small chunks might lack sufficient context and could clutter search results.\n",
      "  > Consider cleaning the source documents or adjusting the chunking separators.\n",
      "\n",
      "--- Sample Chunks Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: wikipedia_software_architecture.txt, Length: 263 chars) ---\n",
      "Software architecture is the set of structures needed to reason about a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relation...\n",
      "\n",
      "--- Chunk 2 (Source: wikipedia_software_architecture.txt, Length: 298 chars) ---\n",
      "The architecture of a software system is a metaphor, analogous to the architecture of a building.[2] It functions as the blueprints for the system and the development project, which project management...\n",
      "\n",
      "--- Chunk 3 (Source: wikipedia_software_architecture.txt, Length: 291 chars) ---\n",
      "Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_chunked_data(json_file='Chuncked_Data/chunked_data_full.json'):\n",
    "    \"\"\"\n",
    "    Load chunked data from JSON file and convert to Document objects.\n",
    "    \n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing chunked data\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            chunked_data = json.load(f)\n",
    "        \n",
    "        # Convert JSON data to Document objects\n",
    "        chunked_documents = [\n",
    "            Document(\n",
    "                page_content=chunk['content'],\n",
    "                metadata={'source': chunk['source']}\n",
    "            ) for chunk in chunked_data\n",
    "        ]\n",
    "        return chunked_documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunked data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_chunked_data(chunked_documents):\n",
    "    \"\"\"\n",
    "    Analyze the chunked documents and provide statistical insights.\n",
    "    \n",
    "    Args:\n",
    "        chunked_documents (list): List of chunked Document objects\n",
    "    \"\"\"\n",
    "    if not chunked_documents:\n",
    "        print(\"‚ö†Ô∏è No chunked documents provided for analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "    \n",
    "    # Calculate the lengths of all chunks\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "    \n",
    "    # Calculate and print key statistics\n",
    "    total_chunks = len(chunk_lengths)\n",
    "    min_size = np.min(chunk_lengths)\n",
    "    max_size = np.max(chunk_lengths)\n",
    "    avg_size = np.mean(chunk_lengths)\n",
    "    std_dev = np.std(chunk_lengths)\n",
    "    median_size = np.median(chunk_lengths)\n",
    "    \n",
    "    print(f\"Total Chunks: {total_chunks}\")\n",
    "    print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "    print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "    print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "    print(f\"Median Chunk Size: {median_size:.2f} characters\")\n",
    "    print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "    \n",
    "    # --- Source Distribution Analysis ---\n",
    "    source_counts = Counter([chunk.metadata.get('source', 'N/A') for chunk in chunked_documents])\n",
    "    print(\"\\n--- Source Distribution ---\")\n",
    "    for source, count in source_counts.most_common():\n",
    "        print(f\"{os.path.basename(source)}: {count} chunks ({count/total_chunks*100:.1f}%)\")\n",
    "    \n",
    "    # --- Automated Quality Feedback ---\n",
    "    CHUNK_SIZE = 500  # Target chunk size\n",
    "    \n",
    "    # 1. Check for high variation in chunk size\n",
    "    if std_dev > 150:\n",
    "        print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "        print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "        print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "    \n",
    "    # 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "    small_chunk_threshold = CHUNK_SIZE * 0.20  # Chunks smaller than 20% of the target size\n",
    "    small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "    \n",
    "    if small_chunk_count > 0:\n",
    "        print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "        print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "        print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "        print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "    \n",
    "    # 3. Check for very large chunks\n",
    "    large_chunk_threshold = CHUNK_SIZE * 1.5  # Chunks larger than 150% of the target size\n",
    "    large_chunk_count = sum(1 for length in chunk_lengths if length > large_chunk_threshold)\n",
    "    \n",
    "    if large_chunk_count > 0:\n",
    "        print(f\"\\n[ADVISORY] Found {large_chunk_count} chunks larger than {large_chunk_threshold} characters.\")\n",
    "        print(f\"  > The largest chunk is {max_size} characters.\")\n",
    "        print(\"  > These large chunks might contain too much information for effective processing.\")\n",
    "    \n",
    "    # Add a success message if no issues are flagged\n",
    "    if std_dev <= 150 and small_chunk_count == 0 and large_chunk_count == 0:\n",
    "        print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "    \n",
    "    # --- Sample Chunks Preview ---\n",
    "    print(\"\\n--- Sample Chunks Preview ---\")\n",
    "    for i, chunk in enumerate(chunked_documents[:3]):  # Print first 3 chunks\n",
    "        chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "        print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "        print(chunk.page_content[:200] + \"...\" if len(chunk.page_content) > 200 else chunk.page_content)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis.\n",
    "    \"\"\"\n",
    "    # Load the chunked data\n",
    "    chunked_documents = load_chunked_data()\n",
    "    \n",
    "    if chunked_documents:\n",
    "        # Run the analysis\n",
    "        analyze_chunked_data(chunked_documents)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Failed to load chunked data. Please check the JSON file path.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VectorDB Analysis Report ===\n",
      "\n",
      "Analysis started at: 2025-06-04 14:07:50\n",
      "\n",
      "\n",
      "=== Setting up Embedding Model ===\n",
      "Using device: cpu for HuggingFaceEmbeddings\n",
      "Sample embedding dimension: 384\n",
      "Embedding model setup complete.\n",
      "\n",
      "=== Loading Chunked Documents ===\n",
      "Successfully loaded 883 documents from Chuncked_Data/chunked_data_full.json\n",
      "\n",
      "=== Creating Vector Database ===\n",
      "FAISS index created successfully.\n",
      "FAISS index saved to: vector_store/pynucleus_mcp_faiss_index\n",
      "\n",
      "Vector Database Statistics:\n",
      "Total documents indexed: 883\n",
      "Embedding dimension: 384\n",
      "\n",
      "=== Running Test Queries ===\n",
      "\n",
      "=== Testing Semantic Search ===\n",
      "Query: What are the advantages of modular chemical plants?\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Similarity Score: 0.5153\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      "Content: To provide a clearer comparative context, Table 1 summarizes the key characteristics, advantages, and disadvantages of modular versus conventional chemical plant construction approaches.\n",
      "\n",
      "Table 1: Com...\n",
      "\n",
      "--- Document 2 ---\n",
      "Similarity Score: 0.5176\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      "3.1.\tFundamentals of Modular Chemical Plants\n",
      "\n",
      "=== Testing Semantic Search ===\n",
      "Query: How does modular design improve scalability?\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Similarity Score: 0.4992\n",
      "Score Explanation: Low similarity - Content may not be very relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: Modularity offers benefits such as reduction in cost (customization can be limited to a portion of the system, rather than needing an overhaul of the entire system), interoperability, shorter learning...\n",
      "\n",
      "--- Document 2 ---\n",
      "Similarity Score: 0.5382\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: . If modularity is properly defined and conceived in the design strategy, modular systems can create significant competitive advantage in markets.  A true modular system does not need to rely on produ...\n",
      "\n",
      "=== Testing Semantic Search ===\n",
      "Query: What are the key factors in system design?\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Similarity Score: 0.6464\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: data_sources/wikipedia_system_design.txt\n",
      "Content: The basic study of system design is the understanding of component parts and their subsequent interaction with one another.[1]\n",
      "\n",
      "Systems design has appeared in a variety of fields, including sustainabi...\n",
      "\n",
      "--- Document 2 ---\n",
      "Similarity Score: 0.7415\n",
      "Score Explanation: Good similarity - Content is relevant\n",
      "Source: data_sources/wikipedia_system_design.txt\n",
      "Content: Thus in product development, systems design involves the process of defining and developing systems, such as interfaces and data, for an electronic control system to satisfy specified requirements. Sy...\n",
      "\n",
      "=== Testing Semantic Search ===\n",
      "Query: Explain the concept of modular design in manufacturing\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Similarity Score: 0.4121\n",
      "Score Explanation: Low similarity - Content may not be very relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: Modular design, or modularity in design, is a design principle that subdivides a system into smaller parts called modules (such as modular process skids), which can be independently created, modified,...\n",
      "\n",
      "--- Document 2 ---\n",
      "Similarity Score: 0.5627\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: In computer hardware\n",
      "\n",
      "Modular design in computer hardware is the same as in other things (e.g. cars, refrigerators, and furniture). The idea is to build computers with easily replaceable parts that us...\n",
      "\n",
      "=== Testing Semantic Search ===\n",
      "Query: What are the challenges in implementing modular systems?\n",
      "\n",
      "Retrieved 2 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Similarity Score: 0.5393\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: Modularity offers benefits such as reduction in cost (customization can be limited to a portion of the system, rather than needing an overhaul of the entire system), interoperability, shorter learning...\n",
      "\n",
      "--- Document 2 ---\n",
      "Similarity Score: 0.6452\n",
      "Score Explanation: Moderate similarity - Content is somewhat relevant\n",
      "Source: data_sources/wikipedia_modular_design.txt\n",
      "Content: A modular design can be characterized by functional partitioning into discrete scalable and reusable modules, rigorous use of well-defined modular interfaces, and making use of industry standards for ...\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Full analysis report saved to: vectordb_outputs/vectordb_analysis_20250604_140750.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class VectorDBManager:\n",
    "    def __init__(self, vector_store_dir=\"vector_store\", output_dir=\"vectordb_outputs\"):\n",
    "        \"\"\"\n",
    "        Initialize the VectorDB Manager.\n",
    "        \n",
    "        Args:\n",
    "            vector_store_dir (str): Directory to store vector database files\n",
    "            output_dir (str): Directory to store output logs and analysis\n",
    "        \"\"\"\n",
    "        self.vector_store_dir = vector_store_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.embeddings = None\n",
    "        self.vector_db = None\n",
    "        self.faiss_index_path = os.path.join(vector_store_dir, 'pynucleus_mcp_faiss_index')\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(vector_store_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize output file\n",
    "        self.output_file = os.path.join(output_dir, f'vectordb_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
    "        self._write_to_output(\"=== VectorDB Analysis Report ===\\n\")\n",
    "        self._write_to_output(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    def _write_to_output(self, text, print_to_console=True):\n",
    "        \"\"\"\n",
    "        Write text to output file and optionally print to console.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to write\n",
    "            print_to_console (bool): Whether to also print to console\n",
    "        \"\"\"\n",
    "        with open(self.output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\")\n",
    "        if print_to_console:\n",
    "            print(text)\n",
    "\n",
    "    def _explain_score(self, score):\n",
    "        \"\"\"\n",
    "        Explain the meaning of a similarity score.\n",
    "        \n",
    "        Args:\n",
    "            score (float): Similarity score\n",
    "            \n",
    "        Returns:\n",
    "            str: Explanation of the score\n",
    "        \"\"\"\n",
    "        if score < 0.5:\n",
    "            return \"Low similarity - Content may not be very relevant\"\n",
    "        elif score < 0.7:\n",
    "            return \"Moderate similarity - Content is somewhat relevant\"\n",
    "        elif score < 0.85:\n",
    "            return \"Good similarity - Content is relevant\"\n",
    "        else:\n",
    "            return \"High similarity - Content is very relevant\"\n",
    "\n",
    "    def setup_embeddings(self):\n",
    "        \"\"\"\n",
    "        Set up the embedding model using sentence-transformers.\n",
    "        \"\"\"\n",
    "        self._write_to_output(\"\\n=== Setting up Embedding Model ===\")\n",
    "        \n",
    "        # Determine device (GPU/CPU)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self._write_to_output(f\"Using device: {device} for HuggingFaceEmbeddings\")\n",
    "\n",
    "        # Configure model parameters\n",
    "        model_kwargs = {'device': device}\n",
    "        encode_kwargs = {'normalize_embeddings': True}  # Important for cosine similarity\n",
    "\n",
    "        # Initialize the embedding model\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "\n",
    "        # Test the embedding model\n",
    "        sample_text = \"Modular Chemical Plants offer faster deployment.\"\n",
    "        sample_embedding = self.embeddings.embed_query(sample_text)\n",
    "        self._write_to_output(f\"Sample embedding dimension: {len(sample_embedding)}\")\n",
    "        self._write_to_output(\"Embedding model setup complete.\")\n",
    "        \n",
    "        return self.embeddings\n",
    "\n",
    "    def load_chunked_documents(self, json_file='Chuncked_Data/chunked_data_full.json'):\n",
    "        \"\"\"\n",
    "        Load chunked documents from JSON file.\n",
    "        \n",
    "        Args:\n",
    "            json_file (str): Path to the JSON file containing chunked data\n",
    "            \n",
    "        Returns:\n",
    "            list: List of Document objects\n",
    "        \"\"\"\n",
    "        self._write_to_output(\"\\n=== Loading Chunked Documents ===\")\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                chunked_data = json.load(f)\n",
    "            \n",
    "            # Convert JSON data to Document objects\n",
    "            chunked_documents = [\n",
    "                Document(\n",
    "                    page_content=chunk['content'],\n",
    "                    metadata={'source': chunk['source']}\n",
    "                ) for chunk in chunked_data\n",
    "            ]\n",
    "            self._write_to_output(f\"Successfully loaded {len(chunked_documents)} documents from {json_file}\")\n",
    "            return chunked_documents\n",
    "        except Exception as e:\n",
    "            self._write_to_output(f\"Error loading chunked data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def create_vector_db(self, chunked_documents):\n",
    "        \"\"\"\n",
    "        Create FAISS vector database from chunked documents.\n",
    "        \n",
    "        Args:\n",
    "            chunked_documents (list): List of Document objects\n",
    "        \"\"\"\n",
    "        self._write_to_output(\"\\n=== Creating Vector Database ===\")\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            self._write_to_output(\"Embeddings not initialized. Setting up embeddings first...\")\n",
    "            self.setup_embeddings()\n",
    "        \n",
    "        try:\n",
    "            self.vector_db = FAISS.from_documents(chunked_documents, self.embeddings)\n",
    "            self._write_to_output(\"FAISS index created successfully.\")\n",
    "            \n",
    "            # Save the FAISS index\n",
    "            self.vector_db.save_local(self.faiss_index_path)\n",
    "            self._write_to_output(f\"FAISS index saved to: {self.faiss_index_path}\")\n",
    "            \n",
    "            # Add statistics about the vector database\n",
    "            self._write_to_output(\"\\nVector Database Statistics:\")\n",
    "            self._write_to_output(f\"Total documents indexed: {len(chunked_documents)}\")\n",
    "            self._write_to_output(f\"Embedding dimension: {len(self.embeddings.embed_query('test'))}\")\n",
    "            \n",
    "            return self.vector_db\n",
    "        except Exception as e:\n",
    "            self._write_to_output(f\"Error creating FAISS index: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def load_vector_db(self):\n",
    "        \"\"\"\n",
    "        Load existing FAISS vector database.\n",
    "        \"\"\"\n",
    "        self._write_to_output(\"\\n=== Loading Vector Database ===\")\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            self._write_to_output(\"Embeddings not initialized. Setting up embeddings first...\")\n",
    "            self.setup_embeddings()\n",
    "        \n",
    "        try:\n",
    "            self.vector_db = FAISS.load_local(\n",
    "                self.faiss_index_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            self._write_to_output(\"FAISS index loaded successfully.\")\n",
    "            return self.vector_db\n",
    "        except Exception as e:\n",
    "            self._write_to_output(f\"Error loading FAISS index: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def test_semantic_search(self, query, k=2):\n",
    "        \"\"\"\n",
    "        Test semantic search functionality.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            k (int): Number of results to return\n",
    "        \"\"\"\n",
    "        self._write_to_output(f\"\\n=== Testing Semantic Search ===\")\n",
    "        self._write_to_output(f\"Query: {query}\")\n",
    "        \n",
    "        if not self.vector_db:\n",
    "            self._write_to_output(\"VectorDB not available. Loading or creating it first...\")\n",
    "            if not self.load_vector_db():\n",
    "                self._write_to_output(\"Failed to load VectorDB. Please create it first.\")\n",
    "                return None\n",
    "        \n",
    "        try:\n",
    "            retrieved_docs_with_scores = self.vector_db.similarity_search_with_score(query, k=k)\n",
    "            \n",
    "            if retrieved_docs_with_scores:\n",
    "                self._write_to_output(f\"\\nRetrieved {len(retrieved_docs_with_scores)} documents:\")\n",
    "                for i, (doc, score) in enumerate(retrieved_docs_with_scores):\n",
    "                    score_explanation = self._explain_score(score)\n",
    "                    self._write_to_output(f\"\\n--- Document {i+1} ---\")\n",
    "                    self._write_to_output(f\"Similarity Score: {score:.4f}\")\n",
    "                    self._write_to_output(f\"Score Explanation: {score_explanation}\")\n",
    "                    self._write_to_output(f\"Source: {doc.metadata.get('source')}\")\n",
    "                    self._write_to_output(f\"Content: {doc.page_content[:200]}...\" if len(doc.page_content) > 200 else doc.page_content)\n",
    "            else:\n",
    "                self._write_to_output(\"No documents retrieved for the query.\")\n",
    "            \n",
    "            return retrieved_docs_with_scores\n",
    "        except Exception as e:\n",
    "            self._write_to_output(f\"Error during similarity search: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate VectorDB setup and testing.\n",
    "    \"\"\"\n",
    "    # Initialize VectorDB Manager\n",
    "    vdb_manager = VectorDBManager()\n",
    "    \n",
    "    # Set up embeddings\n",
    "    vdb_manager.setup_embeddings()\n",
    "    \n",
    "    # Load chunked documents\n",
    "    chunked_docs = vdb_manager.load_chunked_documents()\n",
    "    \n",
    "    if chunked_docs:\n",
    "        # Create vector database\n",
    "        vdb_manager.create_vector_db(chunked_docs)\n",
    "        \n",
    "        # Test semantic search\n",
    "        test_queries = [\n",
    "            \"What are the advantages of modular chemical plants?\",\n",
    "            \"How does modular design improve scalability?\",\n",
    "            \"What are the key factors in system design?\",\n",
    "            \"Explain the concept of modular design in manufacturing\",\n",
    "            \"What are the challenges in implementing modular systems?\"\n",
    "        ]\n",
    "        \n",
    "        vdb_manager._write_to_output(\"\\n=== Running Test Queries ===\")\n",
    "        for query in test_queries:\n",
    "            vdb_manager.test_semantic_search(query)\n",
    "        \n",
    "        vdb_manager._write_to_output(\"\\n=== Analysis Complete ===\")\n",
    "        vdb_manager._write_to_output(f\"Full analysis report saved to: {vdb_manager.output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9d3d6d6] Update: Adding all files to repository\n",
      " 2 files changed, 4 insertions(+), 1 deletion(-)\n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 497 bytes | 497.00 KiB/s, done.\n",
      "Total 5 (delta 4), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   9ef67a4..9d3d6d6  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
