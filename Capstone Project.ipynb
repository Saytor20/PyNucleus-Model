{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:21.155052Z",
     "start_time": "2025-06-04T16:38:20.796518Z"
    }
   },
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:27.868100Z",
     "start_time": "2025-06-04T16:38:27.035975Z"
    }
   },
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"ðŸ“‚ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"â„¹ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- ðŸ“„ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Skip hidden files like .DS_Store\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "        print(f\" â–¶ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Handle PDF files differently\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                # Use PyPDF2 for PDF files\n",
    "                reader = PdfReader(input_path)\n",
    "                full_text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\\n\"\n",
    "            else:\n",
    "                # Use UnstructuredLoader for other file types\n",
    "                loader = UnstructuredLoader(input_path)\n",
    "                documents = loader.load()\n",
    "                full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"   â€¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   â€¢ Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n\\n All files processed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“„ Starting processing for 4 file(s) in 'source_documents' ---\n",
      " â–¶ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " â–¶ Processing: mcp_basics.txt\n",
      "   â€¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      " â–¶ Processing: feasibility_factors.txt\n",
      "   â€¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      " â–¶ Processing: Bist_Madan.pdf\n",
      "   â€¢ Success! Saved to: processed_txt_files/Bist_Madan.txt\n",
      "\n",
      "\n",
      " All files processed.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "# test_crawl_modular_plants.py\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Reads any existing articles.csv to avoid re-scraping\n",
    "# 2. Asks how many NEW articles you want\n",
    "# 3. Google-searches each keyword (â‰¤ TEST_RESULTS_PER_KW hits)\n",
    "# 4. Downloads each candidate (HTML or PDF) â†’ extracts text\n",
    "# 5. Saves rows (title, url, text) to articles.csv\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "import io, time, pathlib, requests, pandas as pd\n",
    "from typing import List\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "import trafilatura\n",
    "from pdfminer.high_level import extract_text as pdf_text\n",
    "from googlesearch import search                         # works with both forks\n",
    "\n",
    "# â”€â”€ CONFIG (testing-mode: small numbers) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "KEYWORDS               = [\n",
    "    \"modular chemical plants cost analysis 2024\",\n",
    "    \"MCP feasibility study\",\n",
    "    \"pre-fabricated chemical modules logistics\",\n",
    "]\n",
    "TEST_RESULTS_PER_KW    = 10      # small batch for testing\n",
    "MIN_LENGTH             = 50      # keep short so we see something fast\n",
    "PAUSE_BETWEEN_DL       = 1       # polite download delay (sec)\n",
    "\n",
    "OUTPUT_DIR   = pathlib.Path(\"data_sources\")\n",
    "OUTPUT_FILE  = OUTPUT_DIR / \"articles.csv\"\n",
    "HEADERS      = {\"User-Agent\": \"Mozilla/5.0 (testing-crawler)\"}\n",
    "TIMEOUT      = 15   # seconds\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€ STEP 1: load existing URLs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n[1] Loading existing article list â€¦\")\n",
    "if OUTPUT_FILE.exists():\n",
    "    seen_urls = set(pd.read_csv(OUTPUT_FILE, usecols=[\"url\"])[\"url\"])\n",
    "    print(f\"    â†’ {len(seen_urls):,} URLs already stored â€“ will skip.\")\n",
    "else:\n",
    "    seen_urls = set()\n",
    "    print(\"    â†’ No previous CSV â€“ starting fresh.\")\n",
    "\n",
    "# â”€â”€ Ask user for target count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    target = int(input(\"\\nHow many NEW articles to fetch (testing)? \").strip())\n",
    "except ValueError:\n",
    "    target = 0\n",
    "if target <= 0:\n",
    "    print(\"Nothing requested â€“ exiting.\")\n",
    "    exit(0)\n",
    "\n",
    "# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def google_search(q: str, limit: int) -> List[str]:\n",
    "    \"\"\"Return up to `limit` URLs using whichever googlesearch signature works.\"\"\"\n",
    "    try:\n",
    "        hits = search(q, num_results=limit, lang=\"en\")\n",
    "    except TypeError:\n",
    "        hits = search(q, lang=\"en\", num=limit, stop=limit)\n",
    "    clean = [\n",
    "        u for u in hits\n",
    "        if u.startswith(\"http\")\n",
    "        and not any(bad in u for bad in (\"google.\", \"/search?\", \"facebook.com\"))\n",
    "    ]\n",
    "    return clean[:limit]\n",
    "\n",
    "def extract_article(url: str, resp: requests.Response) -> str:\n",
    "    ctype = resp.headers.get(\"content-type\", \"\").lower()\n",
    "    if \"pdf\" in ctype or url.lower().endswith(\".pdf\"):\n",
    "        return pdf_text(io.BytesIO(resp.content))\n",
    "    # ensure a reasonable encoding\n",
    "    if resp.encoding is None:\n",
    "        resp.encoding = resp.apparent_encoding\n",
    "    return trafilatura.extract(resp.text) or \"\"\n",
    "\n",
    "# â”€â”€ STEP 2: search & scrape â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "new_rows = []\n",
    "processed = set()\n",
    "print(\"\\n[2] Searching & scraping â€¦\")\n",
    "\n",
    "for kw in KEYWORDS:\n",
    "    if len(new_rows) >= target:\n",
    "        break\n",
    "    print(f\"  â€¢ Keyword: {kw!r}\")\n",
    "\n",
    "    for url in google_search(kw, TEST_RESULTS_PER_KW):\n",
    "        if len(new_rows) >= target:\n",
    "            break\n",
    "        if url in seen_urls or url in processed:\n",
    "            continue\n",
    "\n",
    "        print(f\"    â†’ Fetching: {url}\")\n",
    "        processed.add(url)\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            text = extract_article(url, r)\n",
    "\n",
    "            if len(text) < MIN_LENGTH:\n",
    "                print(\"      Skipped â€“ too short.\")\n",
    "                continue\n",
    "\n",
    "            meta = trafilatura.extract_metadata(r.text) if \"text\" in dir(r) else None\n",
    "            title = (meta.title if meta and meta.title else url.split(\"/\")[-1])[:120]\n",
    "            new_rows.append({\"title\": title, \"url\": url, \"text\": text})\n",
    "            print(f\"      âœ” saved ({len(text):,} chars)\")\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print(f\"      HTTP error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      Fail: {e}\")\n",
    "\n",
    "        time.sleep(PAUSE_BETWEEN_DL)\n",
    "\n",
    "# â”€â”€ STEP 3: save CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n[3] Saving â€¦\")\n",
    "if new_rows:\n",
    "    df_new = pd.DataFrame(new_rows)\n",
    "    if OUTPUT_FILE.exists():\n",
    "        df_new.to_csv(OUTPUT_FILE, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        df_new.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"    Added {len(df_new)} new rows. CSV now has \"\n",
    "          f\"{len(seen_urls) + len(df_new):,} total.\")\n",
    "else:\n",
    "    print(\"    No new qualifying articles extracted.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 4\n",
      "Minimum Chunk Size: 126 characters\n",
      "Maximum Chunk Size: 472 characters\n",
      "Average Chunk Size: 322.00 characters\n",
      "Standard Deviation of Chunk Size: 143.16\n",
      "\n",
      "[INFO] Chunking statistics appear healthy. Sizes are consistent.\n",
      "\n",
      "--- Sample Chunk Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: mcp_basics.txt, Length: 472 chars) ---\n",
      "Modular Chemical Plants (MCPs) represent a paradigm shift in chemical process engineering. They involve constructing plants from standardized, pre-fabricated modules built off-site. This approach significantly reduces on-site construction time and costs compared to traditional stick-built plants. Key advantages include faster deployment, scalability, and potentially lower capital expenditure. However, module transportation and site integration require careful planning\n",
      "\n",
      "--- Chunk 2 (Source: mcp_basics.txt, Length: 126 chars) ---\n",
      ". MCPs are particularly suited for remote locations or projects with uncertain market demands, allowing for phased investment.\n",
      "\n",
      "--- Chunk 3 (Source: feasibility_factors.txt, Length: 445 chars) ---\n",
      "Evaluating the feasibility of an MCP involves assessing technical, economic, and logistical aspects. Feedstock availability and cost are primary drivers for many chemical processes. Market demand for the plant's output and projected pricing influence revenue streams. Capital costs for module fabrication, transportation, and site preparation, along with operating expenses like utilities, labor, and maintenance, determine overall profitability\n",
      "\n",
      "\n",
      "Data ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Inspect the Results ---\n",
    "import numpy as np\n",
    "# --- NEW: Statistical Analysis of Chunks ---\n",
    "print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "\n",
    "# Calculate the lengths of all chunks\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "\n",
    "# Calculate and print key statistics\n",
    "total_chunks = len(chunk_lengths)\n",
    "min_size = np.min(chunk_lengths)\n",
    "max_size = np.max(chunk_lengths)\n",
    "avg_size = np.mean(chunk_lengths)\n",
    "std_dev = np.std(chunk_lengths)\n",
    "\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "\n",
    "# --- Automated Quality Feedback ---\n",
    "\n",
    "# 1. Check for high variation in chunk size\n",
    "# A high standard deviation suggests inconsistent chunking.\n",
    "if std_dev > 150:\n",
    "    print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "    print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "    print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "\n",
    "# 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "small_chunk_threshold = CHUNK_SIZE * 0.20 # Chunks smaller than 20% of the target size\n",
    "small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "\n",
    "if small_chunk_count > 0:\n",
    "    # This check is more specific than just looking at the absolute minimum.\n",
    "    print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "    print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "    print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "    print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "\n",
    "# Add a success message if no issues are flagged\n",
    "if std_dev <= 150 and small_chunk_count == 0:\n",
    "    print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "\n",
    "\n",
    "# --- Manual Inspection of Sample Chunks ---\n",
    "# (This part remains the same)\n",
    "print(\"\\n--- Sample Chunk Preview ---\")\n",
    "# Print the first few chunks to get a feel for their content and structure\n",
    "for i, chunk in enumerate(chunked_documents[:3]): # Print first 3 chunks\n",
    "    chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "\n",
    "print(\"\\n\\nData ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169",
    "ExecuteTime": {
     "end_time": "2025-06-04T16:38:14.365619Z",
     "start_time": "2025-06-04T16:38:14.334009Z"
    }
   },
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"Session ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Log end time\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate_log.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 3\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSession ended at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mdatetime\u001B[49m\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Simple GitHub update function\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate_github\u001B[39m():\n",
      "\u001B[0;31mNameError\u001B[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
