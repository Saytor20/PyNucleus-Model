{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#--------Google Drive Integration--------#\n",
    "# from google.colab import drive, userdata\n",
    "# This gives Colab access to your files in Google Drive.\n",
    "# drive.mount('/content/drive')\n",
    "# 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "\n",
    "\n",
    "#--------Cursor Integration--------#\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "\n",
    "# Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“„ Starting processing for 4 file(s) in 'source_documents' ---\n",
      "  â–¶ Processing: 1-s2.0-S0925527302003742-main.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€¢ âœ… Success! Saved to: processed_txt_files/1-s2.0-S0925527302003742-main.txt\n",
      "  â–¶ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€¢ âœ… Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      "  â–¶ Processing: mcp_basics.txt\n",
      "    â€¢ âœ… Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      "  â–¶ Processing: feasibility_factors.txt\n",
      "    â€¢ âœ… Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      "\n",
      "\n",
      "âœ… All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"ðŸ“‚ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"â„¹ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- ðŸ“„ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        \n",
    "        # Create a clean output filename by changing the extension to .txt\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "        \n",
    "        print(f\"  â–¶ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # The UnstructuredFileLoader automatically handles different file types.\n",
    "            # The \"paged\" mode is faster and works for most text-based files.\n",
    "            # It can be set to \"elements\" or \"ocr\" for more complex, scanned documents.\n",
    "            loader = UnstructuredFileLoader(input_path, mode=\"paged\")\n",
    "            \n",
    "            # The .load() method does all the work of extracting the text\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Combine the page content into a single block of text\n",
    "            full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "                \n",
    "            print(f\"    â€¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    â€¢ Error processing {filename}: {e}\")\n",
    "            \n",
    "    print(\"\\n\\nâœ… All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (4197401654.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    urls, print(f\"âŒ Search error: {e}\") = [], None\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "# FILE: scrape_web.py\n",
    "\n",
    "import os\n",
    "from googlesearch import search\n",
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "# --- Configuration ---\n",
    "KEYWORDS = [\"modular chemical plants cost analysis 2024\", \"MCPs feasibility study\"]\n",
    "NUM_RESULTS_TO_SCRAPE = 5\n",
    "OUTPUT_DIR = 'data_sources'\n",
    "OUTPUT_FILENAME = \"processed_web_content.txt\"\n",
    "FINAL_OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- ðŸŒ Starting Web Scraping ---\")\n",
    "    query = \" \".join(KEYWORDS)\n",
    "    try:\n",
    "        urls = list(search(query, num_results=NUM_RESULTS_TO_SCRAPE, lang=\"en\"))\n",
    "        print(f\"ðŸ” Found {len(urls)} URLs for '{query}'.\")\n",
    "    except Exception as e:\n",
    "        urls, print(f\"âŒ Search error: {e}\") = [], None\n",
    "\n",
    "    final_content = []\n",
    "    for url in urls:\n",
    "        print(f\"  â–¶ Scraping: {url}\")\n",
    "        raw = fetch_url(url)\n",
    "        if raw and (text := extract(raw, include_comments=False)):\n",
    "            final_content.append(f\"--- Content from URL: {url} ---\\n{text}\\n\\n\")\n",
    "            print(\"    â€¢ âœ… Extracted main text\")\n",
    "        else:\n",
    "            print(\"    â€¢ âš ï¸ No main text found or download failed\")\n",
    "\n",
    "    if final_content:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        with open(FINAL_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\".join(final_content))\n",
    "        print(f\"\\nâœ… Saved web content to '{FINAL_OUTPUT_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 4\n",
      "Minimum Chunk Size: 126 characters\n",
      "Maximum Chunk Size: 472 characters\n",
      "Average Chunk Size: 322.00 characters\n",
      "Standard Deviation of Chunk Size: 143.16\n",
      "\n",
      "[INFO] Chunking statistics appear healthy. Sizes are consistent.\n",
      "\n",
      "--- Sample Chunk Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: mcp_basics.txt, Length: 472 chars) ---\n",
      "Modular Chemical Plants (MCPs) represent a paradigm shift in chemical process engineering. They involve constructing plants from standardized, pre-fabricated modules built off-site. This approach significantly reduces on-site construction time and costs compared to traditional stick-built plants. Key advantages include faster deployment, scalability, and potentially lower capital expenditure. However, module transportation and site integration require careful planning\n",
      "\n",
      "--- Chunk 2 (Source: mcp_basics.txt, Length: 126 chars) ---\n",
      ". MCPs are particularly suited for remote locations or projects with uncertain market demands, allowing for phased investment.\n",
      "\n",
      "--- Chunk 3 (Source: feasibility_factors.txt, Length: 445 chars) ---\n",
      "Evaluating the feasibility of an MCP involves assessing technical, economic, and logistical aspects. Feedstock availability and cost are primary drivers for many chemical processes. Market demand for the plant's output and projected pricing influence revenue streams. Capital costs for module fabrication, transportation, and site preparation, along with operating expenses like utilities, labor, and maintenance, determine overall profitability\n",
      "\n",
      "\n",
      "Data ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Inspect the Results ---\n",
    "import numpy as np\n",
    "# --- NEW: Statistical Analysis of Chunks ---\n",
    "print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "\n",
    "# Calculate the lengths of all chunks\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "\n",
    "# Calculate and print key statistics\n",
    "total_chunks = len(chunk_lengths)\n",
    "min_size = np.min(chunk_lengths)\n",
    "max_size = np.max(chunk_lengths)\n",
    "avg_size = np.mean(chunk_lengths)\n",
    "std_dev = np.std(chunk_lengths)\n",
    "\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "\n",
    "# --- Automated Quality Feedback ---\n",
    "\n",
    "# 1. Check for high variation in chunk size\n",
    "# A high standard deviation suggests inconsistent chunking.\n",
    "if std_dev > 150:\n",
    "    print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "    print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "    print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "\n",
    "# 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "small_chunk_threshold = CHUNK_SIZE * 0.20 # Chunks smaller than 20% of the target size\n",
    "small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "\n",
    "if small_chunk_count > 0:\n",
    "    # This check is more specific than just looking at the absolute minimum.\n",
    "    print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "    print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "    print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "    print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "\n",
    "# Add a success message if no issues are flagged\n",
    "if std_dev <= 150 and small_chunk_count == 0:\n",
    "    print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "\n",
    "\n",
    "# --- Manual Inspection of Sample Chunks ---\n",
    "# (This part remains the same)\n",
    "print(\"\\n--- Sample Chunk Preview ---\")\n",
    "# Print the first few chunks to get a feel for their content and structure\n",
    "for i, chunk in enumerate(chunked_documents[:3]): # Print first 3 chunks\n",
    "    chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "\n",
    "print(\"\\n\\nData ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL-SmhSQQ_21"
   },
   "source": [
    "# **Data Scrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748965316233,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "I69992m0RE82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Found 5 URLs for 'modular chemical plants MCPs feasibility pre-fabricated chemical modules'.\n",
      "  â–¶ Scraping: /search?num=7\n",
      "    â€¢ âŒ Download failed\n",
      "  â–¶ Scraping: https://www.hm-ec.com/blog-posts/modular-process-plants-hm\n",
      "    â€¢ âœ… Extracted main text\n",
      "  â–¶ Scraping: https://www.tce.co.in/blogs/Modularization%20of%20chemical%20plants.pdf\n",
      "    â€¢ âš ï¸ No main text found\n",
      "  â–¶ Scraping: https://www.hm-ec.com/blog-posts/modular-plant-construction-hm\n",
      "    â€¢ âœ… Extracted main text\n",
      "  â–¶ Scraping: https://dechema.de/Cost_Engineering_for_Modular_Plants/_/2024.11.14%20Cost%20Engineering%20for%20Modular%20Plants%20FINAL.pdf\n",
      "    â€¢ âš ï¸ No main text found\n",
      "\n",
      "âœ… Saved 2 pages to 'data_sources/web_scraped_mcp_data.txt'.\n",
      "ðŸ“Š Key Metrics:\n",
      "   â€¢ URLs Found    : 5\n",
      "   â€¢ Successful    : 2\n",
      "   â€¢ Failed        : 3\n",
      "   â€¢ Total Chars   : 13192\n",
      "   â€¢ Total Words   : 1822\n"
     ]
    }
   ],
   "source": [
    "# scrape_web.py\n",
    "import os\n",
    "from googlesearch import search\n",
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "# --- Configuration ---\n",
    "KEYWORDS = [\"modular chemical plants cost analysis 2024\", \"MCPs feasibility study\", \"pre-fabricated chemical modules logistics\"]\n",
    "NUM_RESULTS_TO_SCRAPE = 5\n",
    "OUTPUT_DIR = 'data_sources'\n",
    "WEB_OUTPUT_FILENAME = \"web_scraped_content.txt\"\n",
    "FINAL_OUTPUT_PATH = os.path.join(OUTPUT_DIR, WEB_OUTPUT_FILENAME)\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- ðŸŒ Starting Web Scraping ---\")\n",
    "    query = \" \".join(KEYWORDS)\n",
    "    try:\n",
    "        urls = list(search(query, num_results=NUM_RESULTS_TO_SCRAPE, lang=\"en\"))\n",
    "        print(f\"ðŸ” Found {len(urls)} URLs for '{query}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Search error: {e}\")\n",
    "        urls = []\n",
    "\n",
    "    final_content_chunks = []\n",
    "    if urls:\n",
    "        for url in urls:\n",
    "            print(f\"  â–¶ Scraping: {url}\")\n",
    "            raw = fetch_url(url)\n",
    "            if not raw:\n",
    "                print(\"    â€¢ âŒ Download failed\")\n",
    "                continue\n",
    "            text = extract(raw, include_comments=False, include_tables=False)\n",
    "            if text:\n",
    "                header = f\"--- Content from URL: {url} ---\\n\"\n",
    "                final_content_chunks.append(header + text + \"\\n\\n\")\n",
    "                print(\"    â€¢ âœ… Extracted main text\")\n",
    "            else:\n",
    "                print(\"    â€¢ âš ï¸ No main text found\")\n",
    "\n",
    "    if final_content_chunks:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        with open(FINAL_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\".join(final_content_chunks))\n",
    "        print(f\"\\nâœ… Success! Scraped content from {len(final_content_chunks)} URL(s) and saved to '{FINAL_OUTPUT_PATH}'.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ No content was scraped from the web.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main e5e9d56] Update: Adding all files to repository\n",
      " 1 file changed, 46 insertions(+)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 4.48 KiB | 4.48 MiB/s, done.\n",
      "Total 4 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   db1e894..e5e9d56  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
