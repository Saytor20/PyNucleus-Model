{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime\n",
    "# from dotenv import load_dotenv\n",
    "# #\n",
    "# # #--------Google Drive Integration--------#\n",
    "# # # from google.colab import drive, userdata\n",
    "# # # This gives Colab access to your files in Google Drive.\n",
    "# # # drive.mount('/content/drive')\n",
    "# # # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# # GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# # GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# # REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# # NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "# #\n",
    "# #\n",
    "# # #--------Cursor Integration--------#\n",
    "# # # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# #\n",
    "# # # Get GitHub credentials from environment variables\n",
    "# GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "# #\n",
    "# # # Print to verify the variables are loaded (remove this in production)\n",
    "# print(f\"Username: {GITHUB_USERNAME}\")\n",
    "# print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "# #\n",
    "# # Repository information\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "# NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "# LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# # Pull latest changes from GitHub\n",
    "# print(\"Pulling latest changes from GitHub...\")\n",
    "# !git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "# print(\"Repository is up to date!\")\n",
    "\n",
    "# # Log start time\n",
    "# with open(\"update_log.txt\", \"a\") as f:\n",
    "#     f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing imports...\n",
      "‚úÖ scrape_wikipedia_article imported successfully\n",
      "üöÄ All imports ready!\n",
      "\n",
      "Step 1: Processing source documents...\n",
      "--- üìÑ Starting processing for 5 file(s) in 'source_documents' ---\n",
      " ‚ñ∂ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Success! Saved to: converted_to_txt/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " ‚ñ∂ Processing: mcp_basics.txt\n",
      "   ‚Ä¢ Success! Saved to: converted_to_txt/mcp_basics.txt\n",
      " ‚ñ∂ Processing: feasibility_factors.txt\n",
      "   ‚Ä¢ Success! Saved to: converted_to_txt/feasibility_factors.txt\n",
      " ‚ñ∂ Processing: Bist_Madan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Success! Saved to: converted_to_txt/Bist_Madan.txt\n",
      " ‚ñ∂ Processing: sample_document.txt\n",
      "   ‚Ä¢ Success! Saved to: converted_to_txt/sample_document.txt\n",
      "\n",
      " All files processed.\n",
      "\n",
      "Step 2: Scraping Wikipedia articles...\n",
      "üîç Starting Wikipedia article search for 5 keywords...\n",
      "‚ñ∂Ô∏è  Searching for: modular design\n",
      "‚úÖ  Saved article to: web_sources/wikipedia_modular_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: software architecture\n",
      "‚úÖ  Saved article to: web_sources/wikipedia_software_architecture.txt\n",
      "‚ñ∂Ô∏è  Searching for: system design\n",
      "‚úÖ  Saved article to: web_sources/wikipedia_system_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: industrial design\n",
      "‚úÖ  Saved article to: web_sources/wikipedia_industrial_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: supply chain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Saved article to: web_sources/wikipedia_supply_chain.txt\n",
      "\n",
      "‚ú® Article scraping complete!\n",
      "\n",
      "Step 3: Processing and chunking documents...\n",
      "üì∞ Found 5 Wikipedia articles\n",
      "üìÑ Found 5 converted documents\n",
      "üìã Total documents loaded: 10\n",
      "‚úÇÔ∏è Split into 867 chunks\n",
      "\n",
      "‚úÖ Successfully saved chunked data to converted_chunked_data/:\n",
      "  ‚Ä¢ chunked_data_full.json - Complete data with metadata\n",
      "  ‚Ä¢ chunked_data_stats.json - Statistical analysis\n",
      "  ‚Ä¢ chunked_data_content.txt - Human-readable content\n",
      "\n",
      "\n",
      "Step 4: Building and evaluating FAISS vector store...\n",
      "=== FAISS VectorDB Analysis ===\n",
      "Started: 2025-06-10 01:25:18\n",
      "Loaded 867 documents from converted_chunked_data/chunked_data_full.json\n",
      "Embedding device ‚Üí cpu   | dim=384\n",
      "Docs indexed : 867\n",
      "Index file   : vector_db/pynucleus_mcp.faiss\n",
      "Embeds .pkl  : vector_db/embeddings.pkl\n",
      "\n",
      "-- Files in vector_db/ --\n",
      "  ¬∑ embeddings.pkl\n",
      "  ¬∑ pynucleus_mcp.faiss\n",
      "\n",
      "=== Evaluation (Recall@3) ===\n",
      "Q: what are the benefits of modular design  ‚úì   top-score=0.4110\n",
      "Q: how does modular design work in vehicles ‚úì   top-score=0.3477\n",
      "\n",
      "Recall@3: 2/2  ‚Üí  100.0%\n",
      "\n",
      "FAISS log ‚Üí chunk_reports/faiss_analysis_20250610_012518.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Clear any cached imports to ensure we get the latest versions\n",
    "modules_to_reload = [\n",
    "    'core_modules.rag.wiki_scraper',\n",
    "    'core_modules.rag.document_processor', \n",
    "    'core_modules.rag.data_chunking',\n",
    "    'core_modules.rag.vector_store'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Project module imports\n",
    "from core_modules.rag.document_processor import process_documents\n",
    "from core_modules.rag.wiki_scraper import scrape_wikipedia_articles\n",
    "from core_modules.rag.data_chunking import load_and_chunk_files, save_chunked_data\n",
    "from core_modules.rag.vector_store import FAISSDBManager, _load_docs \n",
    "from core_modules.rag.performance_analyzer import PerformanceAnalyzer\n",
    "\n",
    "# Test the import to make sure it works\n",
    "print(\"üîß Testing imports...\")\n",
    "try:\n",
    "    from core_modules.rag.wiki_scraper import scrape_wikipedia_article\n",
    "    print(\"‚úÖ scrape_wikipedia_article imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "\n",
    "print(\"üöÄ All imports ready!\\n\")\n",
    "\n",
    "\n",
    "# Step 1: Process source documents (PDF, DOCX, etc.)\n",
    "print(\"Step 1: Processing source documents...\")\n",
    "process_documents()\n",
    "\n",
    "# Step 2: Scrape Wikipedia articles\n",
    "print(\"\\nStep 2: Scraping Wikipedia articles...\")\n",
    "scrape_wikipedia_articles()\n",
    "\n",
    "# Step 3: Process and chunk all documents\n",
    "print(\"\\nStep 3: Processing and chunking documents...\")\n",
    "chunked_docs = load_and_chunk_files()\n",
    "save_chunked_data(chunked_docs)\n",
    "\n",
    "# Step 4: Build and evaluate the FAISS vector store\n",
    "print(\"\\nStep 4: Building and evaluating FAISS vector store...\")   \n",
    "\n",
    "GROUND_TRUTH = {\n",
    "       \"what are the benefits of modular design\": \"web_sources/wikipedia_modular_design.txt\",\n",
    "       \"how does modular design work in vehicles\": \"web_sources/wikipedia_modular_design.txt\"\n",
    "   }\n",
    "\n",
    "JSON_PATH = \"converted_chunked_data/chunked_data_full.json\"\n",
    "\n",
    "f_mgr = FAISSDBManager()\n",
    "f_docs = _load_docs(JSON_PATH, f_mgr.log)\n",
    "f_mgr.build(f_docs)\n",
    "f_mgr.evaluate(GROUND_TRUTH)\n",
    "print(f\"\\nFAISS log ‚Üí {f_mgr.log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running DWSIM Quick Demo...\n",
      "üîß Starting DWSIM simulation workflow...\n",
      "‚ùå Unexpected error: No module named 'System'\n",
      "\n",
      "üí° To use DWSIM integration:\n",
      "   1. Install DWSIM on your system\n",
      "   2. Set DWSIM_DLL_PATH environment variable\n",
      "   3. Place a .dwsim file in examples/ directory\n",
      "   4. Run: run_dwsim_simulation('your_file.dwsim')\n"
     ]
    }
   ],
   "source": [
    "# DWSIM Simulation - Simple Function Calls\n",
    "from dwsim_workflow import run_dwsim_simulation, quick_dwsim_demo\n",
    "\n",
    "# One-line demo - runs the entire DWSIM workflow\n",
    "quick_dwsim_demo()\n",
    "\n",
    "# Or run a custom simulation:\n",
    "# csv_path = run_dwsim_simulation(\"my_plant.dwsim\", \"results/my_streams.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [],
   "source": [
    "# # Log end time\n",
    "# with open(\"update_log.txt\", \"a\") as f:\n",
    "#     f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# # Simple GitHub update function\n",
    "# def update_github():\n",
    "#     !git add .\n",
    "#     !git commit -m \"Update: Adding all files to repository\"\n",
    "#     !git push origin main\n",
    "#     print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# # To use it, just run:\n",
    "# update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
