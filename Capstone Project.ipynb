{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#--------Google Drive Integration--------#\n",
    "# from google.colab import drive, userdata\n",
    "# This gives Colab access to your files in Google Drive.\n",
    "# drive.mount('/content/drive')\n",
    "# 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "\n",
    "\n",
    "#--------Cursor Integration--------#\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "\n",
    "# Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“„ Starting processing for 4 file(s) in 'source_documents' ---\n",
      "  â–¶ Processing: 1-s2.0-S0925527302003742-main.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/fhqbcx0d6sqc_273srv0dv300000gn/T/ipykernel_41995/2201599768.py:45: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  loader = UnstructuredFileLoader(input_path, mode=\"paged\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€¢ Success! Saved to: processed_txt_files/1-s2.0-S0925527302003742-main.txt\n",
      "  â–¶ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "`mode='paged'` is deprecated in favor of the 'by_page' chunking strategy. Learn more about chunking here: https://docs.unstructured.io/open-source/core-functionality/chunking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      "  â–¶ Processing: mcp_basics.txt\n",
      "    â€¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      "  â–¶ Processing: feasibility_factors.txt\n",
      "    â€¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      "\n",
      "\n",
      " All files processed.\n"
     ]
    }
   ],
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"ðŸ“‚ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"â„¹ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- ðŸ“„ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        \n",
    "        # Create a clean output filename by changing the extension to .txt\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "        \n",
    "        print(f\"  â–¶ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # The UnstructuredFileLoader automatically handles different file types.\n",
    "            # The \"paged\" mode is faster and works for most text-based files.\n",
    "            # It can be set to \"elements\" or \"ocr\" for more complex, scanned documents.\n",
    "            loader = UnstructuredFileLoader(input_path, mode=\"paged\")\n",
    "            \n",
    "            # The .load() method does all the work of extracting the text\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Combine the page content into a single block of text\n",
    "            full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "                \n",
    "            print(f\"    â€¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    â€¢ Error processing {filename}: {e}\")\n",
    "            \n",
    "    print(\"\\n\\n All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Phase 1: Searching Google for URLs ======================\n",
      "Searching for keyword: 'modular chemical plants cost analysis 2024'...\n",
      "An error occurred during Google search: search() got an unexpected keyword argument 'query'\n",
      "Continuing with next keyword...\n",
      "Searching for keyword: 'MCP feasibility study'...\n",
      "An error occurred during Google search: search() got an unexpected keyword argument 'query'\n",
      "Continuing with next keyword...\n",
      "Searching for keyword: 'pre-fabricated chemical modules logistics'...\n",
      "An error occurred during Google search: search() got an unexpected keyword argument 'query'\n",
      "Continuing with next keyword...\n",
      "\n",
      "Found 0 unique candidate URLs.\n",
      "=================\n",
      " Phase 2: Downloading and Extracting Content =================\n",
      "===========================\n",
      " Phase 3: Saving Results ===========================\n",
      " No articles meeting the criteria were found or extracted.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# A SIMPLER CRAWLER USING REQUESTS (REPLACES THE SCRAPY VERSION)\n",
    "# =========================================================================\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import trafilatura\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import io\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "KEYWORDS = [\n",
    "    \"modular chemical plants cost analysis 2024\",\n",
    "    \"MCP feasibility study\",\n",
    "    \"pre-fabricated chemical modules logistics\",\n",
    "]\n",
    "SEARCH_RESULTS_PER_KW = 30 # How many results to fetch per keyword\n",
    "MIN_LENGTH = 50            # Minimum character length to save an article\n",
    "OUTPUT_DIR = pathlib.Path(\"data_source\")\n",
    "OUTPUT_FILE = OUTPUT_DIR / \"articles.csv\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- CRAWLING LOGIC ---\n",
    "\n",
    "# 1. Get all URLs from Google Search\n",
    "# -----------------------------------\n",
    "all_urls = set() # Use a set to automatically handle duplicates\n",
    "print(\" Phase 1: Searching Google for URLs \".center(80, \"=\"))\n",
    "for kw in KEYWORDS:\n",
    "    print(f\"Searching for keyword: '{kw}'...\")\n",
    "    try:\n",
    "        # We add a short delay to be respectful to Google's servers\n",
    "        for url in search(query=kw, num=SEARCH_RESULTS_PER_KW, stop=SEARCH_RESULTS_PER_KW, pause=2.0, lang=\"en\"):\n",
    "            if not any(bad in url for bad in (\"google.\", \"/search?\", \"facebook.com\")):\n",
    "                all_urls.add(url)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Google search: {e}\")\n",
    "        print(\"Continuing with next keyword...\")\n",
    "\n",
    "print(f\"\\nFound {len(all_urls)} unique candidate URLs.\")\n",
    "\n",
    "\n",
    "# 2. Download and Extract Content from each URL\n",
    "# ---------------------------------------------\n",
    "results = []\n",
    "processed_urls = set()\n",
    "print(\"\\n Phase 2: Downloading and Extracting Content \".center(80, \"=\"))\n",
    "\n",
    "for i, url in enumerate(all_urls):\n",
    "    if url in processed_urls:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing ({i+1}/{len(all_urls)}): {url}\")\n",
    "    try:\n",
    "        # Download the content with a timeout\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        text = \"\"\n",
    "        title = \"\"\n",
    "        \n",
    "        # Check if content is PDF or HTML\n",
    "        content_type = response.headers.get(\"content-type\", \"\").lower()\n",
    "        if \"pdf\" in content_type or url.lower().endswith('.pdf'):\n",
    "            # It's a PDF\n",
    "            text = extract_text(io.BytesIO(response.content))\n",
    "            title = url.split('/')[-1] # Use filename as title for PDFs\n",
    "        else:\n",
    "            # It's HTML\n",
    "            # Use trafilatura to get the main text and metadata\n",
    "            article_text = trafilatura.extract(response.text)\n",
    "            if article_text:\n",
    "                text = article_text\n",
    "                metadata = trafilatura.extract_metadata(response.text)\n",
    "                if metadata and metadata.title:\n",
    "                    title = metadata.title\n",
    "\n",
    "        # Check if the extracted text is long enough\n",
    "        if text and len(text) >= MIN_LENGTH:\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'text': text\n",
    "            })\n",
    "            print(f\"  -> Success: Extracted {len(text):,} characters.\")\n",
    "        else:\n",
    "            print(\"  -> Skipped: Content was too short or extraction failed.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  -> Failed to download: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> An unexpected error occurred: {e}\")\n",
    "    \n",
    "    processed_urls.add(url)\n",
    "    time.sleep(1) # Wait 1 second between requests to be polite\n",
    "\n",
    "\n",
    "# 3. Save the results to a CSV file\n",
    "# -----------------------------------\n",
    "print(\"\\n Phase 3: Saving Results \".center(80, \"=\"))\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\" Success! Saved {len(df)} articles to '{OUTPUT_FILE}'\")\n",
    "else:\n",
    "    print(\" No articles meeting the criteria were found or extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 4\n",
      "Minimum Chunk Size: 126 characters\n",
      "Maximum Chunk Size: 472 characters\n",
      "Average Chunk Size: 322.00 characters\n",
      "Standard Deviation of Chunk Size: 143.16\n",
      "\n",
      "[INFO] Chunking statistics appear healthy. Sizes are consistent.\n",
      "\n",
      "--- Sample Chunk Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: mcp_basics.txt, Length: 472 chars) ---\n",
      "Modular Chemical Plants (MCPs) represent a paradigm shift in chemical process engineering. They involve constructing plants from standardized, pre-fabricated modules built off-site. This approach significantly reduces on-site construction time and costs compared to traditional stick-built plants. Key advantages include faster deployment, scalability, and potentially lower capital expenditure. However, module transportation and site integration require careful planning\n",
      "\n",
      "--- Chunk 2 (Source: mcp_basics.txt, Length: 126 chars) ---\n",
      ". MCPs are particularly suited for remote locations or projects with uncertain market demands, allowing for phased investment.\n",
      "\n",
      "--- Chunk 3 (Source: feasibility_factors.txt, Length: 445 chars) ---\n",
      "Evaluating the feasibility of an MCP involves assessing technical, economic, and logistical aspects. Feedstock availability and cost are primary drivers for many chemical processes. Market demand for the plant's output and projected pricing influence revenue streams. Capital costs for module fabrication, transportation, and site preparation, along with operating expenses like utilities, labor, and maintenance, determine overall profitability\n",
      "\n",
      "\n",
      "Data ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Inspect the Results ---\n",
    "import numpy as np\n",
    "# --- NEW: Statistical Analysis of Chunks ---\n",
    "print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "\n",
    "# Calculate the lengths of all chunks\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "\n",
    "# Calculate and print key statistics\n",
    "total_chunks = len(chunk_lengths)\n",
    "min_size = np.min(chunk_lengths)\n",
    "max_size = np.max(chunk_lengths)\n",
    "avg_size = np.mean(chunk_lengths)\n",
    "std_dev = np.std(chunk_lengths)\n",
    "\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "\n",
    "# --- Automated Quality Feedback ---\n",
    "\n",
    "# 1. Check for high variation in chunk size\n",
    "# A high standard deviation suggests inconsistent chunking.\n",
    "if std_dev > 150:\n",
    "    print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "    print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "    print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "\n",
    "# 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "small_chunk_threshold = CHUNK_SIZE * 0.20 # Chunks smaller than 20% of the target size\n",
    "small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "\n",
    "if small_chunk_count > 0:\n",
    "    # This check is more specific than just looking at the absolute minimum.\n",
    "    print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "    print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "    print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "    print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "\n",
    "# Add a success message if no issues are flagged\n",
    "if std_dev <= 150 and small_chunk_count == 0:\n",
    "    print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "\n",
    "\n",
    "# --- Manual Inspection of Sample Chunks ---\n",
    "# (This part remains the same)\n",
    "print(\"\\n--- Sample Chunk Preview ---\")\n",
    "# Print the first few chunks to get a feel for their content and structure\n",
    "for i, chunk in enumerate(chunked_documents[:3]): # Print first 3 chunks\n",
    "    chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "\n",
    "print(\"\\n\\nData ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL-SmhSQQ_21"
   },
   "source": [
    "# **Data Scrapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main a01bcca] Update: Adding all files to repository\n",
      " 1 file changed, 146 insertions(+), 146 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 3.04 KiB | 3.04 MiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   8c7c64b..a01bcca  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
