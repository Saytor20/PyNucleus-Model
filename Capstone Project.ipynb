{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìÑ Starting processing for 4 file(s) in 'source_documents' ---\n",
      " ‚ñ∂ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " ‚ñ∂ Processing: mcp_basics.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      " ‚ñ∂ Processing: feasibility_factors.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      " ‚ñ∂ Processing: Bist_Madan.pdf\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Bist_Madan.txt\n",
      "\n",
      "\n",
      " All files processed.\n"
     ]
    }
   ],
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"üìÇ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"‚Ñπ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- üìÑ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Skip hidden files like .DS_Store\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "        print(f\" ‚ñ∂ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Handle PDF files differently\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                # Use PyPDF2 for PDF files\n",
    "                reader = PdfReader(input_path)\n",
    "                full_text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\\n\"\n",
    "            else:\n",
    "                # Use UnstructuredLoader for other file types\n",
    "                loader = UnstructuredLoader(input_path)\n",
    "                documents = loader.load()\n",
    "                full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"   ‚Ä¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Ä¢ Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n\\n All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:49:11.411850Z",
     "start_time": "2025-06-04T16:49:11.195022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting Wikipedia article search for 5 keywords...\n",
      "‚ñ∂Ô∏è  Searching for: modular design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_modular_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: software architecture\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_software_architecture.txt\n",
      "‚ñ∂Ô∏è  Searching for: system design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_system_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: industrial design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_industrial_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: supply chain\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_supply_chain.txt\n",
      "\n",
      "‚ú® Article scraping complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Keywords to search for in Wikipedia\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"modular design\",\n",
    "    \"software architecture\",\n",
    "    \"system design\",\n",
    "    \"industrial design\",\n",
    "    \"supply chain\"\n",
    "]\n",
    "\n",
    "# Output directory for saved articles\n",
    "DATA_DIR = \"data_sources\"\n",
    "\n",
    "def search_wikipedia(keyword):\n",
    "    \"\"\"Search Wikipedia for a keyword and return the first result URL\"\"\"\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={quote(keyword)}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['query']['search']:\n",
    "        title = data['query']['search'][0]['title']\n",
    "        return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "    return None\n",
    "\n",
    "def scrape_and_save_article(url, keyword):\n",
    "    \"\"\"Scrape a Wikipedia article and save it as a text file\"\"\"\n",
    "    print(f\"‚ñ∂Ô∏è  Searching for: {keyword}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch the article\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the main content\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        if not content:\n",
    "            print(f\"‚ùå  Could not find article content for: {keyword}\")\n",
    "            return\n",
    "        \n",
    "        # Extract text from paragraphs and headers\n",
    "        article_text = \"\"\n",
    "        for element in content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text:\n",
    "                article_text += text + \"\\n\\n\"\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"wikipedia_{keyword.replace(' ', '_')}.txt\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(article_text)\n",
    "            \n",
    "        print(f\"‚úÖ  Saved article to: {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå  Error processing {keyword}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"üîç Starting Wikipedia article search for {len(SEARCH_KEYWORDS)} keywords...\")\n",
    "    \n",
    "    for keyword in SEARCH_KEYWORDS:\n",
    "        article_url = search_wikipedia(keyword)\n",
    "        if article_url:\n",
    "            scrape_and_save_article(article_url, keyword)\n",
    "        else:\n",
    "            print(f\"‚ùå  No article found for: {keyword}\")\n",
    "    \n",
    "    print(\"\\n‚ú® Article scraping complete!\")\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 9 documents for chunking\n",
      "Split into 883 chunks\n",
      "\n",
      "‚úÖ Successfully saved chunked data to Chuncked_Data/:\n",
      "  ‚Ä¢ chunked_data_full.json - Complete data with metadata\n",
      "  ‚Ä¢ chunked_data_stats.json - Statistical analysis\n",
      "  ‚Ä¢ chunked_data_content.txt - Human-readable content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_chunk_files():\n",
    "    \"\"\"\n",
    "    Load and chunk files from both data_sources and processed_txt_files directories\n",
    "    \"\"\"\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Process files from both directories\n",
    "    directories = ['data_sources', 'processed_txt_files']\n",
    "    \n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"‚ö†Ô∏è Directory {directory} not found\")\n",
    "            continue\n",
    "            \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                        # Create Document object with metadata\n",
    "                        doc = Document(\n",
    "                            page_content=text,\n",
    "                            metadata={\"source\": file_path}\n",
    "                        )\n",
    "                        all_documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(all_documents)} documents for chunking\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunked_documents = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(chunked_documents)} chunks\")\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "def save_chunked_data(chunked_documents, output_dir=\"Chuncked_Data\"):\n",
    "    \"\"\"\n",
    "    Save chunked documents into three separate files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save full content with metadata\n",
    "    full_content = []\n",
    "    for i, chunk in enumerate(chunked_documents):\n",
    "        full_content.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"content\": chunk.page_content,\n",
    "            \"source\": chunk.metadata.get('source', 'N/A'),\n",
    "            \"length\": len(chunk.page_content)\n",
    "        })\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_full.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_content, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Save statistical analysis\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunked_documents),\n",
    "        \"chunk_lengths\": [len(chunk.page_content) for chunk in chunked_documents],\n",
    "        \"sources\": list(set(chunk.metadata.get('source', 'N/A') for chunk in chunked_documents)),\n",
    "        \"generated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_stats.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # 3. Save content-only version (for easy reading)\n",
    "    with open(os.path.join(output_dir, \"chunked_data_content.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for i, chunk in enumerate(chunked_documents):\n",
    "            f.write(f\"=== Chunk {i+1} ===\\n\")\n",
    "            f.write(f\"Source: {chunk.metadata.get('source', 'N/A')}\\n\")\n",
    "            f.write(f\"Length: {len(chunk.page_content)} characters\\n\")\n",
    "            f.write(\"\\nContent:\\n\")\n",
    "            f.write(chunk.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved chunked data to {output_dir}/:\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_full.json - Complete data with metadata\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_stats.json - Statistical analysis\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_content.txt - Human-readable content\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and chunk the documents\n",
    "    chunked_docs = load_and_chunk_files()\n",
    "    \n",
    "    # Save the chunked data\n",
    "    save_chunked_data(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 883\n",
      "Minimum Chunk Size: 4 characters\n",
      "Maximum Chunk Size: 500 characters\n",
      "Average Chunk Size: 365.26 characters\n",
      "Median Chunk Size: 416.00 characters\n",
      "Standard Deviation of Chunk Size: 125.58\n",
      "\n",
      "--- Source Distribution ---\n",
      "Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt: 344 chunks (39.0%)\n",
      "Bist_Madan.txt: 296 chunks (33.5%)\n",
      "wikipedia_supply_chain.txt: 74 chunks (8.4%)\n",
      "wikipedia_industrial_design.txt: 63 chunks (7.1%)\n",
      "wikipedia_software_architecture.txt: 61 chunks (6.9%)\n",
      "wikipedia_modular_design.txt: 34 chunks (3.9%)\n",
      "wikipedia_system_design.txt: 7 chunks (0.8%)\n",
      "mcp_basics.txt: 2 chunks (0.2%)\n",
      "feasibility_factors.txt: 2 chunks (0.2%)\n",
      "\n",
      "[ADVISORY] Found 47 chunks smaller than 100.0 characters.\n",
      "  > The smallest chunk is 4 characters.\n",
      "  > These small chunks might lack sufficient context and could clutter search results.\n",
      "  > Consider cleaning the source documents or adjusting the chunking separators.\n",
      "\n",
      "--- Sample Chunks Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: wikipedia_software_architecture.txt, Length: 263 chars) ---\n",
      "Software architecture is the set of structures needed to reason about a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relation...\n",
      "\n",
      "--- Chunk 2 (Source: wikipedia_software_architecture.txt, Length: 298 chars) ---\n",
      "The architecture of a software system is a metaphor, analogous to the architecture of a building.[2] It functions as the blueprints for the system and the development project, which project management...\n",
      "\n",
      "--- Chunk 3 (Source: wikipedia_software_architecture.txt, Length: 291 chars) ---\n",
      "Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_chunked_data(json_file='Chuncked_Data/chunked_data_full.json'):\n",
    "    \"\"\"\n",
    "    Load chunked data from JSON file and convert to Document objects.\n",
    "    \n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing chunked data\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            chunked_data = json.load(f)\n",
    "        \n",
    "        # Convert JSON data to Document objects\n",
    "        chunked_documents = [\n",
    "            Document(\n",
    "                page_content=chunk['content'],\n",
    "                metadata={'source': chunk['source']}\n",
    "            ) for chunk in chunked_data\n",
    "        ]\n",
    "        return chunked_documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunked data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_chunked_data(chunked_documents):\n",
    "    \"\"\"\n",
    "    Analyze the chunked documents and provide statistical insights.\n",
    "    \n",
    "    Args:\n",
    "        chunked_documents (list): List of chunked Document objects\n",
    "    \"\"\"\n",
    "    if not chunked_documents:\n",
    "        print(\"‚ö†Ô∏è No chunked documents provided for analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "    \n",
    "    # Calculate the lengths of all chunks\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "    \n",
    "    # Calculate and print key statistics\n",
    "    total_chunks = len(chunk_lengths)\n",
    "    min_size = np.min(chunk_lengths)\n",
    "    max_size = np.max(chunk_lengths)\n",
    "    avg_size = np.mean(chunk_lengths)\n",
    "    std_dev = np.std(chunk_lengths)\n",
    "    median_size = np.median(chunk_lengths)\n",
    "    \n",
    "    print(f\"Total Chunks: {total_chunks}\")\n",
    "    print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "    print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "    print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "    print(f\"Median Chunk Size: {median_size:.2f} characters\")\n",
    "    print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "    \n",
    "    # --- Source Distribution Analysis ---\n",
    "    source_counts = Counter([chunk.metadata.get('source', 'N/A') for chunk in chunked_documents])\n",
    "    print(\"\\n--- Source Distribution ---\")\n",
    "    for source, count in source_counts.most_common():\n",
    "        print(f\"{os.path.basename(source)}: {count} chunks ({count/total_chunks*100:.1f}%)\")\n",
    "    \n",
    "    # --- Automated Quality Feedback ---\n",
    "    CHUNK_SIZE = 500  # Target chunk size\n",
    "    \n",
    "    # 1. Check for high variation in chunk size\n",
    "    if std_dev > 150:\n",
    "        print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "        print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "        print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "    \n",
    "    # 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "    small_chunk_threshold = CHUNK_SIZE * 0.20  # Chunks smaller than 20% of the target size\n",
    "    small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "    \n",
    "    if small_chunk_count > 0:\n",
    "        print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "        print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "        print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "        print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "    \n",
    "    # 3. Check for very large chunks\n",
    "    large_chunk_threshold = CHUNK_SIZE * 1.5  # Chunks larger than 150% of the target size\n",
    "    large_chunk_count = sum(1 for length in chunk_lengths if length > large_chunk_threshold)\n",
    "    \n",
    "    if large_chunk_count > 0:\n",
    "        print(f\"\\n[ADVISORY] Found {large_chunk_count} chunks larger than {large_chunk_threshold} characters.\")\n",
    "        print(f\"  > The largest chunk is {max_size} characters.\")\n",
    "        print(\"  > These large chunks might contain too much information for effective processing.\")\n",
    "    \n",
    "    # Add a success message if no issues are flagged\n",
    "    if std_dev <= 150 and small_chunk_count == 0 and large_chunk_count == 0:\n",
    "        print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "    \n",
    "    # --- Sample Chunks Preview ---\n",
    "    print(\"\\n--- Sample Chunks Preview ---\")\n",
    "    for i, chunk in enumerate(chunked_documents[:3]):  # Print first 3 chunks\n",
    "        chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "        print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "        print(chunk.page_content[:200] + \"...\" if len(chunk.page_content) > 200 else chunk.page_content)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis.\n",
    "    \"\"\"\n",
    "    # Load the chunked data\n",
    "    chunked_documents = load_chunked_data()\n",
    "    \n",
    "    if chunked_documents:\n",
    "        # Run the analysis\n",
    "        analyze_chunked_data(chunked_documents)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Failed to load chunked data. Please check the JSON file path.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main a78207f] Update: Adding all files to repository\n",
      " 14 files changed, 18257 insertions(+), 1220 deletions(-)\n",
      " create mode 100644 Chuncked_Data/chunked_data_content.txt\n",
      " create mode 100644 Chuncked_Data/chunked_data_full.json\n",
      " create mode 100644 Chuncked_Data/chunked_data_stats.json\n",
      " create mode 100644 analyze_chunks.py\n",
      " delete mode 100644 data_sources/en_wikipedia_org_wiki_Modular_design.txt\n",
      " create mode 100644 data_sources/wikipedia_industrial_design.txt\n",
      " create mode 100644 data_sources/wikipedia_modular_design.txt\n",
      " create mode 100644 data_sources/wikipedia_software_architecture.txt\n",
      " create mode 100644 data_sources/wikipedia_supply_chain.txt\n",
      " create mode 100644 data_sources/wikipedia_system_design.txt\n",
      " create mode 100644 requirements.txt\n",
      " create mode 100644 save_chunked_data.py\n",
      "Enumerating objects: 21, done.\n",
      "Counting objects: 100% (21/21), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (16/16), done.\n",
      "Writing objects: 100% (17/17), 167.13 KiB | 9.28 MiB/s, done.\n",
      "Total 17 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "^C\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
