{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime\n",
    "# from dotenv import load_dotenv\n",
    "# #\n",
    "# # #--------Google Drive Integration--------#\n",
    "# # # from google.colab import drive, userdata\n",
    "# # # This gives Colab access to your files in Google Drive.\n",
    "# # # drive.mount('/content/drive')\n",
    "# # # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# # GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# # GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# # REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# # NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "# #\n",
    "# #\n",
    "# # #--------Cursor Integration--------#\n",
    "# # # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# #\n",
    "# # # Get GitHub credentials from environment variables\n",
    "# GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "# #\n",
    "# # # Print to verify the variables are loaded (remove this in production)\n",
    "# print(f\"Username: {GITHUB_USERNAME}\")\n",
    "# print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "# #\n",
    "# # Repository information\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "# NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "# LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# # Pull latest changes from GitHub\n",
    "# print(\"Pulling latest changes from GitHub...\")\n",
    "# !git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "# print(\"Repository is up to date!\")\n",
    "\n",
    "# # Log start time\n",
    "# with open(\"update_log.txt\", \"a\") as f:\n",
    "#     f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from core_modules.rag import config\n",
    "\n",
    "# Clear any cached imports to ensure we get the latest versions\n",
    "modules_to_reload = [\n",
    "    'core_modules.rag.wiki_scraper',\n",
    "    'core_modules.rag.document_processor', \n",
    "    'core_modules.rag.data_chunking',\n",
    "    'core_modules.rag.vector_store'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Project module imports\n",
    "from core_modules.rag.document_processor import process_documents\n",
    "from core_modules.rag.wiki_scraper import scrape_wikipedia_articles\n",
    "from core_modules.rag.data_chunking import load_and_chunk_files, save_chunked_data\n",
    "from core_modules.rag.vector_store import FAISSDBManager, _load_docs \n",
    "from core_modules.rag.performance_analyzer import PerformanceAnalyzer\n",
    "from core_modules.rag import config\n",
    "\n",
    "# Test the import to make sure it works\n",
    "print(\"ðŸ”§ Testing imports...\")\n",
    "try:\n",
    "    from core_modules.rag.wiki_scraper import scrape_wikipedia_article\n",
    "    print(\"âœ… scrape_wikipedia_article imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "\n",
    "print(\"ðŸš€ All imports ready!\\n\")\n",
    "\n",
    "# Step 1: Process source documents (PDF, DOCX, etc.)\n",
    "print(\"Step 1: Processing source documents...\")\n",
    "process_documents()\n",
    "\n",
    "# Step 2: Scrape Wikipedia articles\n",
    "print(\"\\nStep 2: Scraping Wikipedia articles...\")\n",
    "scrape_wikipedia_articles()\n",
    "\n",
    "# Step 3: Process and chunk all documents\n",
    "print(\"\\nStep 3: Processing and chunking documents...\")\n",
    "chunked_docs = load_and_chunk_files()\n",
    "save_chunked_data(chunked_docs)\n",
    "\n",
    "# Step 4: Build and evaluate the FAISS vector store\n",
    "print(\"\\nStep 4: Building and evaluating FAISS vector store...\")   \n",
    "\n",
    "GROUND_TRUTH = config.GROUND_TRUTH_DATA\n",
    "JSON_PATH = str(config.FULL_JSON_PATH)\n",
    "\n",
    "f_mgr = FAISSDBManager()\n",
    "f_docs = _load_docs(JSON_PATH, f_mgr.log)\n",
    "f_mgr.build(f_docs)\n",
    "f_mgr.evaluate(GROUND_TRUTH)\n",
    "print(f\"\\nFAISS log â†’ {f_mgr.log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test diverse queries\n",
    "print(\"ðŸ” Testing diverse queries...\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the key challenges in implementing modular chemical plants?\",\n",
    "    \"How does supply chain management affect modular design?\",\n",
    "    \"What are the economic benefits of modular construction?\",\n",
    "    \"How does software architecture relate to modular design?\",\n",
    "    \"What are the environmental impacts of modular manufacturing?\"\n",
    "]\n",
    "\n",
    "# Create a new FAISS manager instance\n",
    "f_mgr = FAISSDBManager()\n",
    "f_docs = _load_docs(str(config.FULL_JSON_PATH), f_mgr.log)\n",
    "\n",
    "# Build the index\n",
    "f_mgr.build(f_docs)\n",
    "\n",
    "# Test each query\n",
    "print(\"=== Query Results ===\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    results = f_mgr.search(query, k=3)\n",
    "    \n",
    "    print(\"\\nTop 3 Results:\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Analyze chunking statistics\n",
    "print(\"\\n=== Chunking Statistics ===\")\n",
    "print(f\"Total Chunks: {len(f_docs)}\")\n",
    "print(f\"Average Chunk Size: {sum(len(doc.page_content) for doc in f_docs) / len(f_docs):.1f} characters\")\n",
    "print(f\"Number of Sources: {len(set(doc.metadata.get('source') for doc in f_docs))}\")\n",
    "\n",
    "# Distribution of chunks per source\n",
    "source_counts = {}\n",
    "for doc in f_docs:\n",
    "    source = doc.metadata.get('source', 'Unknown')\n",
    "    source_counts[source] = source_counts.get(source, 0) + 1\n",
    "\n",
    "print(\"\\nChunks per Source:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  â€¢ {source.split('/')[-1]}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running DWSIM Quick Demo...\n",
      "ðŸ”§ Starting DWSIM simulation workflow...\n",
      "âŒ Unexpected error: No module named 'System'\n",
      "\n",
      "ðŸ’¡ To use DWSIM integration:\n",
      "   1. Install DWSIM on your system\n",
      "   2. Set DWSIM_DLL_PATH environment variable\n",
      "   3. Place a .dwsim file in examples/ directory\n",
      "   4. Run: run_dwsim_simulation('your_file.dwsim')\n"
     ]
    }
   ],
   "source": [
    "# DWSIM Simulation - Simple Function Calls\n",
    "from dwsim_workflow import run_dwsim_simulation, quick_dwsim_demo\n",
    "\n",
    "# One-line demo - runs the entire DWSIM workflow\n",
    "quick_dwsim_demo()\n",
    "\n",
    "# Or run a custom simulation:\n",
    "# csv_path = run_dwsim_simulation(\"my_plant.dwsim\", \"results/my_streams.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 135c545] Update: Adding all files to repository\n",
      " 4 files changed, 27 insertions(+), 76 deletions(-)\n",
      " rename DIRECTORY_RENAME_SUMMARY.md => project_info/DIRECTORY_RENAME_SUMMARY.md (100%)\n",
      " rename PROJECT_STRUCTURE.md => project_info/PROJECT_STRUCTURE.md (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 8, done.\n",
      "Counting objects: 100% (8/8), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 779 bytes | 779.00 KiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   7699006..135c545  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
