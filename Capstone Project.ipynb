{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"📂 Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"ℹ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- 📄 Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Skip hidden files like .DS_Store\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "        print(f\" ▶ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Handle PDF files differently\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                # Use PyPDF2 for PDF files\n",
    "                reader = PdfReader(input_path)\n",
    "                full_text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\\n\"\n",
    "            else:\n",
    "                # Use UnstructuredLoader for other file types\n",
    "                loader = UnstructuredLoader(input_path)\n",
    "                documents = loader.load()\n",
    "                full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"   • Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   • Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n\\n All files processed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:49:11.411850Z",
     "start_time": "2025-06-04T16:49:11.195022Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# ── CONFIGURATION ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) The URL you want to scrape:\n",
    "TARGET_URL = \"https://en.wikipedia.org/wiki/Modular_design\"\n",
    "\n",
    "# 2) Where to put the plain‐text output:\n",
    "DATA_DIR = \"data_sources\"\n",
    "\n",
    "\n",
    "# ── HELPER TO TURN A URL INTO A SAFE FILENAME ────────────────────────────────────\n",
    "\n",
    "def make_filename_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a URL into a filesystem‐safe name.\n",
    "    Example: \"https://example.com/foo/bar.html\" -> \"example.com_foo_bar.txt\"\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    # join netloc + path, replace any \"/\" or \".\" with \"_\"\n",
    "    raw_name = parsed.netloc + parsed.path\n",
    "    safe_name = raw_name.replace(\"/\", \"_\").replace(\".\", \"_\").strip(\"_\")\n",
    "    return safe_name + \".txt\"\n",
    "\n",
    "\n",
    "# ── MAIN SCRAPER FUNCTION ──────────────────────────────────────────────────────\n",
    "\n",
    "def fetch_and_save_text(url: str, output_dir: str):\n",
    "    # 1) Make sure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2) Build our output filename from the URL\n",
    "    out_filename = make_filename_from_url(url)\n",
    "    out_path = os.path.join(output_dir, out_filename)\n",
    "\n",
    "    print(f\"▶️  Fetching: {url}\")\n",
    "    try:\n",
    "        # 3) Grab the HTML\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                          \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌  Error fetching page: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4) Parse with BeautifulSoup and extract visible text\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    page_text = soup.get_text(separator=\" \").strip()\n",
    "\n",
    "    # 5) Save to a .txt file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(page_text)\n",
    "\n",
    "    print(f\"✅  Saved plain text to: {out_path}\")\n",
    "\n",
    "\n",
    "# ── ENTRY POINT ────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_and_save_text(TARGET_URL, DATA_DIR)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️  Fetching: https://en.wikipedia.org/wiki/Modular_design\n",
      "✅  Saved plain text to: data_sources/en_wikipedia_org_wiki_Modular_design.txt\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- 4. Inspect the Results ---\n",
    "import numpy as np\n",
    "# --- NEW: Statistical Analysis of Chunks ---\n",
    "print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "\n",
    "# Calculate the lengths of all chunks\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "\n",
    "# Calculate and print key statistics\n",
    "total_chunks = len(chunk_lengths)\n",
    "min_size = np.min(chunk_lengths)\n",
    "max_size = np.max(chunk_lengths)\n",
    "avg_size = np.mean(chunk_lengths)\n",
    "std_dev = np.std(chunk_lengths)\n",
    "\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "\n",
    "# --- Automated Quality Feedback ---\n",
    "\n",
    "# 1. Check for high variation in chunk size\n",
    "# A high standard deviation suggests inconsistent chunking.\n",
    "if std_dev > 150:\n",
    "    print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "    print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "    print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "\n",
    "# 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "small_chunk_threshold = CHUNK_SIZE * 0.20 # Chunks smaller than 20% of the target size\n",
    "small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "\n",
    "if small_chunk_count > 0:\n",
    "    # This check is more specific than just looking at the absolute minimum.\n",
    "    print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "    print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "    print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "    print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "\n",
    "# Add a success message if no issues are flagged\n",
    "if std_dev <= 150 and small_chunk_count == 0:\n",
    "    print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "\n",
    "\n",
    "# --- Manual Inspection of Sample Chunks ---\n",
    "# (This part remains the same)\n",
    "print(\"\\n--- Sample Chunk Preview ---\")\n",
    "# Print the first few chunks to get a feel for their content and structure\n",
    "for i, chunk in enumerate(chunked_documents[:3]): # Print first 3 chunks\n",
    "    chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "\n",
    "print(\"\\n\\nData ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
