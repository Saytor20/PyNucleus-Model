{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#--------Google Drive Integration--------#\n",
    "# from google.colab import drive, userdata\n",
    "# This gives Colab access to your files in Google Drive.\n",
    "# drive.mount('/content/drive')\n",
    "# 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "\n",
    "\n",
    "#--------Cursor Integration--------#\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "\n",
    "# Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone the Repository\n",
    "# repo_path = f'/content/{REPOSITORY_NAME}'\n",
    "# !git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git {repo_path}\n",
    "\n",
    "# # Change the current working directory to the repository\n",
    "# # All subsequent commands will run from inside the repo folder.\n",
    "# os.chdir(repo_path)\n",
    "\n",
    "# # Add a new line to your log file with the current date and time.\n",
    "# log_message = f\"Notebook saved on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "# with open(LOG_FILENAME, \"a\") as f:\n",
    "#     f.write(log_message + \"\\n\")\n",
    "# print(f\"Updated '{LOG_FILENAME}'\")\n",
    "\n",
    "# # Copy the latest version of notebook from Drive into the cloned repo.\n",
    "# !cp \"{NOTEBOOK_DRIVE_PATH}\" \"{NOTEBOOK_REPO_FILENAME}\"\n",
    "# print(f\"Copied '{NOTEBOOK_REPO_FILENAME}' from Google Drive.\")\n",
    "\n",
    "# # Git identity for commenting\n",
    "# !git config user.name \"{GITHUB_USERNAME}\"\n",
    "# !git config user.email \"{GITHUB_USERNAME}@users.noreply.github.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748965316223,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "SvrR6N-LH7HB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: /Users/mohammadalmusaiteer/PyNucleus-Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 491.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 2 documents.\n",
      "  - Document 1 Source: mcp_basics.txt\n",
      "  - Document 2 Source: feasibility_factors.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion and Preprocessing for RAG\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the path to your data (current directory)\n",
    "DATA_DIR = os.getcwd()\n",
    "\n",
    "# Load all.txt files from the current directory\n",
    "print(f\"Loading documents from: {DATA_DIR}\")\n",
    "loader = DirectoryLoader(DATA_DIR, glob=\"*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents.\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"  - Document {i+1} Source: {os.path.basename(doc.metadata.get('source', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 2 documents into 4 chunks.\n"
     ]
    }
   ],
   "source": [
    "#2. Text Chunking\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Create the text splitter with these parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Apply the splitter to the loaded documents\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplit {len(documents)} documents into {len(chunked_documents)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Analysis & Quality Check ---\n",
      "Total Chunks: 4\n",
      "Minimum Chunk Size: 126 characters\n",
      "Maximum Chunk Size: 472 characters\n",
      "Average Chunk Size: 322.00 characters\n",
      "Standard Deviation of Chunk Size: 143.16\n",
      "\n",
      "[INFO] Chunking statistics appear healthy. Sizes are consistent.\n",
      "\n",
      "--- Sample Chunk Preview ---\n",
      "\n",
      "--- Chunk 1 (Source: mcp_basics.txt, Length: 472 chars) ---\n",
      "Modular Chemical Plants (MCPs) represent a paradigm shift in chemical process engineering. They involve constructing plants from standardized, pre-fabricated modules built off-site. This approach significantly reduces on-site construction time and costs compared to traditional stick-built plants. Key advantages include faster deployment, scalability, and potentially lower capital expenditure. However, module transportation and site integration require careful planning\n",
      "\n",
      "--- Chunk 2 (Source: mcp_basics.txt, Length: 126 chars) ---\n",
      ". MCPs are particularly suited for remote locations or projects with uncertain market demands, allowing for phased investment.\n",
      "\n",
      "--- Chunk 3 (Source: feasibility_factors.txt, Length: 445 chars) ---\n",
      "Evaluating the feasibility of an MCP involves assessing technical, economic, and logistical aspects. Feedstock availability and cost are primary drivers for many chemical processes. Market demand for the plant's output and projected pricing influence revenue streams. Capital costs for module fabrication, transportation, and site preparation, along with operating expenses like utilities, labor, and maintenance, determine overall profitability\n",
      "\n",
      "\n",
      "Data ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Inspect the Results ---\n",
    "import numpy as np\n",
    "# --- NEW: Statistical Analysis of Chunks ---\n",
    "print(\"\\n--- Statistical Analysis & Quality Check ---\")\n",
    "\n",
    "# Calculate the lengths of all chunks\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunked_documents]\n",
    "\n",
    "# Calculate and print key statistics\n",
    "total_chunks = len(chunk_lengths)\n",
    "min_size = np.min(chunk_lengths)\n",
    "max_size = np.max(chunk_lengths)\n",
    "avg_size = np.mean(chunk_lengths)\n",
    "std_dev = np.std(chunk_lengths)\n",
    "\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"Minimum Chunk Size: {min_size} characters\")\n",
    "print(f\"Maximum Chunk Size: {max_size} characters\")\n",
    "print(f\"Average Chunk Size: {avg_size:.2f} characters\")\n",
    "print(f\"Standard Deviation of Chunk Size: {std_dev:.2f}\")\n",
    "\n",
    "# --- Automated Quality Feedback ---\n",
    "\n",
    "# 1. Check for high variation in chunk size\n",
    "# A high standard deviation suggests inconsistent chunking.\n",
    "if std_dev > 150:\n",
    "    print(f\"\\n[WARNING] High chunk size variation detected (Std Dev: {std_dev:.2f}).\")\n",
    "    print(\"  > This suggests documents may have irregular structures (e.g., many short lines or lists).\")\n",
    "    print(\"  > Resulting chunks may have inconsistent levels of context.\")\n",
    "\n",
    "# 2. Check for and count potentially \"orphaned\" or very small chunks\n",
    "small_chunk_threshold = CHUNK_SIZE * 0.20 # Chunks smaller than 20% of the target size\n",
    "small_chunk_count = sum(1 for length in chunk_lengths if length < small_chunk_threshold)\n",
    "\n",
    "if small_chunk_count > 0:\n",
    "    # This check is more specific than just looking at the absolute minimum.\n",
    "    print(f\"\\n[ADVISORY] Found {small_chunk_count} chunks smaller than {small_chunk_threshold} characters.\")\n",
    "    print(f\"  > The smallest chunk is {min_size} characters.\")\n",
    "    print(\"  > These small chunks might lack sufficient context and could clutter search results.\")\n",
    "    print(\"  > Consider cleaning the source documents or adjusting the chunking separators.\")\n",
    "\n",
    "# Add a success message if no issues are flagged\n",
    "if std_dev <= 150 and small_chunk_count == 0:\n",
    "    print(\"\\n[INFO] Chunking statistics appear healthy. Sizes are consistent.\")\n",
    "\n",
    "\n",
    "# --- Manual Inspection of Sample Chunks ---\n",
    "# (This part remains the same)\n",
    "print(\"\\n--- Sample Chunk Preview ---\")\n",
    "# Print the first few chunks to get a feel for their content and structure\n",
    "for i, chunk in enumerate(chunked_documents[:3]): # Print first 3 chunks\n",
    "    chunk_source = os.path.basename(chunk.metadata.get('source', 'N/A'))\n",
    "    print(f\"\\n--- Chunk {i+1} (Source: {chunk_source}, Length: {len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "\n",
    "\n",
    "print(\"\\n\\nData ingestion and preprocessing is complete. The 'chunked_documents' are ready for the next stage (embedding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL-SmhSQQ_21"
   },
   "source": [
    "# **Data Scrapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748965316233,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "I69992m0RE82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Found 5 URLs for 'modular chemical plants MCPs feasibility pre-fabricated chemical modules'.\n",
      "  â–¶ Scraping: /search?num=7\n",
      "    â€¢ âŒ Download failed\n",
      "  â–¶ Scraping: https://www.hm-ec.com/blog-posts/modular-process-plants-hm\n",
      "    â€¢ âœ… Extracted main text\n",
      "  â–¶ Scraping: https://www.tce.co.in/blogs/Modularization%20of%20chemical%20plants.pdf\n",
      "    â€¢ âš ï¸ No main text found\n",
      "  â–¶ Scraping: https://dechema.de/Cost_Engineering_for_Modular_Plants/_/2024.11.14%20Cost%20Engineering%20for%20Modular%20Plants%20FINAL.pdf\n",
      "    â€¢ âš ï¸ No main text found\n",
      "  â–¶ Scraping: https://aris.iaea.org/publications/smr_book_2020.pdf\n",
      "    â€¢ âŒ Download failed\n",
      "\n",
      "âœ… Saved 1 pages to 'data_sources/web_scraped_mcp_data.txt'.\n",
      "ðŸ“Š Key Metrics:\n",
      "   â€¢ URLs Found    : 5\n",
      "   â€¢ Successful    : 1\n",
      "   â€¢ Failed        : 4\n",
      "   â€¢ Total Chars   : 5861\n",
      "   â€¢ Total Words   : 803\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Installation (run once in your environment) ---\n",
    "\n",
    "import os\n",
    "from googlesearch import search\n",
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "KEYWORDS = [\"modular chemical plants\", \"MCPs feasibility\", \"pre-fabricated chemical modules\"]\n",
    "NUM_RESULTS = 5\n",
    "OUTPUT_PATH = os.path.join(\"data_sources\", \"web_scraped_mcp_data.txt\")\n",
    "\n",
    "# --- 3. Search + Scrape ---\n",
    "query = \" \".join(KEYWORDS)\n",
    "try:\n",
    "    urls = list(search(query, num_results=NUM_RESULTS, lang=\"en\"))\n",
    "    print(f\"ðŸ” Found {len(urls)} URLs for '{query}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Search error: {e}\")\n",
    "    urls = []\n",
    "\n",
    "scraped_contents = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"  â–¶ Scraping: {url}\")\n",
    "    raw = fetch_url(url)\n",
    "    if not raw:\n",
    "        print(\"    â€¢ âŒ Download failed\")\n",
    "        fail_count += 1\n",
    "        continue\n",
    "\n",
    "    text = extract(raw, include_comments=False, include_tables=False)\n",
    "    if text:\n",
    "        scraped_contents.append(f\"--- Content from: {url} ---\\n{text}\\n\\n\")\n",
    "        success_count += 1\n",
    "        print(\"    â€¢ âœ… Extracted main text\")\n",
    "    else:\n",
    "        print(\"    â€¢ âš ï¸ No main text found\")\n",
    "        fail_count += 1\n",
    "\n",
    "# --- 4. Save + Metrics ---\n",
    "if scraped_contents:\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(scraped_contents))\n",
    "\n",
    "    total_chars = sum(len(chunk) for chunk in scraped_contents)\n",
    "    total_words = sum(len(chunk.split()) for chunk in scraped_contents)\n",
    "\n",
    "    print(f\"\\nâœ… Saved {success_count} pages to '{OUTPUT_PATH}'.\")\n",
    "    print(\"ðŸ“Š Key Metrics:\")\n",
    "    print(f\"   â€¢ URLs Found    : {len(urls)}\")\n",
    "    print(f\"   â€¢ Successful    : {success_count}\")\n",
    "    print(f\"   â€¢ Failed        : {fail_count}\")\n",
    "    print(f\"   â€¢ Total Chars   : {total_chars}\")\n",
    "    print(f\"   â€¢ Total Words   : {total_words}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No text was scraped; nothing to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main d3fa170] Update: Adding all files to repository\n",
      " 2 files changed, 198 insertions(+), 120 deletions(-)\n",
      " create mode 100644 data_sources/web_scraped_mcp_data.txt\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 5.49 KiB | 5.49 MiB/s, done.\n",
      "Total 5 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   2702bc3..d3fa170  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
