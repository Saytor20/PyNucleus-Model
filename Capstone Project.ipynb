{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìÑ Starting processing for 4 file(s) in 'source_documents' ---\n",
      " ‚ñ∂ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " ‚ñ∂ Processing: mcp_basics.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      " ‚ñ∂ Processing: feasibility_factors.txt\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      " ‚ñ∂ Processing: Bist_Madan.pdf\n",
      "   ‚Ä¢ Success! Saved to: processed_txt_files/Bist_Madan.txt\n",
      "\n",
      "\n",
      " All files processed.\n"
     ]
    }
   ],
   "source": [
    "#----- Date processing for all documents types -----#\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder where you will place all your source files (PDFs, DOCX, TXT, etc.)\n",
    "INPUT_DIR = 'source_documents'\n",
    "\n",
    "# Folder where the processed .txt files will be saved\n",
    "OUTPUT_DIR = 'processed_txt_files'\n",
    "\n",
    "# --- Main Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the input directory if it doesn't exist and give instructions\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        print(f\"üìÇ Creating directory: '{INPUT_DIR}'\")\n",
    "        os.makedirs(INPUT_DIR)\n",
    "        print(f\" Please place your files (PDF, DOCX, TXT, etc.) in the '{INPUT_DIR}' directory and run the script again.\")\n",
    "        exit()\n",
    "\n",
    "    # Create the output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    files_to_process = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f))]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"‚Ñπ The '{INPUT_DIR}' directory is empty. Nothing to process.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"--- üìÑ Starting processing for {len(files_to_process)} file(s) in '{INPUT_DIR}' ---\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        # Skip hidden files like .DS_Store\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_filename = os.path.splitext(os.path.basename(filename))[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "        print(f\" ‚ñ∂ Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Handle PDF files differently\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                # Use PyPDF2 for PDF files\n",
    "                reader = PdfReader(input_path)\n",
    "                full_text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    full_text += page.extract_text() + \"\\n\\n\"\n",
    "            else:\n",
    "                # Use UnstructuredLoader for other file types\n",
    "                loader = UnstructuredLoader(input_path)\n",
    "                documents = loader.load()\n",
    "                full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            # Save the extracted text to a new .txt file\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"   ‚Ä¢ Success! Saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚Ä¢ Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n\\n All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:49:11.411850Z",
     "start_time": "2025-06-04T16:49:11.195022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting Wikipedia article search for 5 keywords...\n",
      "‚ñ∂Ô∏è  Searching for: modular design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_modular_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: software architecture\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_software_architecture.txt\n",
      "‚ñ∂Ô∏è  Searching for: system design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_system_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: industrial design\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_industrial_design.txt\n",
      "‚ñ∂Ô∏è  Searching for: supply chain\n",
      "‚úÖ  Saved article to: data_sources/wikipedia_supply_chain.txt\n",
      "\n",
      "‚ú® Article scraping complete!\n"
     ]
    }
   ],
   "source": [
    "\"--- Wikipedia Data Scraping ---\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Keywords to search for in Wikipedia\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"modular design\",\n",
    "    \"software architecture\",\n",
    "    \"system design\",\n",
    "    \"industrial design\",\n",
    "    \"supply chain\"\n",
    "]\n",
    "\n",
    "# Output directory for saved articles\n",
    "DATA_DIR = \"data_sources\"\n",
    "\n",
    "def search_wikipedia(keyword):\n",
    "    \"\"\"Search Wikipedia for a keyword and return the first result URL\"\"\"\n",
    "    search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={quote(keyword)}&format=json\"\n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['query']['search']:\n",
    "        title = data['query']['search'][0]['title']\n",
    "        return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "    return None\n",
    "\n",
    "def scrape_and_save_article(url, keyword):\n",
    "    \"\"\"Scrape a Wikipedia article and save it as a text file\"\"\"\n",
    "    print(f\"‚ñ∂Ô∏è  Searching for: {keyword}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch the article\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the main content\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        if not content:\n",
    "            print(f\"‚ùå  Could not find article content for: {keyword}\")\n",
    "            return\n",
    "        \n",
    "        # Extract text from paragraphs and headers\n",
    "        article_text = \"\"\n",
    "        for element in content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            text = element.get_text().strip()\n",
    "            if text:\n",
    "                article_text += text + \"\\n\\n\"\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"wikipedia_{keyword.replace(' ', '_')}.txt\"\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(article_text)\n",
    "            \n",
    "        print(f\"‚úÖ  Saved article to: {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå  Error processing {keyword}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"üîç Starting Wikipedia article search for {len(SEARCH_KEYWORDS)} keywords...\")\n",
    "    \n",
    "    for keyword in SEARCH_KEYWORDS:\n",
    "        article_url = search_wikipedia(keyword)\n",
    "        if article_url:\n",
    "            scrape_and_save_article(article_url, keyword)\n",
    "        else:\n",
    "            print(f\"‚ùå  No article found for: {keyword}\")\n",
    "    \n",
    "    print(\"\\n‚ú® Article scraping complete!\")\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 9 documents for chunking\n",
      "Split into 883 chunks\n",
      "\n",
      "‚úÖ Successfully saved chunked data to Chuncked_Data/:\n",
      "  ‚Ä¢ chunked_data_full.json - Complete data with metadata\n",
      "  ‚Ä¢ chunked_data_stats.json - Statistical analysis\n",
      "  ‚Ä¢ chunked_data_content.txt - Human-readable content\n"
     ]
    }
   ],
   "source": [
    "# ---- Document Chunking and Analysis ----#\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_and_chunk_files():\n",
    "    \"\"\"\n",
    "    Load and chunk files from both data_sources and processed_txt_files directories\n",
    "    \"\"\"\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Process files from both directories\n",
    "    directories = ['data_sources', 'processed_txt_files']\n",
    "    \n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"‚ö†Ô∏è Directory {directory} not found\")\n",
    "            continue\n",
    "            \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                        # Create Document object with metadata\n",
    "                        doc = Document(\n",
    "                            page_content=text,\n",
    "                            metadata={\"source\": file_path}\n",
    "                        )\n",
    "                        all_documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(all_documents)} documents for chunking\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunked_documents = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(chunked_documents)} chunks\")\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "def save_chunked_data(chunked_documents, output_dir=\"Chuncked_Data\"):\n",
    "    \"\"\"\n",
    "    Save chunked documents into three separate files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save full content with metadata\n",
    "    full_content = []\n",
    "    for i, chunk in enumerate(chunked_documents):\n",
    "        full_content.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"content\": chunk.page_content,\n",
    "            \"source\": chunk.metadata.get('source', 'N/A'),\n",
    "            \"length\": len(chunk.page_content)\n",
    "        })\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_full.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_content, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Save statistical analysis\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunked_documents),\n",
    "        \"chunk_lengths\": [len(chunk.page_content) for chunk in chunked_documents],\n",
    "        \"sources\": list(set(chunk.metadata.get('source', 'N/A') for chunk in chunked_documents)),\n",
    "        \"generated_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"chunked_data_stats.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # 3. Save content-only version (for easy reading)\n",
    "    with open(os.path.join(output_dir, \"chunked_data_content.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for i, chunk in enumerate(chunked_documents):\n",
    "            f.write(f\"=== Chunk {i+1} ===\\n\")\n",
    "            f.write(f\"Source: {chunk.metadata.get('source', 'N/A')}\\n\")\n",
    "            f.write(f\"Length: {len(chunk.page_content)} characters\\n\")\n",
    "            f.write(\"\\nContent:\\n\")\n",
    "            f.write(chunk.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved chunked data to {output_dir}/:\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_full.json - Complete data with metadata\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_stats.json - Statistical analysis\")\n",
    "    print(f\"  ‚Ä¢ chunked_data_content.txt - Human-readable content\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and chunk the documents\n",
    "    chunked_docs = load_and_chunk_files()\n",
    "    \n",
    "    # Save the chunked data\n",
    "    save_chunked_data(chunked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  FAISS Vector Store (flat-folder, detailed logging, no log-wipe)\n",
    "import os, json, pickle, shutil, torch\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _mkdir_clean(path: str):\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _mkdir_if_missing(path: str):            # <- preserve logs\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _timestamp(): return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "def _now():        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def _logfile(out_dir: str, prefix: str):\n",
    "    _mkdir_if_missing(out_dir)\n",
    "    path = os.path.join(out_dir, f\"{prefix}_{_timestamp()}.txt\")\n",
    "    def write(msg: str, echo=True):\n",
    "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        if echo: print(msg)\n",
    "    return write, path\n",
    "\n",
    "def _load_docs(json_path: str, log) -> List[Document]:\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        log(f\"Loaded {len(data)} documents from {json_path}\")\n",
    "        return [Document(page_content=d[\"content\"],\n",
    "                         metadata={\"source\": d[\"source\"]}) for d in data]\n",
    "    except Exception as e:\n",
    "        log(f\"‚ö†Ô∏è  {e} ‚Äì falling back to 3 dummy docs.\")\n",
    "        dummy = [\n",
    "            (\"Modular chemical plants reduce construction time.\", \"dummy_1\"),\n",
    "            (\"Scalability is a key advantage of modular design.\", \"dummy_2\"),\n",
    "            (\"Challenges include supply-chain coordination.\", \"dummy_3\"),\n",
    "        ]\n",
    "        return [Document(page_content=t, metadata={\"source\": s}) for t, s in dummy]\n",
    "\n",
    "# ---------- manager ----------\n",
    "class FAISSDBManager:\n",
    "    def __init__(self, vec_dir=\"faiss_store\", out_dir=\"vectordb_outputs\"):\n",
    "        _mkdir_clean(vec_dir)      # re-create store each run\n",
    "        self.vec_dir, self.out_dir = vec_dir, out_dir\n",
    "        self.index_path = os.path.join(vec_dir, \"pynucleus_mcp.faiss\")\n",
    "        self.embed_path = os.path.join(vec_dir, \"embeddings.pkl\")\n",
    "        self.log, self.log_path = _logfile(out_dir, \"faiss_analysis\")\n",
    "        self.log(\"=== FAISS VectorDB Analysis ===\")\n",
    "        self.log(f\"Started: {_now()}\")\n",
    "        self.db, self.embeddings = None, None\n",
    "\n",
    "    def _emb(self):\n",
    "        if self.embeddings is None:\n",
    "            dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                model_kwargs={\"device\": dev},\n",
    "                encode_kwargs={\"normalize_embeddings\": True})\n",
    "            self.log(f\"Embedding device ‚Üí {dev}   | dim={len(self.embeddings.embed_query('hi'))}\")\n",
    "        return self.embeddings\n",
    "\n",
    "    def build(self, docs):\n",
    "        emb = self._emb()\n",
    "        self.db = FAISS.from_documents(docs, emb)\n",
    "        self.db.save_local(self.index_path)\n",
    "        pickle.dump(emb, open(self.embed_path, \"wb\"))\n",
    "        self.log(f\"Docs indexed : {len(docs)}\")\n",
    "        self.log(f\"Index file   : {self.index_path}\")\n",
    "        self.log(f\"Embeds .pkl  : {self.embed_path}\")\n",
    "\n",
    "        # List everything in vec_dir so you can verify\n",
    "        self.log(\"\\n-- Files in faiss_store/ --\")\n",
    "        for f in os.listdir(self.vec_dir):\n",
    "            self.log(f\"  ¬∑ {f}\")\n",
    "\n",
    "    def load(self):\n",
    "        if self.db is None:\n",
    "            if self.embeddings is None and os.path.isfile(self.embed_path):\n",
    "                self.embeddings = pickle.load(open(self.embed_path, \"rb\"))\n",
    "            self.db = FAISS.load_local(self.index_path,\n",
    "                                       self.embeddings,\n",
    "                                       allow_dangerous_deserialization=True)\n",
    "\n",
    "    def search(self, q: str, k=3):\n",
    "        self.load()\n",
    "        return self.db.similarity_search_with_score(q, k)\n",
    "\n",
    "    def evaluate(self, gt: Dict[str, str], k=3):\n",
    "        self.log(\"\\n=== Evaluation (Recall@3) ===\")\n",
    "        hits = 0\n",
    "        for q, expect in gt.items():\n",
    "            res = self.search(q, k)\n",
    "            best = res[0][1] if res else float(\"inf\")\n",
    "            good = any(expect in d.page_content or expect == d.metadata[\"source\"] for d, _ in res)\n",
    "            hits += good\n",
    "            self.log(f\"Q: {q[:45]:<45}  {'‚úì' if good else '‚úó'}   top-score={best:.4f}\")\n",
    "        self.log(f\"\\nRecall@{k}: {hits}/{len(gt)}  ‚Üí  {hits/len(gt):.1%}\")\n",
    "\n",
    "# ---------- quick demo ----------\n",
    "GROUND_TRUTH = {\n",
    "    \"advantages of modular chemical plants\": \"dummy_1\",\n",
    "    \"scalability of modular design\": \"dummy_2\",\n",
    "}\n",
    "JSON_PATH = \"Chuncked_Data/chunked_data_full.json\"\n",
    "\n",
    "f_mgr = FAISSDBManager()\n",
    "f_docs = _load_docs(JSON_PATH, f_mgr.log)\n",
    "f_mgr.build(f_docs)\n",
    "f_mgr.evaluate(GROUND_TRUTH)\n",
    "print(f\"\\nFAISS log ‚Üí {f_mgr.log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [],
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
