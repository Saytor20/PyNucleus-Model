{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21621,
     "status": "ok",
     "timestamp": 1748965189264,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "pxJK6GpyVui7",
    "outputId": "5fcaf74f-4292-4847-fb37-57d1c0d9a971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: Saytor20\n",
      "Token: ghp_...\n",
      "Pulling latest changes from GitHub...\n",
      "From https://github.com/Saytor20/PyNucleus-Model\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "Repository is up to date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "#\n",
    "# #--------Google Drive Integration--------#\n",
    "# # from google.colab import drive, userdata\n",
    "# # This gives Colab access to your files in Google Drive.\n",
    "# # drive.mount('/content/drive')\n",
    "# # 'GITHUB_USERNAME' and 'GITHUB_TOKEN' saved as secrets in Colab.\n",
    "# GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "# REPOSITORY_NAME = 'PyNucleus-Model' # Your repository name\n",
    "# NOTEBOOK_DRIVE_PATH = \"/content/drive/MyDrive/PyNucleus Project/Capstone Project.ipynb\"\n",
    "#\n",
    "#\n",
    "# #--------Cursor Integration--------#\n",
    "# # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "#\n",
    "# # Get GitHub credentials from environment variables\n",
    "GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "#\n",
    "# # Print to verify the variables are loaded (remove this in production)\n",
    "print(f\"Username: {GITHUB_USERNAME}\")\n",
    "print(f\"Token: {GITHUB_TOKEN[:4]}...\") # Only print first 4 chars of token for security\n",
    "#\n",
    "# Repository information\n",
    "REPOSITORY_NAME = 'PyNucleus-Model'\n",
    "NOTEBOOK_REPO_FILENAME = \"Capstone Project.ipynb\"\n",
    "LOG_FILENAME = \"update_log.txt\"\n",
    "\n",
    "# Pull latest changes from GitHub\n",
    "print(\"Pulling latest changes from GitHub...\")\n",
    "!git pull https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPOSITORY_NAME}.git main\n",
    "\n",
    "print(\"Repository is up to date!\")\n",
    "\n",
    "# Log start time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\" {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Log Update\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84DXL9QuH0Tx"
   },
   "source": [
    "# **Data Ingestion and Preprocessing for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Processing source documents...\n",
      "--- ðŸ“„ Starting processing for 4 file(s) in 'source_documents' ---\n",
      " â–¶ Processing: Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ Success! Saved to: processed_txt_files/Manuscript Draft_Can Modular Plants Lower African Industrialization Barriers.txt\n",
      " â–¶ Processing: mcp_basics.txt\n",
      "   â€¢ Success! Saved to: processed_txt_files/mcp_basics.txt\n",
      " â–¶ Processing: feasibility_factors.txt\n",
      "   â€¢ Success! Saved to: processed_txt_files/feasibility_factors.txt\n",
      " â–¶ Processing: Bist_Madan.pdf\n",
      "   â€¢ Success! Saved to: processed_txt_files/Bist_Madan.txt\n",
      "\n",
      "\n",
      " All files processed.\n",
      "\n",
      "Step 2: Scraping Wikipedia articles...\n",
      "ðŸ” Starting Wikipedia article search for 5 keywords...\n",
      "â–¶ï¸  Searching for: modular design\n",
      "âœ…  Saved article to: data_sources/wikipedia_modular_design.txt\n",
      "â–¶ï¸  Searching for: software architecture\n",
      "âœ…  Saved article to: data_sources/wikipedia_software_architecture.txt\n",
      "â–¶ï¸  Searching for: system design\n",
      "âœ…  Saved article to: data_sources/wikipedia_system_design.txt\n",
      "â–¶ï¸  Searching for: industrial design\n",
      "âœ…  Saved article to: data_sources/wikipedia_industrial_design.txt\n",
      "â–¶ï¸  Searching for: supply chain\n",
      "âœ…  Saved article to: data_sources/wikipedia_supply_chain.txt\n",
      "\n",
      "âœ¨ Article scraping complete!\n",
      "\n",
      "Step 3: Processing and chunking documents...\n",
      "\n",
      "Step 4: Building and evaluating FAISS vector store...\n",
      "=== FAISS VectorDB Analysis ===\n",
      "Started: 2025-06-09 14:16:24\n",
      "Loaded 1 documents from Chuncked_Data/chunked_data_full.json\n",
      "âš ï¸  string indices must be integers, not 'str' â€“ falling back to 3 dummy docs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammadalmusaiteer/PyNucleus-Model/faiss_manager.py:64: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO: Loading faiss.\n",
      "INFO: Successfully loaded faiss.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding device â†’ cpu   | dim=384\n",
      "Docs indexed : 3\n",
      "Index file   : faiss_store/pynucleus_mcp.faiss\n",
      "Embeds .pkl  : faiss_store/embeddings.pkl\n",
      "\n",
      "-- Files in faiss_store/ --\n",
      "  Â· embeddings.pkl\n",
      "  Â· pynucleus_mcp.faiss\n",
      "\n",
      "=== Evaluation (Recall@3) ===\n",
      "Q: advantages of modular chemical plants          âœ“   top-score=0.4128\n",
      "Q: scalability of modular design                  âœ“   top-score=0.3195\n",
      "\n",
      "Recall@3: 2/2  â†’  100.0%\n",
      "\n",
      "FAISS log â†’ vectordb_outputs/faiss_analysis_20250609_141624.txt\n"
     ]
    }
   ],
   "source": [
    "# Project module imports\n",
    "from document_processor import process_documents\n",
    "from wiki_scraper import scrape_wikipedia_articles\n",
    "from data_processor import load_and_chunk_files, save_chunked_data\n",
    "from faiss_manager import FAISSDBManager, _load_docs\n",
    "from performance_analyzer import PerformanceAnalyzer\n",
    "\n",
    "\n",
    "# Step 1: Process source documents (PDF, DOCX, etc.)\n",
    "print(\"Step 1: Processing source documents...\")\n",
    "process_documents()\n",
    "\n",
    "# Step 2: Scrape Wikipedia articles\n",
    "print(\"\\nStep 2: Scraping Wikipedia articles...\")\n",
    "scrape_wikipedia_articles()\n",
    "\n",
    "# Step 3: Process and chunk all documents\n",
    "print(\"\\nStep 3: Processing and chunking documents...\")\n",
    "chunked_docs = load_and_chunk_files()\n",
    "save_chunked_data(chunked_docs)\n",
    "\n",
    "# Step 4: Build and evaluate the FAISS vector store\n",
    "print(\"\\nStep 4: Building and evaluating FAISS vector store...\")\n",
    "GROUND_TRUTH = {\n",
    "    \"advantages of modular chemical plants\": \"dummy_1\",\n",
    "    \"scalability of modular design\": \"dummy_2\",\n",
    "}\n",
    "JSON_PATH = \"Chuncked_Data/chunked_data_full.json\"\n",
    "\n",
    "f_mgr = FAISSDBManager()\n",
    "f_docs = _load_docs(JSON_PATH, f_mgr.log)\n",
    "f_mgr.build(f_docs)\n",
    "f_mgr.evaluate(GROUND_TRUTH)\n",
    "print(f\"\\nFAISS log â†’ {f_mgr.log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hISFF6TUEB_H"
   },
   "source": [
    "# This is the last cell of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T16:50:23.979205Z",
     "start_time": "2025-06-04T16:50:22.863275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1748965317848,
     "user": {
      "displayName": "Mohammad A.",
      "userId": "15199683412159334052"
     },
     "user_tz": 240
    },
    "id": "tjThfmG7EDzG",
    "outputId": "c0e3a16d-87b8-4a04-ec34-d92e7264e169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 1c09938] Update: Adding all files to repository\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13 files changed, 55 insertions(+), 6289 deletions(-)\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_162222.txt\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_172120.txt\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_172323.txt\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_172445.txt\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_172847.txt\n",
      " delete mode 100644 vectordb_outputs/faiss_analysis_20250604_174006.txt\n",
      " create mode 100644 vectordb_outputs/faiss_analysis_20250609_141624.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 24, done.\n",
      "Counting objects: 100% (24/24), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (13/13), done.\n",
      "Writing objects: 100% (13/13), 19.45 KiB | 6.48 MiB/s, done.\n",
      "Total 13 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\n",
      "remote: Resolving deltas: 100% (6/6), completed with 6 local objects.\u001b[K\n",
      "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
      "remote: \u001b[1;33mwarning\u001b[m: File faiss_store/embeddings.pkl is 87.16 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
      "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
      "To https://github.com/Saytor20/PyNucleus-Model.git\n",
      "   c12419f..1c09938  main -> main\n",
      "All files pushed to GitHub successfully!\n"
     ]
    }
   ],
   "source": [
    "# Log end time\n",
    "with open(\"update_log.txt\", \"a\") as f:\n",
    "    f.write(f\"\\n {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} changes made and pushed to origin main\\n\")\n",
    "\n",
    "# Simple GitHub update function\n",
    "def update_github():\n",
    "    !git add .\n",
    "    !git commit -m \"Update: Adding all files to repository\"\n",
    "    !git push origin main\n",
    "    print(\"All files pushed to GitHub successfully!\")\n",
    "\n",
    "# To use it, just run:\n",
    "update_github()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
