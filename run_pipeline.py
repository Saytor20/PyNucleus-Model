#!/usr/bin/env python3
"""
PyNucleus Pipeline CLI

Command-line interface for the PyNucleus chemical process simulation and RAG system.
Provides commands to run the full pipeline, query LLMs, and test various components.

Usage:
    python run_pipeline.py run --config-path <config.json> --output-dir <output>
    python run_pipeline.py pipeline-and-ask --config-path <config.json> --question "..."
"""

############################################################
# 4 â€” Typer CLI entrypoint
############################################################
# Dependency: pip install typer[all]

from pathlib import Path
import sys
import typer
from typing import Optional
import glob
import os
from datetime import datetime

# Add src to Python path
src_path = str(Path(__file__).parent / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

from pynucleus.integration.config_manager import ConfigManager
from pynucleus.pipeline.pipeline_utils import run_full_pipeline
from pynucleus.utils.logging_config import configure_logging, get_logger, log_system_info
from pynucleus.llm.query_llm import LLMQueryManager

app = typer.Typer(
    add_completion=False, 
    pretty_exceptions_enable=False,
    help="PyNucleus Pipeline CLI - RAG + DWSIM Chemical Process Simulation"
)

def find_latest_report(output_dir: Path) -> Optional[Path]:
    """
    Find the latest generated report file in the LLM reports directory.
    
    Args:
        output_dir: Base output directory
        
    Returns:
        Path to the latest report file, or None if no reports found
    """
    llm_reports_dir = output_dir / "llm_reports"
    
    if not llm_reports_dir.exists():
        return None
    
    # Look for markdown and text files (common report formats)
    patterns = ["*.md", "*.txt"]
    report_files = []
    
    for pattern in patterns:
        report_files.extend(llm_reports_dir.glob(pattern))
    
    if not report_files:
        return None
    
    # Return the most recently modified file
    latest_file = max(report_files, key=lambda f: f.stat().st_mtime)
    return latest_file

@app.command("run")
def run_pipeline(
    config_path: Path = typer.Option(..., "--config-path", help="JSON/CSV config file"),
    output_dir: Path = typer.Option("data/05_output", "--output-dir", help="Save location"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    log_file: Optional[Path] = typer.Option(None, "--log-file", help="Custom log file path (default: logs/pipeline.log)"),
):
    """Execute the full PyNucleus pipeline from the command line."""
    
    # Initialize robust logging configuration
    try:
        # Setup logging with enhanced configuration
        logger = configure_logging(
            level="DEBUG" if verbose else "INFO",
            log_file=log_file
        )
        
        # Get CLI-specific logger
        cli_logger = get_logger(__name__)
        
        # Log startup information
        cli_logger.info("ğŸš€ PyNucleus CLI started")
        cli_logger.info(f"ğŸ“‹ Configuration: {config_path}")
        cli_logger.info(f"ğŸ“ Output directory: {output_dir}")
        cli_logger.info(f"ğŸ”§ Verbose mode: {'ON' if verbose else 'OFF'}")
        
        if verbose:
            log_system_info(cli_logger)
        
        # Validate inputs
        if not config_path.exists():
            cli_logger.error(f"âŒ Configuration file not found: {config_path}")
            raise typer.Exit(1)
        
        # Load configuration
        cli_logger.info("âš™ï¸ Loading configuration...")
        cfg_mgr = ConfigManager(config_dir=config_path.parent)
        settings = cfg_mgr.load(config_path.name)
        cli_logger.info("âœ… Configuration loaded successfully")
        
        # Ensure output directory exists
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        cli_logger.info(f"ğŸ“‚ Output directory prepared: {output_dir}")
        
        # Run the pipeline
        cli_logger.info("ğŸ”„ Starting pipeline execution...")
        result = run_full_pipeline(settings=settings, output_dir=output_dir)
        
        if result:
            cli_logger.info("ğŸ‰ Pipeline completed successfully!")
            cli_logger.info(f"ğŸ“Š Results: {result.get('summary', 'N/A')}")
        else:
            cli_logger.warning("âš ï¸ Pipeline completed with warnings")
            
    except KeyboardInterrupt:
        cli_logger.info("â¹ï¸ Pipeline interrupted by user")
        raise typer.Exit(130)  # Standard exit code for Ctrl+C
        
    except Exception as e:
        # Use logger if available, otherwise print to stderr
        try:
            cli_logger.error(f"âŒ Pipeline failed: {str(e)}")
            if verbose:
                cli_logger.exception("Full error details:")
        except:
            print(f"âŒ Pipeline failed: {str(e)}", file=sys.stderr)
        
        raise typer.Exit(1)

@app.command("pipeline-and-ask")
def pipeline_and_ask(
    config_path: Path = typer.Option(..., "--config-path", help="JSON/CSV config file"),
    question: str = typer.Option(..., "--question", help="Question to ask the LLM about the report"),
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID"),
    output_dir: Path = typer.Option("data/05_output", "--output-dir", help="Save location"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    log_file: Optional[Path] = typer.Option(None, "--log-file", help="Custom log file path"),
):
    """Run the pipeline and then query an LLM about the latest generated report."""
    
    try:
        # Setup logging
        logger = configure_logging(
            level="DEBUG" if verbose else "INFO",
            log_file=log_file
        )
        
        cli_logger = get_logger(__name__)
        
        cli_logger.info("ğŸš€ Starting Pipeline-and-Ask command")
        cli_logger.info(f"ğŸ“‹ Configuration: {config_path}")
        cli_logger.info(f"â“ Question: {question}")
        cli_logger.info(f"ğŸ¤– Model ID: {model_id}")
        cli_logger.info(f"ğŸ“ Output directory: {output_dir}")
        
        # Validate inputs
        if not config_path.exists():
            cli_logger.error(f"âŒ Configuration file not found: {config_path}")
            raise typer.Exit(1)
        
        # Step 1: Run the pipeline
        print("=" * 60)
        print("ğŸ”„ STEP 1: Running PyNucleus Pipeline")
        print("=" * 60)
        
        # Load configuration
        cli_logger.info("âš™ï¸ Loading configuration...")
        cfg_mgr = ConfigManager(config_dir=config_path.parent)
        settings = cfg_mgr.load(config_path.name)
        cli_logger.info("âœ… Configuration loaded successfully")
        
        # Ensure output directory exists
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Run the pipeline
        pipeline_result = run_full_pipeline(settings=settings, output_dir=output_dir)
        
        if not pipeline_result or not pipeline_result.get('success', True):
            cli_logger.error("âŒ Pipeline execution failed")
            print("âŒ Pipeline execution failed. Cannot proceed to LLM query.")
            raise typer.Exit(1)
        
        print("âœ… Pipeline completed successfully!")
        
        # Step 2: Find the latest report
        print("\n" + "=" * 60)
        print("ğŸ” STEP 2: Finding Latest Report")
        print("=" * 60)
        
        latest_report = find_latest_report(output_dir)
        
        if not latest_report:
            cli_logger.error("âŒ No reports found in the output directory")
            print(f"âŒ No reports found in {output_dir}/llm_reports/")
            print("ğŸ’¡ Make sure the pipeline generates LLM reports before querying.")
            raise typer.Exit(1)
        
        print(f"âœ… Found latest report: {latest_report}")
        cli_logger.info(f"ğŸ“„ Using report: {latest_report}")
        
        # Step 3: Query the LLM
        print("\n" + "=" * 60)
        print("ğŸ¤– STEP 3: Querying LLM")
        print("=" * 60)
        
        print(f"ğŸ¤– Initializing LLM with model: {model_id}")
        print(f"â“ Question: {question}")
        print(f"ğŸ“„ Report: {latest_report.name}")
        
        # Initialize LLM Query Manager
        try:
            llm_manager = LLMQueryManager(
                model_id=model_id,
                device="cpu",  # Default to CPU for compatibility
                max_tokens=8192
            )
            
            cli_logger.info(f"ğŸ¤– LLM Manager initialized with model: {model_id}")
            
            # Query the LLM
            print("\nğŸ”„ Generating response...")
            response = llm_manager.ask_llm(
                user_query=question,
                report_file_path=latest_report,
                system_message="You are an expert chemical process engineer analyzing simulation reports.",
                generation_params={
                    'max_length': 500,  # Reasonable response length
                    'temperature': 0.7,
                    'do_sample': True,
                    'top_p': 0.9
                }
            )
            
            # Display results
            print("\n" + "=" * 60)
            print("ğŸ“‹ LLM RESPONSE")
            print("=" * 60)
            print(f"Question: {question}")
            print(f"Report: {latest_report.name}")
            print(f"Model: {model_id}")
            print("-" * 60)
            print(response)
            print("=" * 60)
            
            cli_logger.info("âœ… LLM query completed successfully")
            
        except Exception as e:
            cli_logger.error(f"âŒ LLM query failed: {str(e)}")
            print(f"âŒ Failed to query LLM: {str(e)}")
            if verbose:
                import traceback
                traceback.print_exc()
            raise typer.Exit(1)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Operation interrupted by user")
        raise typer.Exit(130)
        
    except Exception as e:
        print(f"âŒ Command failed: {str(e)}")
        if 'cli_logger' in locals():
            cli_logger.error(f"âŒ Command failed: {str(e)}")
        raise typer.Exit(1)

@app.command("test-logging")
def test_logging():
    """Test the logging configuration."""
    
    # Setup logging in test mode
    logger = configure_logging(level="DEBUG")
    cli_logger = get_logger("test")
    
    print("ğŸ§ª Testing logging configuration...")
    
    # Test different log levels
    cli_logger.debug("ğŸ” This is a DEBUG message")
    cli_logger.info("ğŸ“‹ This is an INFO message") 
    cli_logger.warning("âš ï¸ This is a WARNING message")
    cli_logger.error("âŒ This is an ERROR message")
    
    # Log system info
    log_system_info(cli_logger)
    
    # Check log file
    log_file = Path("logs/pipeline.log")
    if log_file.exists():
        cli_logger.info(f"âœ… Log file created: {log_file}")
        cli_logger.info(f"ğŸ“ Log file size: {log_file.stat().st_size} bytes")
    else:
        cli_logger.error("âŒ Log file not found")
    
    print("âœ… Logging test completed!")

@app.command("ingest")
def ingest_documents(
    source_dir: Path = typer.Option(..., "--source-dir", help="Source directory containing documents to ingest"),
    output_dir: Path = typer.Option("data/03_intermediate", "--output-dir", help="Output directory for processed documents"),
    backend: str = typer.Option("chroma", "--backend", help="Vector store backend (chroma|faiss|qdrant)"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Ingest and process documents for RAG system."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("ğŸš€ Starting document ingestion")
        cli_logger.info(f"ğŸ“ Source directory: {source_dir}")
        cli_logger.info(f"ğŸ“ Output directory: {output_dir}")
        
        # Validate source directory
        if not source_dir.exists():
            cli_logger.error(f"âŒ Source directory not found: {source_dir}")
            raise typer.Exit(1)
        
        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Validate backend selection
        if backend not in ['chroma', 'faiss', 'qdrant']:
            cli_logger.error(f"âŒ Invalid backend: {backend}. Must be 'chroma', 'faiss', or 'qdrant'")
            raise typer.Exit(1)
        
        cli_logger.info(f"ğŸ”§ Using vector store backend: {backend}")
        
        if backend == 'qdrant':
            cli_logger.info("âš ï¸  Qdrant stub -- not yet enabled")
            print("Qdrant stub -- not yet enabled")
            return
        
        # Initialize RAG core
        from pynucleus.rag import RAGCore
        rag_core = RAGCore(data_dir=output_dir.parent)
        
        # Process documents using real ChromaDB ingestion
        cli_logger.info("ğŸ”„ Processing documents...")
        
        if backend == 'chroma':
            # Use the actual ChromaDB ingestion function
            from pynucleus.rag.collector import ingest
            ingest(source_dir=str(source_dir))
            cli_logger.info("âœ… Successfully ingested documents into ChromaDB")
            print(f"ğŸ“Š Documents ingested into ChromaDB collection: pynucleus_documents")
        else:
            # Fallback to RAG core for other backends
            result = rag_core.process_documents(source_dir=str(source_dir))
            if result["status"] == "success":
                cli_logger.info(f"âœ… Successfully processed {result['processed_count']} documents")
                print(f"ğŸ“Š Processed {result['processed_count']} out of {result['total_files']} files")
            else:
                cli_logger.error(f"âŒ Document processing failed: {result['message']}")
                raise typer.Exit(1)
            
    except Exception as e:
        cli_logger.error(f"âŒ Ingestion failed: {str(e)}")
        print(f"âŒ Error: {str(e)}")
        raise typer.Exit(1)

@app.command("build-faiss")  
def build_faiss_index(
    chunk_dir: Path = typer.Option("data/03_intermediate/converted_chunked_data", "--chunk-dir", help="Directory containing chunk data"),
    index_dir: Path = typer.Option("data/04_models/chunk_reports", "--index-dir", help="Output directory for FAISS index"),
    force_rebuild: bool = typer.Option(False, "--force-rebuild", help="Force rebuild even if index exists"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Build FAISS vector index from processed chunks."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("ğŸš€ Starting FAISS index building")
        cli_logger.info(f"ğŸ“ Chunk directory: {chunk_dir}")
        cli_logger.info(f"ğŸ“ Index directory: {index_dir}")
        cli_logger.info(f"ğŸ”„ Force rebuild: {force_rebuild}")
        
        # Validate chunk directory
        if not chunk_dir.exists():
            cli_logger.error(f"âŒ Chunk directory not found: {chunk_dir}")
            raise typer.Exit(1)
        
        # Create index directory
        index_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize RAG core
        from pynucleus.rag import RAGCore
        rag_core = RAGCore(data_dir=index_dir.parent.parent)
        
        # Build index
        cli_logger.info("ğŸ”„ Building FAISS index...")
        result = rag_core.build_index(force_rebuild=force_rebuild)
        
        if result["status"] == "success":
            cli_logger.info(f"âœ… Successfully built FAISS index")
            print(f"ğŸ“Š Index size: {result.get('index_size', 'Unknown')}")
            print(f"ğŸ“ Dimensions: {result.get('dimensions', 'Unknown')}")
            print(f"ğŸ“„ Chunks indexed: {result.get('chunks_indexed', 'Unknown')}")
            print(f"ğŸ“ Index saved to: {index_dir}")
        elif result["status"] == "exists":
            print(f"â„¹ï¸  Index already exists: {result['message']}")
            print(f"ğŸ“ Existing indices: {result.get('existing_indices', [])}")
        else:
            cli_logger.error(f"âŒ Index building failed: {result['message']}")
            raise typer.Exit(1)
            
    except Exception as e:
        cli_logger.error(f"âŒ FAISS index building failed: {str(e)}")
        print(f"âŒ Error: {str(e)}")
        raise typer.Exit(1)

@app.command("ask")
def ask_question(
    question: str = typer.Option(..., "--question", help="Question to ask the RAG system"),
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID for enhanced responses"),
    top_k: int = typer.Option(5, "--top-k", help="Number of top results to retrieve"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Ask a question to the RAG system."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("ğŸš€ Starting RAG query")
        cli_logger.info(f"â“ Question: {question}")
        cli_logger.info(f"ğŸ¤– Model ID: {model_id}")
        cli_logger.info(f"ğŸ“Š Top K: {top_k}")
        
        # DSPy components removed, using RAG system directly
        cli_logger.info("ğŸ”„ Using RAG system for query processing")
        
        # Initialize RAG pipeline
        from pynucleus.pipeline.pipeline_rag import RAGPipeline
        rag_pipeline = RAGPipeline(data_dir="data")
        
        # Query the RAG system
        cli_logger.info("ğŸ”„ Processing question...")
        result = rag_pipeline.query(question, top_k=top_k)
        
        # Display results
        print("=" * 60)
        print("ğŸ“‹ RAG SYSTEM RESPONSE")
        print("=" * 60)
        print(f"Question: {question}")
        print(f"Confidence: {result.get('confidence', 0):.2f}")
        print("-" * 60)
        print(f"Answer: {result.get('answer', 'No answer available')}")
        print("-" * 60)
        print("Sources:")
        for i, source in enumerate(result.get('sources', []), 1):
            print(f"  {i}. {source}")
        print("=" * 60)
        
        # Always use LLM for enhanced response
        try:
            from pynucleus.llm.llm_runner import LLMRunner
            llm_runner = LLMRunner(model_id=model_id)
            
            cli_logger.info(f"ğŸ¤– Generating enhanced response with {model_id}")
            
            # Create enhanced prompt with context using Guidance integration
            from pynucleus.llm.prompting import build_prompt
            context = result.get('answer', '')
            enhanced_prompt = build_prompt(context, question)
            
            llm_response = llm_runner.ask(
                question=enhanced_prompt,
                max_length=500,
                temperature=0.7
            )
            
            print("\nğŸ¤– ENHANCED LLM RESPONSE")
            print("-" * 60)
            print(llm_response)
            print("=" * 60)
            
        except Exception as llm_error:
            cli_logger.error(f"âŒ LLM processing failed: {llm_error}")
            print(f"\nâŒ LLM processing failed: {llm_error}")
            print("ğŸ”„ Falling back to RAG-only response")
                
        cli_logger.info("âœ… Question processed successfully")
        
    except Exception as e:
        cli_logger.error(f"âŒ Question processing failed: {str(e)}")
        print(f"âŒ Error: {str(e)}")
        raise typer.Exit(1)

# DSPy command removed - DSPy functionality disabled to avoid API dependencies
# Use the regular 'ask' or 'chat' commands instead for local LLM processing

@app.command("chat")
def interactive_chat(
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID for responses"),
    top_k: int = typer.Option(5, "--top-k", help="Number of top results to retrieve from RAG"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    use_dspy: bool = typer.Option(False, "--use-dspy/--no-dspy", help="DSPy disabled - using local RAG+LLM instead"),
):
    """Start an interactive chat session with the LLM."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("ğŸš€ Starting interactive chat session")
        cli_logger.info(f"ğŸ¤– Model ID: {model_id}")
        cli_logger.info(f"ğŸ“Š Top K: {top_k}")
        # DSPy functionality has been removed to avoid API dependencies
        # Using local RAG + LLM processing only
        
        # Initialize systems
        rag_pipeline = None
        llm_runner = None
        
        # Initialize RAG pipeline and LLM
        try:
            from pynucleus.pipeline.pipeline_rag import RAGPipeline
            from pynucleus.llm.llm_runner import LLMRunner
            
            cli_logger.info("ğŸ”§ Initializing RAG pipeline and LLM runner")
            rag_pipeline = RAGPipeline(data_dir="data")
            llm_runner = LLMRunner(model_id=model_id)
            print("âœ… RAG + LLM mode enabled")
        except Exception as e:
            cli_logger.error(f"âŒ Failed to initialize RAG/LLM systems: {e}")
            print(f"âŒ Failed to initialize systems: {e}")
            raise typer.Exit(1)
        
        # Start interactive session
        print("\n" + "=" * 60)
        print("ğŸ¤– PYNUCLEUS INTERACTIVE CHAT")
        print("=" * 60)
        print("ğŸ’¡ Ask questions about chemical processes, simulations, or technical topics")
        print("ğŸ“ Type 'quit', 'exit', or press Ctrl+C to end the session")
        print("ğŸ”„ Type 'clear' to clear the screen")
        print("â“ Type 'help' for more commands")
        print("=" * 60)
        
        question_count = 0
        
        while True:
            try:
                # Get user input
                print(f"\n[Q{question_count + 1}]", end=" ")
                question = input("ğŸ’­ Your question: ").strip()
                
                # Handle special commands
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ Goodbye! Chat session ended.")
                    break
                elif question.lower() == 'clear':
                    os.system('clear' if os.name == 'posix' else 'cls')
                    continue
                elif question.lower() == 'help':
                    print("\nğŸ“‹ Available Commands:")
                    print("  â€¢ quit/exit/q  - End chat session")
                    print("  â€¢ clear        - Clear screen")
                    print("  â€¢ help         - Show this help")
                    print("  â€¢ Any question - Ask the AI system")
                    continue
                elif not question:
                    print("âŒ Please enter a question or command")
                    continue
                
                question_count += 1
                
                print(f"\nğŸ”„ Processing question {question_count}...")
                start_time = datetime.now()
                
                # Process with RAG + LLM
                try:
                    # Get RAG response
                    rag_result = rag_pipeline.query(question, top_k=top_k)
                    
                    # Get enhanced LLM response using Guidance integration
                    from pynucleus.llm.prompting import build_prompt
                    context = rag_result.get('answer', '')
                    enhanced_prompt = build_prompt(context, question)
                    
                    llm_response = llm_runner.ask(
                        question=enhanced_prompt,
                        max_length=500,
                        temperature=0.7
                    )
                    
                    # Display results
                    print("\n" + "ğŸ“‹ " + "=" * 58)
                    print(f"Answer: {llm_response}")
                    print("=" * 60)
                    
                    # Show sources if available
                    sources = rag_result.get('sources', [])
                    if sources:
                        print("ğŸ“š Sources:")
                        for i, source in enumerate(sources[:3], 1):  # Show top 3 sources
                            print(f"  {i}. {source}")
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    print(f"â±ï¸  Response time: {duration:.2f}s")
                    print(f"ğŸ¤– Model: {model_id}")
                    print(f"ğŸ“Š Confidence: {rag_result.get('confidence', 0):.2f}")
                    
                except Exception as e:
                    cli_logger.error(f"âŒ RAG/LLM processing failed: {e}")
                    print(f"âŒ Processing failed: {e}")
                    continue
                
            except KeyboardInterrupt:
                print("\n\nâ¹ï¸ Chat session interrupted by user")
                break
            except EOFError:
                print("\n\nğŸ‘‹ Chat session ended")
                break
            except Exception as e:
                cli_logger.error(f"âŒ Unexpected error: {e}")
                print(f"âŒ Unexpected error: {e}")
                continue
        
        # Session summary
        print(f"\nğŸ“Š Chat session summary:")
        print(f"  â€¢ Questions asked: {question_count}")
        print(f"  â€¢ System used: RAG + LLM")
        print(f"  â€¢ Model: {model_id}")
        
        cli_logger.info(f"âœ… Chat session ended. Questions processed: {question_count}")
        
    except Exception as e:
        cli_logger.error(f"âŒ Chat session failed: {str(e)}")
        print(f"âŒ Error starting chat: {str(e)}")
        raise typer.Exit(1)

if __name__ == "__main__":
    app() 