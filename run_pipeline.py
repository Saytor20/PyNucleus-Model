#!/usr/bin/env python3
"""
PyNucleus Pipeline CLI

Command-line interface for the PyNucleus chemical process simulation and RAG system.
Provides commands to run the full pipeline, query LLMs, and test various components.

Usage:
    python run_pipeline.py run --config-path <config.json> --output-dir <output>
    python run_pipeline.py pipeline-and-ask --config-path <config.json> --question "..."
"""

############################################################
# 4 ‚Äî Typer CLI entrypoint
############################################################
# Dependency: pip install typer[all]

from pathlib import Path
import sys
import typer
from typing import Optional
import glob
import os
from datetime import datetime

# Add src to Python path
src_path = str(Path(__file__).parent / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

from pynucleus.integration.config_manager import ConfigManager
from pynucleus.pipeline.pipeline_utils import run_full_pipeline
from pynucleus.utils.logging_config import configure_logging, get_logger, log_system_info
from pynucleus.llm.query_llm import LLMQueryManager

app = typer.Typer(
    add_completion=False, 
    pretty_exceptions_enable=False,
    help="PyNucleus Pipeline CLI - RAG + DWSIM Chemical Process Simulation"
)

def find_latest_report(output_dir: Path) -> Optional[Path]:
    """
    Find the latest generated report file in the LLM reports directory.
    
    Args:
        output_dir: Base output directory
        
    Returns:
        Path to the latest report file, or None if no reports found
    """
    llm_reports_dir = output_dir / "llm_reports"
    
    if not llm_reports_dir.exists():
        return None
    
    # Look for markdown and text files (common report formats)
    patterns = ["*.md", "*.txt"]
    report_files = []
    
    for pattern in patterns:
        report_files.extend(llm_reports_dir.glob(pattern))
    
    if not report_files:
        return None
    
    # Return the most recently modified file
    latest_file = max(report_files, key=lambda f: f.stat().st_mtime)
    return latest_file

@app.command("run")
def run_pipeline(
    config_path: Path = typer.Option(..., "--config-path", help="JSON/CSV config file"),
    output_dir: Path = typer.Option("data/05_output", "--output-dir", help="Save location"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    log_file: Optional[Path] = typer.Option(None, "--log-file", help="Custom log file path (default: logs/pipeline.log)"),
):
    """Execute the full PyNucleus pipeline from the command line."""
    
    # Initialize robust logging configuration
    try:
        # Setup logging with enhanced configuration
        logger = configure_logging(
            level="DEBUG" if verbose else "INFO",
            log_file=log_file
        )
        
        # Get CLI-specific logger
        cli_logger = get_logger(__name__)
        
        # Log startup information
        cli_logger.info("üöÄ PyNucleus CLI started")
        cli_logger.info(f"üìã Configuration: {config_path}")
        cli_logger.info(f"üìÅ Output directory: {output_dir}")
        cli_logger.info(f"üîß Verbose mode: {'ON' if verbose else 'OFF'}")
        
        if verbose:
            log_system_info(cli_logger)
        
        # Validate inputs
        if not config_path.exists():
            cli_logger.error(f"‚ùå Configuration file not found: {config_path}")
            raise typer.Exit(1)
        
        # Load configuration
        cli_logger.info("‚öôÔ∏è Loading configuration...")
        cfg_mgr = ConfigManager(config_dir=config_path.parent)
        settings = cfg_mgr.load(config_path.name)
        cli_logger.info("‚úÖ Configuration loaded successfully")
        
        # Ensure output directory exists
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        cli_logger.info(f"üìÇ Output directory prepared: {output_dir}")
        
        # Run the pipeline
        cli_logger.info("üîÑ Starting pipeline execution...")
        result = run_full_pipeline(settings=settings, output_dir=output_dir)
        
        if result:
            cli_logger.info("üéâ Pipeline completed successfully!")
            cli_logger.info(f"üìä Results: {result.get('summary', 'N/A')}")
        else:
            cli_logger.warning("‚ö†Ô∏è Pipeline completed with warnings")
            
    except KeyboardInterrupt:
        cli_logger.info("‚èπÔ∏è Pipeline interrupted by user")
        raise typer.Exit(130)  # Standard exit code for Ctrl+C
        
    except Exception as e:
        # Use logger if available, otherwise print to stderr
        try:
            cli_logger.error(f"‚ùå Pipeline failed: {str(e)}")
            if verbose:
                cli_logger.exception("Full error details:")
        except:
            print(f"‚ùå Pipeline failed: {str(e)}", file=sys.stderr)
        
        raise typer.Exit(1)

@app.command("pipeline-and-ask")
def pipeline_and_ask(
    config_path: Path = typer.Option(..., "--config-path", help="JSON/CSV config file"),
    question: str = typer.Option(..., "--question", help="Question to ask the LLM about the report"),
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID"),
    output_dir: Path = typer.Option("data/05_output", "--output-dir", help="Save location"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    log_file: Optional[Path] = typer.Option(None, "--log-file", help="Custom log file path"),
):
    """Run the pipeline and then query an LLM about the latest generated report."""
    
    try:
        # Setup logging
        logger = configure_logging(
            level="DEBUG" if verbose else "INFO",
            log_file=log_file
        )
        
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üöÄ Starting Pipeline-and-Ask command")
        cli_logger.info(f"üìã Configuration: {config_path}")
        cli_logger.info(f"‚ùì Question: {question}")
        cli_logger.info(f"ü§ñ Model ID: {model_id}")
        cli_logger.info(f"üìÅ Output directory: {output_dir}")
        
        # Validate inputs
        if not config_path.exists():
            cli_logger.error(f"‚ùå Configuration file not found: {config_path}")
            raise typer.Exit(1)
        
        # Step 1: Run the pipeline
        print("=" * 60)
        print("üîÑ STEP 1: Running PyNucleus Pipeline")
        print("=" * 60)
        
        # Load configuration
        cli_logger.info("‚öôÔ∏è Loading configuration...")
        cfg_mgr = ConfigManager(config_dir=config_path.parent)
        settings = cfg_mgr.load(config_path.name)
        cli_logger.info("‚úÖ Configuration loaded successfully")
        
        # Ensure output directory exists
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Run the pipeline
        pipeline_result = run_full_pipeline(settings=settings, output_dir=output_dir)
        
        if not pipeline_result or not pipeline_result.get('success', True):
            cli_logger.error("‚ùå Pipeline execution failed")
            print("‚ùå Pipeline execution failed. Cannot proceed to LLM query.")
            raise typer.Exit(1)
        
        print("‚úÖ Pipeline completed successfully!")
        
        # Step 2: Find the latest report
        print("\n" + "=" * 60)
        print("üîç STEP 2: Finding Latest Report")
        print("=" * 60)
        
        latest_report = find_latest_report(output_dir)
        
        if not latest_report:
            cli_logger.error("‚ùå No reports found in the output directory")
            print(f"‚ùå No reports found in {output_dir}/llm_reports/")
            print("üí° Make sure the pipeline generates LLM reports before querying.")
            raise typer.Exit(1)
        
        print(f"‚úÖ Found latest report: {latest_report}")
        cli_logger.info(f"üìÑ Using report: {latest_report}")
        
        # Step 3: Query the LLM
        print("\n" + "=" * 60)
        print("ü§ñ STEP 3: Querying LLM")
        print("=" * 60)
        
        print(f"ü§ñ Initializing LLM with model: {model_id}")
        print(f"‚ùì Question: {question}")
        print(f"üìÑ Report: {latest_report.name}")
        
        # Initialize LLM Query Manager
        try:
            llm_manager = LLMQueryManager(
                model_id=model_id,
                device="cpu",  # Default to CPU for compatibility
                max_tokens=8192
            )
            
            cli_logger.info(f"ü§ñ LLM Manager initialized with model: {model_id}")
            
            # Query the LLM
            print("\nüîÑ Generating response...")
            response = llm_manager.ask_llm(
                user_query=question,
                report_file_path=latest_report,
                system_message="You are an expert chemical process engineer analyzing simulation reports.",
                generation_params={
                    'max_length': 500,  # Reasonable response length
                    'temperature': 0.7,
                    'do_sample': True,
                    'top_p': 0.9
                }
            )
            
            # Display results
            print("\n" + "=" * 60)
            print("üìã LLM RESPONSE")
            print("=" * 60)
            print(f"Question: {question}")
            print(f"Report: {latest_report.name}")
            print(f"Model: {model_id}")
            print("-" * 60)
            print(response)
            print("=" * 60)
            
            cli_logger.info("‚úÖ LLM query completed successfully")
            
        except Exception as e:
            cli_logger.error(f"‚ùå LLM query failed: {str(e)}")
            print(f"‚ùå Failed to query LLM: {str(e)}")
            if verbose:
                import traceback
                traceback.print_exc()
            raise typer.Exit(1)
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Operation interrupted by user")
        raise typer.Exit(130)
        
    except Exception as e:
        print(f"‚ùå Command failed: {str(e)}")
        if 'cli_logger' in locals():
            cli_logger.error(f"‚ùå Command failed: {str(e)}")
        raise typer.Exit(1)

@app.command("test-logging")
def test_logging():
    """Test the logging configuration."""
    
    # Setup logging in test mode
    logger = configure_logging(level="DEBUG")
    cli_logger = get_logger("test")
    
    print("üß™ Testing logging configuration...")
    
    # Test different log levels
    cli_logger.debug("üîç This is a DEBUG message")
    cli_logger.info("üìã This is an INFO message") 
    cli_logger.warning("‚ö†Ô∏è This is a WARNING message")
    cli_logger.error("‚ùå This is an ERROR message")
    
    # Log system info
    log_system_info(cli_logger)
    
    # Check log file
    log_file = Path("logs/pipeline.log")
    if log_file.exists():
        cli_logger.info(f"‚úÖ Log file created: {log_file}")
        cli_logger.info(f"üìè Log file size: {log_file.stat().st_size} bytes")
    else:
        cli_logger.error("‚ùå Log file not found")
    
    print("‚úÖ Logging test completed!")

@app.command("ingest")
def ingest_documents(
    source_dir: Path = typer.Option(..., "--source-dir", help="Source directory containing documents to ingest"),
    output_dir: Path = typer.Option("data/03_intermediate", "--output-dir", help="Output directory for processed documents"),
    backend: str = typer.Option("faiss", "--backend", help="Vector store backend (faiss|qdrant)"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Ingest and process documents for RAG system."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üöÄ Starting document ingestion")
        cli_logger.info(f"üìÅ Source directory: {source_dir}")
        cli_logger.info(f"üìÅ Output directory: {output_dir}")
        
        # Validate source directory
        if not source_dir.exists():
            cli_logger.error(f"‚ùå Source directory not found: {source_dir}")
            raise typer.Exit(1)
        
        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Validate backend selection
        if backend not in ['faiss', 'qdrant']:
            cli_logger.error(f"‚ùå Invalid backend: {backend}. Must be 'faiss' or 'qdrant'")
            raise typer.Exit(1)
        
        cli_logger.info(f"üîß Using vector store backend: {backend}")
        
        if backend == 'qdrant':
            cli_logger.info("‚ö†Ô∏è  Qdrant stub -- not yet enabled")
            print("Qdrant stub -- not yet enabled")
            return
        
        # Initialize RAG core
        from pynucleus.rag import RAGCore
        rag_core = RAGCore(data_dir=output_dir.parent)
        
        # Process documents
        cli_logger.info("üîÑ Processing documents...")
        result = rag_core.process_documents(source_dir=str(source_dir))
        
        if result["status"] == "success":
            cli_logger.info(f"‚úÖ Successfully processed {result['processed_count']} documents")
            print(f"üìä Processed {result['processed_count']} out of {result['total_files']} files")
            print(f"üìÅ Results saved to: {output_dir}")
        else:
            cli_logger.error(f"‚ùå Document processing failed: {result['message']}")
            raise typer.Exit(1)
            
    except Exception as e:
        cli_logger.error(f"‚ùå Ingestion failed: {str(e)}")
        print(f"‚ùå Error: {str(e)}")
        raise typer.Exit(1)

@app.command("build-faiss")  
def build_faiss_index(
    chunk_dir: Path = typer.Option("data/03_intermediate/converted_chunked_data", "--chunk-dir", help="Directory containing chunk data"),
    index_dir: Path = typer.Option("data/04_models/chunk_reports", "--index-dir", help="Output directory for FAISS index"),
    force_rebuild: bool = typer.Option(False, "--force-rebuild", help="Force rebuild even if index exists"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Build FAISS vector index from processed chunks."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üöÄ Starting FAISS index building")
        cli_logger.info(f"üìÅ Chunk directory: {chunk_dir}")
        cli_logger.info(f"üìÅ Index directory: {index_dir}")
        cli_logger.info(f"üîÑ Force rebuild: {force_rebuild}")
        
        # Validate chunk directory
        if not chunk_dir.exists():
            cli_logger.error(f"‚ùå Chunk directory not found: {chunk_dir}")
            raise typer.Exit(1)
        
        # Create index directory
        index_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize RAG core
        from pynucleus.rag import RAGCore
        rag_core = RAGCore(data_dir=index_dir.parent.parent)
        
        # Build index
        cli_logger.info("üîÑ Building FAISS index...")
        result = rag_core.build_index(force_rebuild=force_rebuild)
        
        if result["status"] == "success":
            cli_logger.info(f"‚úÖ Successfully built FAISS index")
            print(f"üìä Index size: {result.get('index_size', 'Unknown')}")
            print(f"üìê Dimensions: {result.get('dimensions', 'Unknown')}")
            print(f"üìÑ Chunks indexed: {result.get('chunks_indexed', 'Unknown')}")
            print(f"üìÅ Index saved to: {index_dir}")
        elif result["status"] == "exists":
            print(f"‚ÑπÔ∏è  Index already exists: {result['message']}")
            print(f"üìÅ Existing indices: {result.get('existing_indices', [])}")
        else:
            cli_logger.error(f"‚ùå Index building failed: {result['message']}")
            raise typer.Exit(1)
            
    except Exception as e:
        cli_logger.error(f"‚ùå FAISS index building failed: {str(e)}")
        print(f"‚ùå Error: {str(e)}")
        raise typer.Exit(1)

@app.command("ask")
def ask_question(
    question: str = typer.Option(..., "--question", help="Question to ask the RAG system"),
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID for enhanced responses"),
    top_k: int = typer.Option(5, "--top-k", help="Number of top results to retrieve"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Ask a question to the RAG system."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üöÄ Starting RAG query")
        cli_logger.info(f"‚ùì Question: {question}")
        cli_logger.info(f"ü§ñ Model ID: {model_id}")
        cli_logger.info(f"üìä Top K: {top_k}")
        
        # Check for compiled DSPy program first
        from src.pynucleus.llm.dspy_compile import DSPyCompiler
        
        compiler = DSPyCompiler()
        compiled_program = compiler.load_compiled_program()
        
        if compiled_program:
            cli_logger.info("üéØ Using compiled DSPy program")
            # Use DSPy answer engine
            from src.pynucleus.llm.answer_engine import DSPyAnswerEngine
            engine = DSPyAnswerEngine(model_id=model_id)
            result = engine.answer_general(question)
            
            # Display DSPy results
            print("=" * 60)
            print("üß† DSPY ENHANCED RESPONSE")
            print("=" * 60)
            print(f"Question: {question}")
            print(f"Generation Time: {result.get('generation_time', 0):.2f}s")
            print("-" * 60)
            print(f"Answer: {result.get('answer', 'No answer available')}")
            print("-" * 60)
            print(f"Model: {result.get('model_id', model_id)}")
            print(f"DSPy Used: {result.get('dspy_used', False)}")
            print("=" * 60)
        else:
            cli_logger.warning("‚ö†Ô∏è No compiled DSPy program found, using fallback RAG system")
            
            # Initialize RAG pipeline
            from pynucleus.pipeline.pipeline_rag import RAGPipeline
            rag_pipeline = RAGPipeline(data_dir="data")
            
            # Query the RAG system
            cli_logger.info("üîÑ Processing question...")
            result = rag_pipeline.query(question, top_k=top_k)
            
            # Display results
            print("=" * 60)
            print("üìã RAG SYSTEM RESPONSE")
            print("=" * 60)
            print(f"Question: {question}")
            print(f"Confidence: {result.get('confidence', 0):.2f}")
            print("-" * 60)
            print(f"Answer: {result.get('answer', 'No answer available')}")
            print("-" * 60)
            print("Sources:")
            for i, source in enumerate(result.get('sources', []), 1):
                print(f"  {i}. {source}")
            print("=" * 60)
        
        # Always use LLM for enhanced response
        try:
            from pynucleus.llm.llm_runner import LLMRunner
            llm_runner = LLMRunner(model_id=model_id)
            
            cli_logger.info(f"ü§ñ Generating enhanced response with {model_id}")
            
            # Create enhanced prompt with context
            context = result.get('answer', '')
            enhanced_prompt = f"Based on this scientific information: {context}\n\nQuestion: {question}\n\nProvide a clear, technical answer:"
            
            llm_response = llm_runner.ask(
                question=enhanced_prompt,
                max_length=500,
                temperature=0.7
            )
            
            print("\nü§ñ ENHANCED LLM RESPONSE")
            print("-" * 60)
            print(llm_response)
            print("=" * 60)
            
        except Exception as llm_error:
            cli_logger.error(f"‚ùå LLM processing failed: {llm_error}")
            print(f"\n‚ùå LLM processing failed: {llm_error}")
            print("üîÑ Falling back to RAG-only response")
                
        cli_logger.info("‚úÖ Question processed successfully")
        
    except Exception as e:
        cli_logger.error(f"‚ùå Question processing failed: {str(e)}")
        print(f"‚ùå Error: {str(e)}")
        raise typer.Exit(1)

@app.command("dspy-compile")
def dspy_compile(
    csv_path: Path = typer.Option("docs/devset/dspy_examples.csv", "--csv-path", help="Path to development examples CSV"),
    output_dir: Path = typer.Option("data/dspy_artifacts", "--output-dir", help="Output directory for compiled artifacts"),
    ci: bool = typer.Option(False, "--ci", help="Run in CI mode (create artifact but don't commit)"),
    create_sample: bool = typer.Option(False, "--create-sample", help="Create sample development dataset"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
):
    """Compile DSPy programs using development examples."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üß† Starting DSPy compilation")
        cli_logger.info(f"üìã CSV Path: {csv_path}")
        cli_logger.info(f"üìÅ Output Directory: {output_dir}")
        cli_logger.info(f"üîß CI Mode: {ci}")
        
        # Import the compilation utility
        from src.pynucleus.llm.dspy_compile import compile_dspy_main
        
        # Run compilation
        exit_code = compile_dspy_main(
            csv_path=str(csv_path),
            output_dir=str(output_dir),
            ci_mode=ci,
            create_sample=create_sample
        )
        
        if exit_code == 0:
            if create_sample:
                cli_logger.info("‚úÖ Sample dataset created successfully")
                print("üìã Sample development dataset created!")
                print(f"üìÅ Location: {csv_path}")
                print("üí° Edit this file to add your own examples, then run compilation again")
            elif ci:
                cli_logger.info("‚úÖ CI compilation completed successfully")
                print("üéØ DSPy compilation completed in CI mode!")
                print("üìÑ Artifact created but not committed")
            else:
                cli_logger.info("‚úÖ DSPy compilation completed successfully")
                print("üéâ DSPy compilation completed!")
                print(f"üìÅ Compiled artifacts saved to: {output_dir}")
                print("üí° Now 'pynucleus ask' will use the compiled DSPy program")
        else:
            cli_logger.error("‚ùå DSPy compilation failed")
            print("‚ùå DSPy compilation failed. Check logs for details.")
            raise typer.Exit(1)
            
    except Exception as e:
        cli_logger.error(f"‚ùå DSPy compilation error: {e}")
        print(f"‚ùå Error: {str(e)}")
        raise typer.Exit(1)

@app.command("chat")
def interactive_chat(
    model_id: str = typer.Option("Qwen/Qwen2.5-1.5B-Instruct", "--model-id", help="LLM model ID for responses"),
    top_k: int = typer.Option(5, "--top-k", help="Number of top results to retrieve from RAG"),
    verbose: bool = typer.Option(False, "--verbose/--no-verbose", help="Verbose logging"),
    use_dspy: bool = typer.Option(True, "--use-dspy/--no-dspy", help="Use DSPy if available"),
):
    """Start an interactive chat session with the LLM."""
    try:
        # Setup logging
        logger = configure_logging(level="DEBUG" if verbose else "INFO")
        cli_logger = get_logger(__name__)
        
        cli_logger.info("üöÄ Starting interactive chat session")
        cli_logger.info(f"ü§ñ Model ID: {model_id}")
        cli_logger.info(f"üìä Top K: {top_k}")
        cli_logger.info(f"üß† DSPy enabled: {use_dspy}")
        
        # Initialize systems
        dspy_engine = None
        rag_pipeline = None
        llm_runner = None
        
        # Check for compiled DSPy program first
        if use_dspy:
            try:
                from src.pynucleus.llm.dspy_compile import DSPyCompiler
                from src.pynucleus.llm.answer_engine import DSPyAnswerEngine
                
                compiler = DSPyCompiler()
                compiled_program = compiler.load_compiled_program()
                
                if compiled_program:
                    cli_logger.info("üéØ Initializing DSPy answer engine")
                    dspy_engine = DSPyAnswerEngine(model_id=model_id)
                    
                    # Check if DSPy, LocalDSPy, or SimpleLocal was actually configured successfully
                    if dspy_engine.local_dspy_configured:
                        print("‚úÖ LocalDSPy enhanced mode enabled (LangChain + structured prompting)")
                        print("üéØ Using DSPy-like structured reasoning with local models")
                    elif dspy_engine.simple_local_configured:
                        print("‚úÖ SimpleLocal enhanced mode enabled (Direct transformers + structured prompting)")
                        print("üéØ Using basic structured reasoning with local models")
                    elif dspy_engine.dspy_configured:
                        print("‚úÖ DSPy enhanced mode enabled")
                    else:
                        print("‚ö†Ô∏è DSPy disabled - local models not supported, using standard RAG + LLM")
                        dspy_engine = None  # Clear the engine so we use fallback
                else:
                    cli_logger.warning("‚ö†Ô∏è No compiled DSPy program found")
                    print("‚ö†Ô∏è No compiled DSPy program found, using standard RAG + LLM")
            except Exception as e:
                cli_logger.warning(f"‚ö†Ô∏è DSPy initialization failed: {e}")
                print(f"‚ö†Ô∏è DSPy initialization failed: {e}")
        
        # Fallback to RAG pipeline
        if not dspy_engine:
            try:
                from pynucleus.pipeline.pipeline_rag import RAGPipeline
                from pynucleus.llm.llm_runner import LLMRunner
                
                cli_logger.info("üîß Initializing RAG pipeline and LLM runner")
                rag_pipeline = RAGPipeline(data_dir="data")
                llm_runner = LLMRunner(model_id=model_id)
                print("‚úÖ RAG + LLM mode enabled")
            except Exception as e:
                cli_logger.error(f"‚ùå Failed to initialize RAG/LLM systems: {e}")
                print(f"‚ùå Failed to initialize systems: {e}")
                raise typer.Exit(1)
        
        # Start interactive session
        print("\n" + "=" * 60)
        print("ü§ñ PYNUCLEUS INTERACTIVE CHAT")
        print("=" * 60)
        print("üí° Ask questions about chemical processes, simulations, or technical topics")
        print("üìù Type 'quit', 'exit', or press Ctrl+C to end the session")
        print("üîÑ Type 'clear' to clear the screen")
        print("‚ùì Type 'help' for more commands")
        print("=" * 60)
        
        question_count = 0
        
        while True:
            try:
                # Get user input
                print(f"\n[Q{question_count + 1}]", end=" ")
                question = input("üí≠ Your question: ").strip()
                
                # Handle special commands
                if question.lower() in ['quit', 'exit', 'q']:
                    print("üëã Goodbye! Chat session ended.")
                    break
                elif question.lower() == 'clear':
                    os.system('clear' if os.name == 'posix' else 'cls')
                    continue
                elif question.lower() == 'help':
                    print("\nüìã Available Commands:")
                    print("  ‚Ä¢ quit/exit/q  - End chat session")
                    print("  ‚Ä¢ clear        - Clear screen")
                    print("  ‚Ä¢ help         - Show this help")
                    print("  ‚Ä¢ Any question - Ask the AI system")
                    continue
                elif not question:
                    print("‚ùå Please enter a question or command")
                    continue
                
                question_count += 1
                
                print(f"\nüîÑ Processing question {question_count}...")
                start_time = datetime.now()
                
                # Process with DSPy if available
                if dspy_engine:
                    try:
                        result = dspy_engine.answer_general(question)
                        
                        # Display DSPy results
                        print("\n" + "üß† " + "=" * 58)
                        print(f"Answer: {result.get('answer', 'No answer available')}")
                        print("=" * 60)
                        print(f"‚è±Ô∏è  Generation time: {result.get('generation_time', 0):.2f}s")
                        print(f"ü§ñ Model: {result.get('model_id', model_id)}")
                        if dspy_engine.local_dspy_configured:
                            dspy_mode = "LocalDSPy"
                        elif dspy_engine.simple_local_configured:
                            dspy_mode = "SimpleLocal"
                        elif dspy_engine.dspy_configured:
                            dspy_mode = "DSPy"
                        else:
                            dspy_mode = "None"
                        print(f"üéØ Enhanced Mode: {dspy_mode}")
                        
                    except Exception as e:
                        cli_logger.error(f"‚ùå DSPy processing failed: {e}")
                        print(f"‚ùå DSPy processing failed: {e}")
                        continue
                
                # Process with RAG + LLM
                else:
                    try:
                        # Get RAG response
                        rag_result = rag_pipeline.query(question, top_k=top_k)
                        
                        # Get enhanced LLM response
                        context = rag_result.get('answer', '')
                        enhanced_prompt = f"Based on this scientific information: {context}\n\nQuestion: {question}\n\nProvide a clear, technical answer:"
                        
                        llm_response = llm_runner.ask(
                            question=enhanced_prompt,
                            max_length=500,
                            temperature=0.7
                        )
                        
                        # Display results
                        print("\n" + "üìã " + "=" * 58)
                        print(f"Answer: {llm_response}")
                        print("=" * 60)
                        
                        # Show sources if available
                        sources = rag_result.get('sources', [])
                        if sources:
                            print("üìö Sources:")
                            for i, source in enumerate(sources[:3], 1):  # Show top 3 sources
                                print(f"  {i}. {source}")
                        
                        end_time = datetime.now()
                        duration = (end_time - start_time).total_seconds()
                        print(f"‚è±Ô∏è  Response time: {duration:.2f}s")
                        print(f"ü§ñ Model: {model_id}")
                        print(f"üìä Confidence: {rag_result.get('confidence', 0):.2f}")
                        
                    except Exception as e:
                        cli_logger.error(f"‚ùå RAG/LLM processing failed: {e}")
                        print(f"‚ùå Processing failed: {e}")
                        continue
                
            except KeyboardInterrupt:
                print("\n\n‚èπÔ∏è Chat session interrupted by user")
                break
            except EOFError:
                print("\n\nüëã Chat session ended")
                break
            except Exception as e:
                cli_logger.error(f"‚ùå Unexpected error: {e}")
                print(f"‚ùå Unexpected error: {e}")
                continue
        
        # Session summary
        print(f"\nüìä Chat session summary:")
        print(f"  ‚Ä¢ Questions asked: {question_count}")
        if dspy_engine:
            if dspy_engine.local_dspy_configured:
                system_type = "LocalDSPy Enhanced"
            elif dspy_engine.simple_local_configured:
                system_type = "SimpleLocal Enhanced"
            elif dspy_engine.dspy_configured:
                system_type = "DSPy Enhanced"
            else:
                system_type = "DSPy (fallback mode)"
        else:
            system_type = "RAG + LLM"
        
        print(f"  ‚Ä¢ System used: {system_type}")
        print(f"  ‚Ä¢ Model: {model_id}")
        
        cli_logger.info(f"‚úÖ Chat session ended. Questions processed: {question_count}")
        
    except Exception as e:
        cli_logger.error(f"‚ùå Chat session failed: {str(e)}")
        print(f"‚ùå Error starting chat: {str(e)}")
        raise typer.Exit(1)

if __name__ == "__main__":
    app() 