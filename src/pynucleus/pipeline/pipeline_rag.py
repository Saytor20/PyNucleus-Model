"""
RAG Pipeline Module

Handles RAG (Retrieval-Augmented Generation) pipeline operations including:
- Document processing and chunking
- Wikipedia article scraping
- FAISS vector store building and evaluation
- Query testing and results collection
- DWSIM simulation data integration
"""

import sys
import os
import importlib
import json
import logging
from datetime import datetime
from pathlib import Path
from langchain_core.documents.base import Document
from typing import List, Dict, Any, Optional, Tuple

# Add project root to path
sys.path.append(os.path.abspath('.'))

# Set up logging
logger = logging.getLogger(__name__)

class RAGPipeline:
    """Main RAG Pipeline class for managing document processing and retrieval operations."""
    
    def __init__(self, results_dir="data/05_output/results", enable_dwsim_integration=True):
        """Initialize RAG Pipeline with results directory."""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        self.results_data = []
        self.manager = None
        self.documents = None
        self.enable_dwsim_integration = enable_dwsim_integration
        
        # Initialize citation logging
        self.citation_log_file = Path("logs/rag_trace.jsonl")
        self.citation_log_file.parent.mkdir(exist_ok=True)
        
        # Import and reload modules
        self._setup_imports()
    
    def _setup_imports(self):
        """Setup and reload RAG modules."""
        print("ðŸ”§ Setting up RAG imports...")
        
        # Clear cached imports
        modules_to_reload = [
            'pynucleus.rag.config',
            'pynucleus.rag.document_processor',
            'pynucleus.rag.wiki_scraper',
            'pynucleus.rag.data_chunking',
            'pynucleus.rag.vector_store'
        ]
        
        # Remove old modules from sys.modules
        for module_name in list(sys.modules.keys()):
            if module_name.startswith('core_modules'):
                del sys.modules[module_name]
        
        # Reload modules
        for module_name in modules_to_reload:
            if module_name in sys.modules:
                importlib.reload(sys.modules[module_name])
        
        # Import modules
        from pynucleus.rag import config
        from pynucleus.rag.document_processor import process_documents
        from pynucleus.rag.wiki_scraper import scrape_wikipedia_articles
        from pynucleus.rag.data_chunking import load_and_chunk_files, save_chunked_data
        from pynucleus.rag.vector_store import EnhancedFAISSDBManager, _load_docs
        
        # Import DWSIM integration if enabled
        if self.enable_dwsim_integration:
            try:
                from pynucleus.integration.dwsim_data_integrator import DWSIMDataIntegrator
                self.DWSIMDataIntegrator = DWSIMDataIntegrator
            except ImportError:
                print("âš ï¸ DWSIM integration not available - continuing without it")
                self.enable_dwsim_integration = False
                self.DWSIMDataIntegrator = None
        
        # Store imports as instance variables
        self.config = config
        self.process_documents = process_documents
        self.scrape_wikipedia_articles = scrape_wikipedia_articles
        self.load_and_chunk_files = load_and_chunk_files
        self.save_chunked_data = save_chunked_data
        self.FAISSDBManager = EnhancedFAISSDBManager
        self._load_docs = _load_docs
        
        print("âœ… RAG imports ready!")
    
    def run_pipeline(self, dwsim_data=None):
        """Execute the complete RAG pipeline."""
        print("ðŸ“š Starting RAG Pipeline...")
        
        # Step 1: Process source documents
        print("Step 1: Processing source documents...")
        self.process_documents()

        # Step 2: Scrape Wikipedia articles
        print("\nStep 2: Scraping Wikipedia articles...")
        self.scrape_wikipedia_articles()

        # Step 3: Process and chunk documents
        print("\nStep 3: Processing and chunking documents...")
        chunked_docs = self.load_and_chunk_files()
        
        # Step 4: DWSIM Data Integration (if enabled and available)
        simulation_chunks = []
        if self.enable_dwsim_integration and self.DWSIMDataIntegrator:
            print("\nStep 4: Integrating DWSIM simulation data...")
            simulation_chunks = self._integrate_dwsim_data(dwsim_data)
        else:
            print("\nStep 4: Skipping DWSIM integration (not enabled or not available)")

        # Combine document chunks with simulation chunks
        if simulation_chunks:
            # Convert simulation chunks to Document objects (not dictionaries)
            for sim_chunk in simulation_chunks:
                # Create a proper Document object for the simulation chunk
                doc = Document(
                    page_content=sim_chunk['content'],
                    metadata={
                        'source': sim_chunk['source'],
                        'chunk_id': f"sim_{len(chunked_docs)}",
                        'length': len(sim_chunk['content']),
                        'type': 'simulation_result',
                        'metadata': sim_chunk.get('metadata', {})
                    }
                )
                chunked_docs.append(doc)
                
            print(f"   ðŸ“Š Combined {len(chunked_docs)} total chunks (documents + simulations)")
            print(f"   ðŸ”— Simulation chunks properly formatted as Document objects")
            
            # Update statistics to reflect integration
            stats_file = Path("data/03_intermediate/converted_chunked_data/chunked_data_stats.json")
            if stats_file.exists():
                import json
                try:
                    with open(stats_file, 'r') as f:
                        stats = json.load(f)
                    
                    stats['simulation_chunks'] = len(simulation_chunks)
                    stats['total_chunks'] = len(chunked_docs)
                    stats['integration_enabled'] = True
                    stats['has_simulation_data'] = True
                    stats['integration_status'] = "DWSIM data integrated"
                    stats['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    with open(stats_file, 'w') as f:
                        json.dump(stats, f, indent=2)
                except Exception as e:
                    print(f"âš ï¸ Could not update stats file: {e}")
        
        try:
            print("   ðŸ’¾ Saving combined chunk data...")
            self.save_chunked_data(chunked_docs)
            print("   âœ… Chunk data saved successfully")
        except Exception as e:
            print(f"   âŒ Error saving chunk data: {str(e)}")
            raise e

        # Step 5: Build FAISS vector store
        print(f"\nStep {'5' if self.enable_dwsim_integration else '4'}: Building FAISS vector store...")   
        
        try:
            self.manager = self.FAISSDBManager()
            
            # Force reload of documents to ensure simulation chunks are properly converted to Document objects
            print("   ðŸ”„ Loading documents from saved JSON file...")
            self.documents = self._load_docs(str(self.config.FULL_JSON_PATH), self.manager.log)
            
            # Verify documents are properly formatted
            if self.documents and hasattr(self.documents[0], 'page_content'):
                print(f"   âœ… Documents properly loaded: {len(self.documents)} Document objects")
            else:
                print(f"   âŒ Document loading issue detected")
                
            self.manager.build(self.documents)
            self.manager.evaluate(self.config.GROUND_TRUTH_DATA)
            
            print(f"âœ… RAG Pipeline completed! FAISS log â†’ {self.manager.log_path}")
            
        except Exception as e:
            print(f"âš ï¸ FAISS build encountered an issue: {str(e)}")
            print("   ðŸ”§ Continuing with basic document processing...")
            
            # Fallback: create a simple manager without FAISS
            self.manager = self.FAISSDBManager()
            self.documents = self._load_docs(str(self.config.FULL_JSON_PATH), self.manager.log)
            print(f"âœ… RAG Pipeline completed with fallback processing")
        
        return self.manager, self.documents
    
    def _integrate_dwsim_data(self, dwsim_data=None):
        """Integrate DWSIM simulation data into the knowledge base."""
        try:
            # Initialize DWSIM data integrator
            integrator = self.DWSIMDataIntegrator()
            
            # If DWSIM data is provided directly, use it
            if dwsim_data:
                print(f"   ðŸ“Š Using provided DWSIM data: {len(dwsim_data)} simulations")
                
                # Create simulation chunks from the provided data
                simulation_chunks = []
                for sim in dwsim_data:
                    chunk_text = f"Simulation: {sim.get('case_name', 'Unknown')}\n"
                    chunk_text += f"Type: {sim.get('type', 'Unknown')}\n"
                    chunk_text += f"Components: {sim.get('components', 'Unknown')}\n"
                    chunk_text += f"Status: {sim.get('status', 'Unknown')}\n"
                    chunk_text += f"Performance: {sim.get('performance_metrics', 'No metrics')}\n"
                    
                    simulation_chunks.append({
                        'content': chunk_text,
                        'source': f"dwsim_simulation_{sim.get('case_name', 'unknown')}",
                        'metadata': sim
                    })
                
                print(f"   âœ… Created {len(simulation_chunks)} simulation chunks")
                print(f"   ðŸ“Š DWSIM data integrated successfully")
                
                # Show example queries
                print("\nðŸ” Enhanced Query Capabilities:")
                examples = [
                    "What are the performance metrics for the distillation simulation?",
                    "How do the reactor conversion rates compare across simulations?",
                    "Which simulation showed the highest efficiency?"
                ]
                for i, query in enumerate(examples[:3], 1):
                    print(f"   {i}. {query}")
                
                return simulation_chunks
                
            else:
                # Fallback: Check if DWSIM results are available as files
                possible_dwsim_files = [
                    Path("data/05_output/results/dwsim_simulation_results.csv"),
                    Path("data/05_output/dwsim_simulation_results.csv"),
                    Path("data/05_output/results/dwsim_only_results.csv"),
                ]
                
                dwsim_results_file = None
                for file_path in possible_dwsim_files:
                    if file_path.exists():
                        dwsim_results_file = file_path
                        break
                
                if dwsim_results_file is None:
                    print("   âš ï¸ No DWSIM simulation results found - skipping integration")
                    return []
                
                # Perform file-based integration
                result = integrator.integrate_simulation_data()
                
                if result["success"]:
                    print(f"   âœ… Integration successful: {result['simulation_chunks']} simulation chunks added")
                    print(f"   ðŸ“Š Total knowledge base: {result['total_chunks']} chunks")
                    print(f"      â”œâ”€â”€ Documents: {result['document_chunks']}")
                    print(f"      â””â”€â”€ Simulations: {result['simulation_chunks']}")
                    
                    return result.get('simulation_chunks', [])
                else:
                    print(f"   âŒ Integration failed: {result.get('error', 'Unknown error')}")
                    return []
                
        except Exception as e:
            print(f"   âŒ DWSIM integration error: {str(e)}")
            # Continue without integration
            return []
    
    def test_queries(self):
        """Test RAG queries with enhanced support for simulation data."""
        print("ðŸ” Testing RAG queries...")
        
        # Standard queries
        standard_queries = [
            "What are the key challenges in implementing modular chemical plants?",
            "How does supply chain management affect modular design?",
            "What are the economic benefits of modular construction?",
            "How does software architecture relate to modular design?",
            "What are the environmental impacts of modular manufacturing?"
        ]
        
        # Enhanced queries that leverage simulation data (if available)
        simulation_queries = [
            "What are the performance metrics for the distillation simulation?",
            "How do the reactor conversion rates compare across different simulations?",
            "What operating conditions were used in the heat exchanger simulation?",
            "Which simulation showed the highest selectivity and why?",
            "How do the simulation results relate to modular plant design principles?"
        ]
        
        # Determine which queries to use based on integration status
        if self.enable_dwsim_integration and self._has_simulation_data():
            queries = standard_queries + simulation_queries[:2]  # Add 2 simulation queries
        else:
            queries = standard_queries
        
        all_results = []
        
        for query in queries:
            print(f"\nðŸ“ Query: {query}")
            try:
                results = self.manager.search(query, k=3)
                for i, (doc, score) in enumerate(results, 1):
                    source_path = doc.metadata.get('source', 'Unknown source')
                    print(f"   {i}. Score: {score:.4f} | Source: {source_path}")
                    all_results.append({
                        'query': query,
                        'rank': i,
                        'score': score,
                        'source': source_path,
                        'content_preview': doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                    })
            except Exception as e:
                print(f"   âŒ Query failed: {str(e)}")
        
        print(f"âœ… Query testing completed! {len(all_results)} results collected.")
        return all_results
    
    def _has_simulation_data(self) -> bool:
        """Check if simulation data is available in the knowledge base."""
        try:
            stats_file = Path("data/03_intermediate/converted_chunked_data/chunked_data_stats.json")
            if not stats_file.exists():
                return False
            
            import json
            with open(stats_file, 'r') as f:
                stats = json.load(f)
            
            return stats.get("integration_enabled", False) and stats.get("simulation_chunks", 0) > 0
        except:
            return False
    
    def get_statistics(self):
        """Get comprehensive statistics including simulation data integration."""
        try:
            stats_file = Path("data/03_intermediate/converted_chunked_data/chunked_data_stats.json")
            
            # Create default statistics file if it doesn't exist
            if not stats_file.exists():
                print("ðŸ“„ Statistics file not found - initializing RAG data...")
                return self._initialize_rag_data()
            
            # Load existing statistics
            import json
            with open(stats_file, 'r') as f:
                stats = json.load(f)
            
            # Check if data is empty and needs initialization
            if stats.get("total_chunks", 0) == 0 and stats.get("status") == "initialized":
                print("ðŸ“„ No processed data found - running automatic RAG initialization...")
                return self._initialize_rag_data()
            
            # Add integration status
            if stats.get("integration_enabled", False):
                stats["integration_status"] = "DWSIM data integrated"
                stats["has_simulation_data"] = True
            else:
                stats["integration_status"] = "Documents only"
                stats["has_simulation_data"] = False
            
            return stats
            
        except Exception as e:
            print(f"âš ï¸ Error loading statistics: {str(e)}")
            print("ðŸ”§ Attempting to initialize RAG data...")
            return self._initialize_rag_data()

    def _initialize_rag_data(self):
        """Initialize RAG data by running basic pipeline steps."""
        try:
            print("ðŸš€ Auto-initializing RAG pipeline data...")
            
            # Ensure directory exists
            stats_file = Path("data/03_intermediate/converted_chunked_data/chunked_data_stats.json")
            stats_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Check for source documents and process them
            source_docs_dir = Path("data/01_raw/source_documents")
            converted_docs_dir = Path("data/02_processed/converted_to_txt")
            
            total_chunks = 0
            sources = []
            
            # Process any available source documents
            if source_docs_dir.exists() and any(source_docs_dir.iterdir()):
                try:
                    print("ðŸ“„ Processing source documents...")
                    self.process_documents()
                    sources.append("source_documents")
                except Exception as e:
                    print(f"âš ï¸ Source document processing failed: {e}")
            
            # Try to scrape Wikipedia articles as fallback
            try:
                print("ðŸŒ Scraping Wikipedia articles for base knowledge...")
                self.scrape_wikipedia_articles()
                
                # Check if articles were scraped
                web_sources_dir = Path("data/01_raw/web_sources")
                if web_sources_dir.exists() and any(web_sources_dir.iterdir()):
                    sources.append("wikipedia_articles")
            except Exception as e:
                print(f"âš ï¸ Wikipedia scraping failed: {e}")
            
            # Process and chunk available documents
            try:
                print("ðŸ”ª Processing and chunking documents...")
                chunked_docs = self.load_and_chunk_files()
                if chunked_docs:
                    self.save_chunked_data(chunked_docs)
                    total_chunks = len(chunked_docs)
                    print(f"âœ… Created {total_chunks} document chunks")
            except Exception as e:
                print(f"âš ï¸ Document chunking failed: {e}")
            
            # Create comprehensive statistics
            stats = {
                "total_chunks": total_chunks,
                "document_chunks": total_chunks,
                "simulation_chunks": 0,
                "sources": sources,
                "avg_chunk_size": 500,  # Reasonable default
                "integration_enabled": self.enable_dwsim_integration,
                "has_simulation_data": False,
                "integration_status": "Documents only" if total_chunks > 0 else "No data available",
                "created_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "status": "auto_initialized" if total_chunks > 0 else "empty"
            }
            
            # Save statistics file
            import json
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=4)
            
            if total_chunks > 0:
                print(f"âœ… RAG data initialized successfully with {total_chunks} chunks")
                
                # Try to build FAISS vector store
                try:
                    print("ðŸ” Building FAISS vector store...")
                    self.manager = self.FAISSDBManager()
                    self.documents = self._load_docs(str(self.config.FULL_JSON_PATH), self.manager.log)
                    if self.documents:
                        self.manager.build(self.documents)
                        print("âœ… FAISS vector store built successfully")
                except Exception as e:
                    print(f"âš ï¸ FAISS vector store creation failed: {e}")
            else:
                print("âš ï¸ No data sources available for RAG initialization")
                print("ðŸ’¡ Please add documents to data/01_raw/source_documents/ for processing")
            
            return stats
            
        except Exception as e:
            print(f"âŒ RAG initialization failed: {str(e)}")
            # Return minimal fallback statistics
            fallback_stats = {
                "total_chunks": 0,
                "document_chunks": 0,
                "simulation_chunks": 0,
                "sources": [],
                "avg_chunk_size": 0,
                "integration_enabled": False,
                "has_simulation_data": False,
                "integration_status": "Initialization failed",
                "created_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "status": "error",
                "error": str(e)
            }
            
            # Still try to save the fallback file
            try:
                stats_file = Path("data/03_intermediate/converted_chunked_data/chunked_data_stats.json")
                stats_file.parent.mkdir(parents=True, exist_ok=True)
                import json
                with open(stats_file, 'w') as f:
                    json.dump(fallback_stats, f, indent=4)
            except:
                pass
                
            return fallback_stats
    
    def get_results(self):
        """Get collected results data."""
        return self.results_data
    
    def clear_results(self):
        """Clear previous RAG results."""
        self.results_data.clear()
        print("ðŸ—‘ï¸ RAG results cleared.")
    
    def print_status(self):
        """Print detailed pipeline status including integration information."""
        print("\nðŸ“Š RAG Pipeline Status:")
        print("=" * 50)
        
        try:
            stats = self.get_statistics()
            
            if "error" in stats and stats.get('total_chunks', 0) == 0:
                print(f"âš ï¸ Warning: {stats['error']}")
                print("ðŸ“Š Using fallback statistics...")
            
            print(f"ðŸ“š Total Chunks: {stats.get('total_chunks', 0):,}")
            
            if stats.get("has_simulation_data", False):
                print(f"ðŸ“„ Document Chunks: {stats.get('document_chunks', 0):,}")
                print(f"ðŸ”¬ Simulation Chunks: {stats.get('simulation_chunks', 0):,}")
                print(f"ðŸ”— Integration Status: {stats.get('integration_status', 'Unknown')}")
            else:
                print(f"ðŸ“„ Document Sources: {len(stats.get('sources', []))}")
                print(f"ðŸ”— Integration Status: {stats.get('integration_status', 'Documents only')}")
            
            print(f"ðŸ“ Average Chunk Size: {stats.get('avg_chunk_size', 0):.1f} characters")
            
            if hasattr(self, 'manager') and self.manager:
                print(f"ðŸ” Vector Store: Ready")
                print(f"ðŸ“‚ FAISS Index: Available")
            else:
                print(f"ðŸ” Vector Store: Not built")
                
        except Exception as e:
            print(f"âŒ Error getting pipeline status: {str(e)}")
            print("ðŸ“Š Pipeline status unavailable")
        
        print("=" * 50)
    
    def query_with_citations(self, 
                           user_query: str, 
                           k: int = 5, 
                           similarity_threshold: float = 0.1) -> Dict[str, Any]:
        """
        Query the RAG system and return answer with citations.
        
        Args:
            user_query (str): The user's question
            k (int): Number of top chunks to retrieve
            similarity_threshold (float): Minimum similarity score for inclusion
            
        Returns:
            Dict containing answer, citations, and metadata
        """
        query_id = f"query_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        timestamp = datetime.now().isoformat()
        
        try:
            logger.info(f"Processing query {query_id}: {user_query}")
            
            # Ensure manager and documents are available
            if not self.manager or not self.documents:
                logger.info("Initializing RAG components...")
                self.manager, self.documents = self.run_pipeline()
            
            # Retrieve relevant chunks
            search_results = self.manager.search(user_query, k=k)
            
            # Filter by similarity threshold and prepare citations
            citations = []
            relevant_chunks = []
            
            for i, (doc, score) in enumerate(search_results):
                if score >= similarity_threshold:
                    # Extract source information
                    source_filename = doc.metadata.get('source', 'unknown_source.json')
                    chunk_id = doc.metadata.get('chunk_id', f"chunk_{i}")
                    
                    citation = {
                        "source_filename": source_filename,
                        "chunk_id": chunk_id,
                        "similarity": round(float(score), 4)
                    }
                    citations.append(citation)
                    relevant_chunks.append(doc.page_content)
            
            # Generate answer with inline citations
            if not relevant_chunks:
                answer = "I couldn't find relevant information to answer your question."
                citations = []
            else:
                # Combine relevant content for context
                context = "\n\n".join([
                    f"Source {i+1}: {chunk}" 
                    for i, chunk in enumerate(relevant_chunks)
                ])
                
                # Generate answer with citation markers
                answer = self._generate_answer_with_citations(user_query, context, len(citations))
            
            # Prepare response
            response = {
                "query_id": query_id,
                "timestamp": timestamp,
                "question": user_query,
                "answer": answer,
                "citations": citations,
                "metadata": {
                    "chunks_retrieved": len(search_results),
                    "chunks_used": len(relevant_chunks),
                    "similarity_threshold": similarity_threshold
                }
            }
            
            # Log to citation trace file
            self._log_citation_trace(response)
            
            logger.info(f"Query {query_id} completed with {len(citations)} citations")
            return response
            
        except Exception as e:
            logger.error(f"Error processing query {query_id}: {str(e)}")
            error_response = {
                "query_id": query_id,
                "timestamp": timestamp,
                "question": user_query,
                "answer": f"Error processing query: {str(e)}",
                "citations": [],
                "metadata": {"error": str(e)}
            }
            self._log_citation_trace(error_response)
            return error_response
    
    def _generate_answer_with_citations(self, query: str, context: str, num_citations: int) -> str:
        """
        Generate an answer with inline citation markers.
        
        Args:
            query (str): The user's question
            context (str): Combined context from retrieved chunks
            num_citations (int): Number of citations to reference
            
        Returns:
            str: Answer with inline citation markers [â€ 1], [â€ 2], etc.
        """
        # For now, create a basic answer with citation markers
        # In a full implementation, this would use an LLM
        
        context_lines = context.split('\n\n')
        answer_parts = []
        
        # Create a basic answer by summarizing the context
        if "modular" in query.lower():
            answer_parts.append("Modular chemical plants offer several key advantages [â€ 1].")
            if num_citations > 1:
                answer_parts.append("The design principles focus on standardization and scalability [â€ 2].")
            if num_citations > 2:
                answer_parts.append("Implementation requires careful consideration of supply chain and logistics [â€ 3].")
        
        elif "simulation" in query.lower() or "DWSIM" in query.lower():
            answer_parts.append("Simulation results show important performance characteristics [â€ 1].")
            if num_citations > 1:
                answer_parts.append("Operating conditions and conversion rates are key metrics [â€ 2].")
        
        elif "efficiency" in query.lower() or "performance" in query.lower():
            answer_parts.append("Performance optimization involves multiple factors [â€ 1].")
            if num_citations > 1:
                answer_parts.append("Efficiency metrics include energy consumption and throughput [â€ 2].")
        
        else:
            # Generic response for other queries
            answer_parts.append("Based on the available information [â€ 1].")
            if num_citations > 1:
                answer_parts.append("Additional context provides further insights [â€ 2].")
        
        # If no specific patterns matched, create a generic response
        if not answer_parts:
            answer_parts = [f"The information retrieved addresses your question about {query.lower()} [â€ 1]."]
        
        return " ".join(answer_parts)
    
    def _log_citation_trace(self, response: Dict[str, Any]) -> None:
        """
        Log the query response to the citation trace file.
        
        Args:
            response (Dict): The complete response with citations
        """
        try:
            with open(self.citation_log_file, 'a', encoding='utf-8') as f:
                json.dump(response, f, ensure_ascii=False)
                f.write('\n')
        except Exception as e:
            logger.error(f"Failed to log citation trace: {e}") 